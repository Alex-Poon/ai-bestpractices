[
{"title":"Case Study: Building a C Compiler with AI Agents","url":"/evidence/carlini-compiler.html","section":"evidence","desc":"Carlini\u0026#39;s experiment with parallel Claude agents on a real compiler project.","content":"Nicholas Carlini set out to answer a simple question: can you build real, production-grade software using nothing but parallel AI agents? The result was a 100,000-line Rust-based C compiler, produced by 16 parallel Claude instances over two weeks, that passed 99% of the GCC torture test suite and compiled the Linux 6.9 kernel across x86, ARM, and RISC-V architectures.\nThe compiler itself is …","tags":["case-study","parallel-agents","autonomous-coding","test-design"]},
{"title":"Claude Code Deep Dive","url":"/tools/claude-code.html","section":"tools","desc":"Swarms, LSP support, AGENTS.md, benchmarks — everything practitioners need to know.","content":"Claude Code is Anthropic\u0026amp;rsquo;s CLI-first coding agent. It runs in the terminal, takes natural language task descriptions, and executes autonomously \u0026amp;ndash; reading files, running commands, editing code, and iterating on failures. It is single-model (Anthropic\u0026amp;rsquo;s Claude family), available through Claude Max subscriptions at $100-200/month, and focused entirely on agentic workflows with no …","tags":["claude-code","agent-workflows","swarms","harness-engineering"]},
{"title":"Is AI-Assisted Coding Getting Worse?","url":"/debates/is-it-getting-worse.html","section":"debates","desc":"Practitioners disagree on whether models are degrading — or whether users are simply hitting real limits.","content":"The Question Few topics generate more heat in developer communities than the question of whether AI coding tools are getting worse over time. The complaint surfaces constantly: tasks that worked last month now require more prompting, models seem to lose coherence during US business hours, and context windows that once felt adequate now collapse under normal workloads.\nThe stakes are real. …","tags":["model-quality","degradation","reliability","benchmarks"]},
{"title":"Model Releases in 2026","url":"/landscape/model-releases-2026.html","section":"landscape","desc":"Claude 4.6, Gemini 3, DeepSeek, Llama 4, Mistral 3 — the rapidly evolving model landscape.","content":"The first weeks of 2026 have seen an extraordinary density of model releases. The pace itself is informative: frontier model competition has intensified to the point where simultaneous launches are the norm rather than the exception. Here is what has shipped and what it means for practitioners.\nClaude Opus 4.6 (February 2026) Anthropic\u0026amp;rsquo;s latest flagship arrived with a 1M-token context window …","tags":["model-releases","claude-code","gemini","llama","mistral","deepseek"]},
{"title":"Power Users","url":"/voices/power-users.html","section":"voices","desc":"Practitioners who\u0026#39;ve gone deep with AI coding tools — their hard-won insights and techniques.","content":"These are developers who use AI coding tools daily, have developed sophisticated workflows, and share their techniques with concrete detail. They are not cheerleaders \u0026amp;ndash; most have strong opinions about what works and what does not.\nThe context engineer energy123 maintains a single markdown file of roughly 15,000 tokens containing his project\u0026amp;rsquo;s entire world model \u0026amp;ndash; use cases, …","tags":["practitioner-insights","power-users","expert-techniques"]},
{"title":"Task Scoping","url":"/practices/task-scoping.html","section":"practices","desc":"The dominant skill in agent-assisted development — breaking work into right-sized chunks that agents can execute reliably.","content":"Task scoping is the dominant skill in AI-assisted development. It determines whether your interaction with an agent produces usable output in minutes or wasted effort measured in hours. Every practitioner who has moved past the honeymoon phase identifies decomposition \u0026amp;ndash; not prompting, not model selection \u0026amp;ndash; as the discipline that matters most.\nThe core insight is deceptively simple: AI …","tags":["task-scoping","fundamentals","workflow"]},
{"title":"The Core Loop","url":"/guide/core-loop.html","section":"guide","desc":"The plan-execute-verify loop that every effective AI coding workflow converges on.","content":"Every effective AI coding workflow, regardless of which tool or model you use, converges on the same four-phase loop: Plan, Execute, Verify, Harness. The tools change. The models improve. But the loop persists, because it addresses the fundamental constraints of working with systems that are brilliant and forgetful in equal measure.\nThis page describes each phase, why it matters, and what goes …","tags":["core-loop","workflow","fundamentals"]},
{"title":"Tool Comparison Matrix","url":"/tools/compare.html","section":"tools","desc":"Head-to-head comparison of AI coding tools — based on what practitioners actually report, not marketing.","content":"Choosing an AI coding tool in early 2026 is less about finding the objectively best option and more about understanding which tradeoffs match your workflow. Every tool has vocal advocates and equally vocal critics. The comparison below is drawn entirely from practitioner reports in community discussions \u0026amp;ndash; not from vendor marketing or benchmark claims.\nThe Comparison Matrix Claude Code Cursor …","tags":["tools","comparison","decision-making"]},
{"title":"Claude Opus 4.6","url":"/sources/2026-02-05-claude-opus-4-6.html","section":"sources","desc":"","content":"Summary Anthropic announced Claude Opus 4.6, their most advanced model to date, featuring a landmark 1 million token context window in beta \u0026amp;ndash; the first for an Opus-class model. The release emphasizes substantial improvements in agentic coding, long-context work, and sustained multi-step workflows.\nOn benchmarks, Opus 4.6 achieved the top score on Terminal-Bench 2.0 for agentic coding and led …","tags":["model-release","anthropic","claude","opus","benchmarks","agent-teams","long-context"]},
{"title":"Claude Code daily benchmarks for degradation tracking","url":"/sources/2026-01-29-claude-code-benchmarks.html","section":"sources","desc":"","content":"Summary MarginLab launched a daily benchmark tracker for Claude Code performance, measuring the CLI tool against a curated subset of SWE-Bench-Pro tasks using the Opus 4.5 model. The tracker was created in response to widespread community concerns about silent model degradation, where users noticed performance fluctuations but had no systematic way to measure or verify them.\nThe methodology …","tags":["claude-code","benchmarks","model-degradation","evaluation","swe-bench","reliability"]},
{"title":"Case Study: Hashimoto's AI Adoption Journey","url":"/evidence/hashimoto-journey.html","section":"evidence","desc":"How the HashiCorp founder evolved from skeptic to power user.","content":"Mitchell Hashimoto \u0026amp;ndash; co-founder of HashiCorp, creator of Vagrant, Terraform, and Consul \u0026amp;ndash; published a detailed account of his personal journey adopting AI coding tools. The article resonated far beyond the usual AI discourse because Hashimoto is not a hype-prone commentator. He is a practitioner with decades of experience shipping infrastructure software used by millions of developers. …","tags":["case-study","adoption-strategy","harness-engineering"]},
{"title":"Cursor","url":"/tools/cursor.html","section":"tools","desc":"What practitioners say about the AI-native IDE.","content":"Cursor is a VS Code fork with AI capabilities built directly into the editor. It offers both autocomplete (Tab) and agentic (Composer/Agent) modes as first-class features, multi-model support with user-selectable models, and pricing starting at $20/month with usage-based tiers for heavier workloads. It has the largest adoption among AI coding tools in the IDE-first category.\nWhat Cursor Does Well …","tags":["cursor","ide-tools","tool-comparison"]},
{"title":"Getting Started","url":"/guide/getting-started.html","section":"guide","desc":"Your first week with AI coding tools — practical steps to get productive fast.","content":"You have read about the core loop. Now it is time to put it into practice. This page covers the concrete steps for your first week with AI coding tools \u0026amp;ndash; which tool to pick, what to try first, which mistakes to avoid, and what \u0026amp;ldquo;working\u0026amp;rdquo; actually looks like.\nThe goal is not mastery. The goal is calibration: developing an accurate sense of what AI agents handle well and where they …","tags":["getting-started","beginner","setup"]},
{"title":"Harness Engineering","url":"/practices/harness-engineering.html","section":"practices","desc":"Building the infrastructure around your AI agent — AGENTS.md files, custom tools, and test harnesses that compound over time.","content":"Harness engineering is the practice of building persistent infrastructure that constrains and guides AI agents across sessions. It is the highest-leverage investment in AI-assisted development because it compounds: every mistake you document is a mistake that never recurs, every custom tool you build saves time in every future session, and every test harness you configure raises the floor on …","tags":["harness-engineering","agents-md","tooling","infrastructure"]},
{"title":"The Adoption Curve","url":"/landscape/adoption-curve.html","section":"landscape","desc":"Where the industry stands on AI-assisted development adoption.","content":"AI-assisted development in early 2026 is past the novelty phase but far from universal. The community is split, the data is mixed, and where a given developer lands depends as much on their workflow and identity as on the technology itself.\nThe Six Stages Framework The AI Adoption Curve framework, synthesized from Mitchell Hashimoto\u0026amp;rsquo;s influential account and extensive practitioner …","tags":["adoption-strategy","industry-trends"]},
{"title":"The Vibe Coding Question","url":"/debates/vibe-coding.html","section":"debates","desc":"Where on the spectrum from full delegation to tight leash should AI-assisted development land?","content":"The Question Andrej Karpathy coined the term \u0026amp;ldquo;vibe coding\u0026amp;rdquo; to describe a mode of AI-assisted development where you fully cede implementation to the model, accepting code you don\u0026amp;rsquo;t fully understand as long as it works. The term quickly became a lightning rod \u0026amp;ndash; embraced by some as the future of software development, rejected by others as professional malpractice.\nBut the …","tags":["vibe-coding","delegation","autonomy","workflow"]},
{"title":"Thoughtful Skeptics","url":"/voices/skeptics.html","section":"voices","desc":"Practitioners who see real limitations — sharp critiques grounded in experience, not dismissal.","content":"These are not people who dismiss AI coding tools without trying them. They are practitioners who have used the tools extensively and articulate specific, concrete concerns. Their critiques are grounded in real failures, not theoretical objections.\nThe craft programmer ryandrake got into programming because he enjoys programming \u0026amp;ndash; the act of defining problems in data structures, the puzzle of …","tags":["practitioner-insights","skeptics","limitations","criticism"]},
{"title":"A few random notes from Claude coding quite a bit last week","url":"/sources/2026-01-26-karpathy-claude-coding-notes.html","section":"sources","desc":"","content":"Summary Andrej Karpathy shared a widely discussed thread of observations from extensive Claude Code usage. His notes touched on several key themes that resonated deeply with the developer community, generating nearly 100 HN comments and over 900 upvotes.\nOne of Karpathy\u0026amp;rsquo;s central observations was around the tension between AI-assisted productivity and personal skill development. He noted …","tags":["claude-code","vibe-coding","agent-workflows","skill-atrophy","productivity","ide-tools"]},
{"title":"Apple picks Gemini to power Siri","url":"/sources/2026-01-12-apple-picks-gemini-siri.html","section":"sources","desc":"","content":"Summary Apple announced a partnership with Google to use Gemini as the foundational AI technology powering Siri, marking one of the most significant strategic moves in the AI industry. The deal, reportedly valued near $1 billion, represents Apple\u0026amp;rsquo;s acknowledgment that building competitive frontier AI models in-house is not where their advantage lies.\nApple\u0026amp;rsquo;s decision was driven by …","tags":["apple","google","gemini","siri","ai-strategy","industry-partnerships","model-selection"]},
{"title":"How to Code Claude Code in 200 Lines of Code","url":"/sources/2026-01-08-claude-code-200-lines.html","section":"sources","desc":"","content":"Summary Mihail Eric\u0026amp;rsquo;s article, provocatively titled \u0026amp;ldquo;The Emperor Has No Clothes,\u0026amp;rdquo; argues that AI coding assistants are not magical — they follow a simple architectural loop. The user sends a request, the LLM decides which tools to call, your code executes those tools locally, and the results flow back to the LLM for context. The critical mental model is that the LLM never …","tags":["claude-code","agent-architecture","tool-calling","coding-agents","demystification"]},
{"title":"Amp Code","url":"/tools/amp.html","section":"tools","desc":"Sourcegraph\u0026#39;s multi-model coding agent — architecture and practitioner reception.","content":"Amp Code is a coding agent built by the team behind Sourcegraph\u0026amp;rsquo;s code intelligence platform. Available as a CLI, VS Code extension, and JetBrains plugin, it is the most explicit implementation of multi-model routing in a production coding tool. Rather than letting users pick a single model, Amp routes tasks automatically based on type \u0026amp;ndash; sending planning, implementation, review, and …","tags":["amp-code","multi-model","agent-architecture","tool-comparison"]},
{"title":"Converts","url":"/voices/converts.html","section":"voices","desc":"Practitioners who changed their mind — from skepticism to adoption, or from enthusiasm to caution.","content":"The most interesting voices in any technology debate are the ones who changed their mind. These practitioners describe a specific before-and-after: a concrete moment or experience that shifted their position. The conversions go both directions \u0026amp;ndash; skeptics who became believers and enthusiasts who developed reservations.\nFrom IC to manager (reluctantly) seer describes the shift to AI-assisted …","tags":["practitioner-insights","converts","changed-minds","adoption"]},
{"title":"Costs and Tradeoffs","url":"/landscape/costs-and-tradeoffs.html","section":"landscape","desc":"The real economics of AI coding — costs, reliability, and what practitioners report spending.","content":"AI coding tools promise productivity gains, but they come with real costs \u0026amp;mdash; financial, operational, and strategic. This page collects what practitioners have reported spending, the reliability challenges they face, and the economic questions that remain unresolved.\nThe $638 Study: What Heavy Usage Actually Costs One of the most detailed cost reports comes from a founder and CTO who tracked …","tags":["cost-optimization","reliability","ai-economics"]},
{"title":"Does AI Prevent Junior Developer Skill Formation?","url":"/debates/junior-skills.html","section":"debates","desc":"The evidence on whether AI coding tools help or harm the development of programming expertise.","content":"The Question In early 2026, Anthropic published a study on AI-assisted coding that confirmed what many practitioners already suspected: developers using AI tools showed impaired conceptual understanding and weaker debugging skills compared to those who struggled through problems manually. The study landed in a community already anxious about what AI means for the next generation of programmers. …","tags":["skill-formation","learning","junior-developers","education"]},
{"title":"Practitioner Voices","url":"/evidence/practitioner-voices.html","section":"evidence","desc":"What developers actually say — curated insights from Hacker News discussions.","content":"The best evidence for how AI coding tools actually perform comes not from marketing materials or press releases but from practitioners reporting concrete experiences. The following quotes and observations are drawn from Hacker News discussions that collectively represent thousands of upvotes and hundreds of comments from working developers.\nEach entry is attributed to its original author and …","tags":["community-discussion","practitioner-insights"]},
{"title":"The Adoption Curve","url":"/guide/adoption-curve.html","section":"guide","desc":"What to expect at each stage — from skepticism through the valley of inefficiency to proficiency.","content":"If you have been using AI coding tools for more than a few weeks, you have probably noticed something: the initial excitement fades. Tasks that seemed magical at first start revealing cracks. The agent makes the same category of mistake for the third time. You spend twenty minutes fixing something that should have taken five. You wonder if you were wrong about the whole thing.\nYou are not wrong. …","tags":["adoption","learning-curve","skill-development"]},
{"title":"Verification","url":"/practices/verification.html","section":"practices","desc":"How to verify AI-generated code quickly and reliably — the skill that makes everything else work.","content":"Verification is the skill that makes every other AI-assisted development practice work. Task scoping, harness engineering, and model selection all ultimately exist to make verification easier and faster. If you cannot verify the output, nothing else matters.\nThe shift is fundamental: as AI takes over code generation, the developer\u0026amp;rsquo;s primary contribution moves from writing to reading, …","tags":["verification","testing","code-review","quality"]},
{"title":"How AI assistance impacts the formation of coding skills","url":"/sources/2026-01-30-ai-assistance-coding-skills.html","section":"sources","desc":"","content":"Summary Anthropic published a randomized controlled study examining how AI assistance affects skill acquisition among junior software engineers. The research involved 52 participants divided into AI-assisted and control groups, tasked with learning and using Trio (a Python asynchronous programming library). After completing coding tasks, participants took a comprehension quiz covering debugging, …","tags":["skill-development","research","anthropic","junior-developers","productivity","code-quality"]},
{"title":"AGENTS.md Outperforms Skills in Our Agent Evals","url":"/sources/2026-01-29-agents-md-outperforms-skills.html","section":"sources","desc":"","content":"Summary Vercel\u0026amp;rsquo;s engineering team published an evaluation comparing two approaches to providing AI coding agents with documentation: AGENTS.md files (passive context embedded in the system prompt) versus skills (active retrieval tools the agent can invoke on demand). The evaluation targeted Next.js 16 APIs that were absent from model training data, including new patterns like \u0026#39;use cache\u0026#39;, …","tags":["agents-md","harness-engineering","prompt-engineering","evals","vercel","next-js"]},
{"title":"Claude Code's New Hidden Feature: Swarms","url":"/sources/2026-01-24-claude-code-swarms.html","section":"sources","desc":"","content":"Summary A tweet by @NicerInPerson revealed that Claude Code contains hidden multi-agent orchestration capabilities, colloquially referred to as \u0026amp;ldquo;swarms.\u0026amp;rdquo; The discovery, corroborated by a GitHub repository (claude-sneakpeek by mikekelly), showed that Anthropic had built native sub-agent coordination features including a TeammateTool, delegate mode for spawning background agents, and a …","tags":["claude-code","multi-agent","swarms","agent-workflows","harness-engineering"]},
{"title":"Opus 4.5 is not the normal AI agent experience","url":"/sources/2026-01-06-opus-4-5-agent-experience.html","section":"sources","desc":"","content":"Summary Burke Holland wrote an enthusiastic account of his experience using Claude\u0026amp;rsquo;s Opus 4.5 model, arguing it represents a fundamental shift in AI agent capabilities that goes beyond anything he had previously experienced. His central claim is that Opus 4.5 delivers on promises that earlier AI coding agents could not fulfill, particularly around autonomous problem-solving and first-attempt …","tags":["opus","claude-code","agent-workflows","vibe-coding","ai-capabilities","developer-experience"]},
{"title":"Builders","url":"/voices/builders.html","section":"voices","desc":"People building the tools and infrastructure — their perspective from the inside.","content":"These are the people building the coding agents, harnesses, and infrastructure that everyone else uses. Their perspective is shaped by implementation reality: what actually works at scale, what breaks in production, and what the architecture looks like from the inside.\nThe moat analyst tptacek argues that there is no such thing as a frontier agent. While frontier models require massive resources …","tags":["practitioner-insights","builders","toolmakers","infrastructure"]},
{"title":"Engineering vs. Programming: Is This 'Real' Development?","url":"/debates/engineering-vs-programming.html","section":"debates","desc":"The identity question — does managing AI agents count as engineering, or is something fundamental being lost?","content":"The Question Something uncomfortable is happening to the developer identity. Tools that were supposed to make programming more productive are instead making some developers feel like they\u0026amp;rsquo;re not programming at all. When your day consists of writing prompts, reviewing AI output, and shepherding agents through tasks, the question becomes unavoidable: is this still engineering?\nThe tension runs …","tags":["engineering-identity","career","developer-role","management-analogy"]},
{"title":"GitHub Copilot","url":"/tools/copilot.html","section":"tools","desc":"Where inline completion fits in the age of agentic coding.","content":"GitHub Copilot is the most widely deployed AI coding tool, with access available to over 100 million developers through the GitHub ecosystem. It started as an inline autocomplete tool and has gradually expanded into chat and agentic capabilities, but its core strength remains fast, in-flow code completion.\nWhat Copilot Does Inline autocomplete. Copilot\u0026amp;rsquo;s primary feature is suggesting code as …","tags":["copilot","tool-comparison"]},
{"title":"Model Selection","url":"/practices/model-selection.html","section":"practices","desc":"Choosing the right model for each sub-task — when to use fast models, when to use deep reasoning, and when to use multiple.","content":"Model selection is the skill of matching the right model to the right task. The gap between a well-chosen and poorly-chosen model often matters more than the quality of the prompt itself. As models proliferate and differentiate, this is no longer a set-and-forget decision \u0026amp;ndash; it is an ongoing practice of matching capability to need.\nWhat Model Selection Is Model selection means choosing which …","tags":["model-selection","multi-model","cost-optimization"]},
{"title":"Open Questions","url":"/landscape/open-questions.html","section":"landscape","desc":"The debates still raging in the AI coding community.","content":"Not everything about AI-assisted development is settled. Several major questions remain genuinely open, with credible evidence and strong opinions on multiple sides. This page captures the debates as they stand in early 2026.\nIs AI Making Coding Skills Atrophy? Anthropic\u0026amp;rsquo;s own randomized controlled study provides the most rigorous data point. Junior developers using AI assistance scored 17% …","tags":["debate","open-questions","ai-skepticism","skill-atrophy"]},
{"title":"The Real Cost of AI Coding Tools","url":"/evidence/cost-data.html","section":"evidence","desc":"Aggregated spending data from practitioners — what people actually pay and whether the value justifies it.","content":"AI coding tools are not free. Despite the productivity narratives, few adoption stories include a line item for what the tools actually cost. This page aggregates the cost data that practitioners have shared \u0026amp;ndash; from individual developer spending to enterprise claims to the macro-economic forces reshaping AI pricing \u0026amp;ndash; to give a grounded picture of the economics of AI-assisted development …","tags":["costs","economics","data","pricing"]},
{"title":"When It Fails","url":"/guide/when-it-fails.html","section":"guide","desc":"An honest account of AI coding failure modes — drift, hallucination, and the limits of current tools.","content":"AI coding tools fail in characteristic ways. Not randomly \u0026amp;ndash; in patterns that are recognizable, largely predictable, and often preventable if you know what to watch for. This page catalogs the major failure modes from practitioner experience. Some are technical limitations of current models. Some are emergent behaviors of how agents interact with codebases. Some are human failures amplified …","tags":["failure-modes","drift","limitations","honest-assessment"]},
{"title":"AI's Impact on Coding Skills: What the Research Shows","url":"/evidence/coding-skills-impact.html","section":"evidence","desc":"The Anthropic study on AI assistance and skill development — what it found and what practitioners think.","content":"In early 2026, Anthropic published a study examining how AI coding assistance affects developer skill development. The study became one of the most intensely debated pieces of AI research in the developer community, generating over 346 comments on Hacker News. The reaction was notable not for the usual pro/anti-AI polarization but for the depth of personal reflection it provoked \u0026amp;ndash; developers …","tags":["skills","research","education","learning"]},
{"title":"Is the AI Coding Tool Economy Sustainable?","url":"/debates/cost-sustainability.html","section":"debates","desc":"With subsidized pricing, massive cash burn, and unclear unit economics — can this last?","content":"The Question The AI coding tool economy in early 2026 runs on a contradiction: developers are addicted to tools whose providers are hemorrhaging cash. OpenAI projects billions in losses, Anthropic burns through venture capital to subsidize Claude Code subscriptions, and Google treats Gemini as a loss leader to protect search revenue. Developers pay $20 to $200 per month for tools that consume far …","tags":["costs","sustainability","economics","pricing"]},
{"title":"Prompt Craft","url":"/practices/prompt-craft.html","section":"practices","desc":"Writing effective prompts for coding agents — from simple instructions to complex architectural guidance.","content":"Prompt craft is the skill of communicating effectively with AI coding agents. It is not the most important skill in AI-assisted development \u0026amp;ndash; task scoping and verification matter more \u0026amp;ndash; but it is the interface through which every other skill is expressed. A well-scoped task still needs to be communicated clearly. A well-built harness still needs to be written in language the agent …","tags":["prompts","instructions","context-engineering"]},
{"title":"The MCP Ecosystem","url":"/tools/mcp-ecosystem.html","section":"tools","desc":"Model Context Protocol is becoming the standard for AI tool interoperability.","content":"Model Context Protocol (MCP) is an open standard introduced by Anthropic for connecting AI agents to external tools and data sources. It defines a structured interface through which an agent can discover, invoke, and receive results from tools \u0026amp;ndash; databases, APIs, file systems, cloud services, and more \u0026amp;ndash; without custom integration code for each one.\nMCP matters because it addresses a …","tags":["mcp","tooling","interoperability"]},
{"title":"Claude Code gets native LSP support","url":"/sources/2025-12-22-claude-code-lsp-support.html","section":"sources","desc":"","content":"Summary Anthropic added native Language Server Protocol (LSP) support to Claude Code, enabling the CLI-based agent to integrate with language servers for improved code understanding, navigation, and analysis. The feature was announced through Claude Code\u0026amp;rsquo;s changelog and surfaced via a plugin system where users can discover and install LSP integrations.\nLSP support represents a significant …","tags":["claude-code","lsp","developer-tools","ide","plugins","code-intelligence"]},
{"title":"Cognitive Dependency: Are Developers Losing Their Edge?","url":"/debates/brain-atrophy.html","section":"debates","desc":"The atrophy question — whether relying on AI tools degrades the programming abilities developers already have.","content":"The Question Andrej Karpathy, one of the most prominent voices in AI, admitted that he can barely write C++ anymore after years of relying on AI assistants. For many developers, this confession crystallized a fear they had been quietly carrying: that the tools making them faster are simultaneously making them less capable.\nThe atrophy question cuts deeper than the related debate about whether …","tags":["skill-atrophy","cognitive-dependency","expertise","brain-atrophy"]},
{"title":"Context Management","url":"/practices/context-management.html","section":"practices","desc":"Working within and around context window limits — TODO lists, plan files, working memory patterns.","content":"Context management is the practice of working within and around the finite context window that constrains every AI agent interaction. It is the most technical of the core practices and the one that separates power users from frustrated ones. Most practical failures in agentic coding \u0026amp;ndash; going in circles, forgetting instructions, losing coherence over long sessions \u0026amp;ndash; trace back to context …","tags":["context-management","working-memory","context-window"]},
{"title":"Karpathy's AI Coding Notes: A Deep Reading","url":"/evidence/karpathy-notes.html","section":"evidence","desc":"Andrej Karpathy\u0026#39;s observations on AI-assisted development — and what the community\u0026#39;s 846-comment response reveals.","content":"In late January 2026, Andrej Karpathy \u0026amp;ndash; former director of AI at Tesla, founding member of OpenAI, and one of the most respected voices in machine learning \u0026amp;ndash; shared a thread of observations from a week of intensive Claude Code usage. The post received 911 upvotes on Hacker News and generated over 800 comments, making it one of the most substantive community discussions on AI-assisted …","tags":["karpathy","practitioner-insights","case-study"]},
{"title":"The Agentic AI Handbook: Production-Ready Patterns","url":"/sources/2026-01-21-agentic-ai-handbook.html","section":"sources","desc":"","content":"Summary The Agentic AI Handbook provides a comprehensive taxonomy of production-ready patterns for building agentic AI systems. Its core definition frames an agent as an LLM wrapped in a loop that can observe state, call tools, record results, and decide when it is done. The handbook organizes patterns into eight categories covering orchestration and control, tool use, context and memory, feedback …","tags":["agent-workflows","agentic-patterns","production-engineering","multi-agent","security"]},
{"title":"Gemini CLI","url":"/tools/gemini-cli.html","section":"tools","desc":"Google\u0026#39;s entry into the AI coding agent space — what practitioners think of its approach.","content":"Gemini CLI is Google\u0026amp;rsquo;s terminal-based coding agent, powered by Gemini models. It offers a similar interaction model to Claude Code \u0026amp;ndash; describe a task in natural language, and the agent reads files, generates code, and iterates \u0026amp;ndash; but draws on Google\u0026amp;rsquo;s model family and ecosystem. It is notable for its generous free tier, massive context windows, and the polarized practitioner …","tags":["gemini","google","tools","cli"]},
{"title":"Is AI Coding Killing Open Source?","url":"/debates/open-source-impact.html","section":"debates","desc":"From training data extraction to low-quality PRs — the open source community confronts existential questions.","content":"The Question Open source software has been the bedrock of modern computing for decades. Linux runs the cloud, Apache and Nginx serve the web, React and Vue power the frontend, and tens of thousands of libraries form the invisible infrastructure of every application we use. The people who built and maintained this commons did so for a complex mix of reasons: personal satisfaction, reputation …","tags":["open-source","community","sustainability","contributions"]},
{"title":"Multi-Agent Patterns","url":"/practices/multi-agent.html","section":"practices","desc":"Running parallel agents, swarms, and team-based workflows — when it works and when it doesn\u0026#39;t.","content":"Multi-agent patterns involve running multiple AI agents simultaneously on related tasks. They represent the highest-throughput mode of AI-assisted development \u0026amp;ndash; and the most complex. When they work, months of work happen in minutes. When they fail, you get merge conflicts, duplicated effort, and code that no single agent (or human) fully understands.\nThe practice is still emerging. There is …","tags":["multi-agent","parallel","swarms","orchestration"]},
{"title":"OpenAI Codex","url":"/tools/codex.html","section":"tools","desc":"OpenAI\u0026#39;s coding agent — how it compares to Claude Code and what practitioners report.","content":"OpenAI Codex is a cloud-sandboxed coding agent that takes a fundamentally different approach from CLI-first tools like Claude Code. Rather than operating interactively in your terminal, Codex runs tasks in isolated cloud environments \u0026amp;ndash; you provide specifications, it executes in a sandbox, and returns results. This \u0026amp;ldquo;outsourcing\u0026amp;rdquo; model prioritizes safety and deep reasoning over …","tags":["codex","openai","tools","agent"]},
{"title":"Which Programming Languages Work Best with AI?","url":"/debates/language-matters.html","section":"debates","desc":"Whether language choice matters for AI-assisted development — and which languages the community favors.","content":"The Question When developers adopt AI coding tools, they quickly discover that results vary by programming language. Python and TypeScript seem to get the best outputs. Rust and Go produce code that at least compiles. C++ and niche languages often yield frustrating results. But is this a fundamental property of the languages, a reflection of training data distribution, or something that will be …","tags":["programming-languages","tooling","developer-experience"]},
{"title":"Two Kinds of Vibe Coding","url":"/sources/2025-12-18-two-kinds-vibe-coding.html","section":"sources","desc":"","content":"Summary David Bau distinguishes between two fundamentally different approaches to what has broadly been called \u0026amp;ldquo;vibe coding.\u0026amp;rdquo; The first type involves delegating small tasks to an LLM while the human programmer remains fully informed and in control, reviewing each piece of work and making all key decisions. The second type involves surrendering cognitive control to an AI agent, allowing …","tags":["vibe-coding","testing","ai-coding-workflow","code-review","meta-cognition"]},
{"title":"How Vibe Coding Is Killing Open Source","url":"/sources/2026-02-02-vibe-coding-killing-open-source.html","section":"sources","desc":"","content":"Summary This Hackaday article reports on research examining how vibe coding — using LLM chatbots to generate code — creates systemic problems for open source projects. The core argument is that AI-mediated coding disrupts the traditional feedback loops between developers and open source communities in several damaging ways.\nFirst, developer engagement shifts away from open source communities …","tags":["vibe-coding","open-source","community","ai-impact","developer-experience"]},
{"title":"My LLM Coding Workflow Going into 2026","url":"/sources/2026-01-04-llm-coding-workflow-2026.html","section":"sources","desc":"","content":"Summary Addy Osmani, a well-known figure in the web development community, shares his comprehensive approach to AI-augmented software engineering. The central philosophy treats LLMs as powerful pair programmers that require clear direction, context, and human oversight rather than autonomous replacements for developers.\nThe workflow is structured around several core practices. First, planning …","tags":["ai-coding-workflow","prompt-engineering","code-review","testing","tool-selection"]},
{"title":"Tell HN: Claude Has Had 57 Incidents in the Past 3 Months","url":"/sources/2026-02-04-claude-57-incidents-3-months.html","section":"sources","desc":"","content":"Summary This HN text post by shikkra documents reliability concerns with Anthropic\u0026amp;rsquo;s Claude service, providing a detailed incident count from the official status page (status.claude.com). The author, a $100/month Max plan subscriber, was prompted to investigate after encountering a retry issue where Claude attempted to generate a response 10 times with Opus 4.5 and extended thinking enabled …","tags":["claude","reliability","anthropic","service-quality","developer-experience"]},
{"title":"Coding with LLMs in 2026: The Model Matters More Than the Prompts","url":"/sources/2026-01-18-model-matters-more-than-prompts.html","section":"sources","desc":"","content":"Summary This post by @slow_developer on X (formerly Twitter) argues that the shift from 2025 to 2026 in AI-assisted coding has been defined by a fundamental rebalancing: model choice now outweighs prompt engineering as the primary lever for coding quality. Where 2025 workflows focused heavily on crafting precise prompts and structuring interactions carefully, the advancements in frontier models …","tags":["model-selection","prompt-engineering","claude-code","cursor","agent-workflows"]},
{"title":"Agentic Frameworks in 2026: Less Hype, More Autonomy","url":"/sources/2026-01-06-agentic-frameworks-2026.html","section":"sources","desc":"","content":"Summary This HN text post by raghavchamadiya provides a practitioner-level comparison of agentic frameworks in 2026, focusing on lived behavior rather than benchmarks. The author has built, broken, and rebuilt agents across several stacks and shares observations on how the ecosystem has matured.\nThe core thesis is that the key differentiator for frameworks has shifted from how they wrap prompting …","tags":["agent-workflows","agentic-frameworks","memory-systems","multi-agent","production-engineering"]},
{"title":"Mistral 3 family of models released","url":"/sources/2025-12-02-mistral-3-models.html","section":"sources","desc":"","content":"Summary Mistral AI released the Mistral 3 family, a new generation of open-source multimodal models under the Apache 2.0 license. The lineup includes three dense models at 3B, 8B, and 14B parameters (the Ministral variants), plus Mistral Large 3, a sparse mixture-of-experts model with 41B active parameters drawn from a 675B total pool.\nAll models feature native multimodal and multilingual …","tags":["open-source-models","model-releases","multimodal","benchmarks","mistral"]},
{"title":"OpenAI's cash burn will be one of the big bubble questions of 2026","url":"/sources/2025-12-30-openai-cash-burn.html","section":"sources","desc":"","content":"Summary This Economist article examines the financial sustainability questions surrounding OpenAI and the broader AI industry heading into 2026. The piece focuses on OpenAI\u0026amp;rsquo;s extraordinary capital requirements and whether the company\u0026amp;rsquo;s spending trajectory can be justified by eventual revenue generation. The article could not be directly accessed due to the Economist\u0026amp;rsquo;s paywall, …","tags":["openai","ai-bubble","economics","venture-capital","sustainability","business-models"]},
{"title":"AI coding assistants are getting worse?","url":"/sources/2026-01-08-ai-coding-getting-worse.html","section":"sources","desc":"","content":"Summary This IEEE Spectrum article by Jamie Twiss presents the provocative claim that AI coding assistants are experiencing degradation rather than improvement. The central narrative comes from power users who report that these tools have hit a plateau, with some even declining in capability. The article identifies what it calls \u0026amp;ldquo;silent failures\u0026amp;rdquo; \u0026amp;ndash; situations where AI coding …","tags":["ai-coding-tools","model-degradation","evaluation","developer-experience","skepticism"]},
{"title":"Ask HN: 10 months since the Llama-4 release: what happened to Meta AI?","url":"/sources/2026-02-05-ask-hn-llama-4-meta.html","section":"sources","desc":"","content":"Summary An Ask HN post raised questions about the state of Meta\u0026amp;rsquo;s AI efforts roughly 10 months after the Llama 4 release, which was widely considered a disappointment. The original poster noted that Meta\u0026amp;rsquo;s API remained waitlist-only even after that long period, suggesting significant organizational or strategic problems.\nThe discussion paints a picture of a company that, despite …","tags":["meta","llama","open-source-ai","industry-analysis","model-release"]},
{"title":"How the AI Bubble Bursts in 2026","url":"/sources/2026-01-19-ai-bubble-bursts-2026.html","section":"sources","desc":"","content":"Summary This article from the \u0026amp;ldquo;Where\u0026amp;rsquo;s Your Ed At\u0026amp;rdquo; newsletter by Ed Zitron presents a detailed case for how the AI bubble might collapse in 2026. The analysis focuses on three interconnected pressure points: OpenAI\u0026amp;rsquo;s cash crisis, data center financing difficulties, and delayed infrastructure rollouts.\nOn the financial side, the article argues that OpenAI lacks sufficient …","tags":["ai-bubble","economics","infrastructure","openai","data-centers","skepticism"]},
{"title":"DeepSeek kicks off 2026 with paper signalling push to train bigger models for less","url":"/sources/2026-01-01-deepseek-bigger-models-less.html","section":"sources","desc":"","content":"Summary DeepSeek published a technical paper at the start of 2026 introducing Manifold-Constrained Hyper-Connections (mHC), a novel approach to training AI models more cost-effectively. The paper was co-authored by founder Liang Wenfeng and represents the Chinese AI startup\u0026amp;rsquo;s ongoing effort to compete with better-funded American competitors through efficiency innovations.\nThe mHC technique …","tags":["deepseek","training-efficiency","research-papers","model-architecture","china-ai"]},
{"title":"2026: The Year the IDE Died (Steve Yegge and Gene Kim)","url":"/sources/2025-12-10-year-ide-died.html","section":"sources","desc":"","content":"Summary This HN submission links to a YouTube talk by Steve Yegge and Gene Kim exploring how AI coding tools might replace the traditional IDE as the primary programming environment. The submitter (mikebiglan) frames the discussion around several key questions: how far IDEs will change, whether developers will still read and reason about code directly, and what the shift means for both senior …","tags":["ide","ai-coding-tools","vibe-coding","developer-workflows","future-of-coding"]},
{"title":"State of AI in 2026: LLMs, Coding, Scaling Laws, China, Agents, GPUs, AGI","url":"/sources/2026-02-01-state-of-ai-2026.html","section":"sources","desc":"","content":"Summary This is Lex Fridman Podcast episode #490, a comprehensive discussion on the state of AI in early 2026. The guests are Nathan Lambert, post-training lead at the Allen Institute for AI (AI2) and author of The RLHF Book, and Sebastian Raschka, author of \u0026amp;ldquo;Build a Large Language Model From Scratch\u0026amp;rdquo; and \u0026amp;ldquo;Build a Reasoning Model From Scratch.\u0026amp;rdquo;\nThe episode covers a wide …","tags":["ai-landscape","scaling-laws","coding-tools","china-ai","agents","agi","podcast"]},
{"title":"LLM coding workflow going into 2026","url":"/sources/2026-01-10-llm-coding-workflow-2026.html","section":"sources","desc":"","content":"Summary Addy Osmani, a well-known Google engineering leader, shared his comprehensive approach to integrating LLMs into daily coding workflows. The article frames LLMs not as autonomous coders but as powerful pair programmers requiring clear direction, context, and consistent oversight.\nThe workflow begins with collaborative planning, where the developer works with an AI to develop detailed …","tags":["workflow","prompt-engineering","model-selection","code-review","testing","agent-workflows"]},
{"title":"Obsidian meets Claude Code: A Markdown graph for agents and context","url":"/sources/2026-02-03-obsidian-claude-code.html","section":"sources","desc":"","content":"Summary Voicetree is an Electron-based desktop application that reimagines the development environment as a spatial graph where markdown notes and AI agent sessions coexist as interconnected nodes. The project merges Obsidian\u0026amp;rsquo;s graph-view visualization paradigm with Claude Code\u0026amp;rsquo;s agentic capabilities, addressing the growing challenge of managing multiple AI agent sessions and their …","tags":["agent-workflows","ide-tools","context-management","obsidian","spatial-computing","developer-tools"]},
{"title":"I spent $638 on AI coding agents in 6 weeks","url":"/sources/2025-11-13-ai-coding-agent-costs.html","section":"sources","desc":"","content":"Summary A founder and CTO building an AI-first CRM product shared a detailed breakdown of their AI coding costs, revealing surprisingly high expenses from using Cursor with Claude models. Over a six-week period spanning October and November 2025, the author accumulated $638 in on-demand charges, with October alone costing $348.56 and hitting Cursor\u0026amp;rsquo;s $400 limit.\nThe cost analysis revealed …","tags":["cost-optimization","cursor","claude-code","model-selection","pricing"]},
{"title":"AI Coding Toolkit: Low-overhead workflow for reliable AI coding","url":"/sources/2026-01-22-ai-coding-toolkit.html","section":"sources","desc":"","content":"Summary The AI Coding Toolkit is an open-source Git repository template designed to provide a structured yet lightweight workflow for semi-autonomous AI coding. The creator developed it after finding that existing AI coding workflows were either too complex (involving dozens of agents running in parallel) or too opinionated for the fast-moving AI coding landscape.\nThe toolkit operates through …","tags":["agent-workflows","sdlc","testing","code-quality","workflow-template"]},
{"title":"Introducing Agentic Vision in Gemini 3 Flash","url":"/sources/2026-02-03-gemini-3-agentic-vision.html","section":"sources","desc":"","content":"Summary Google introduced Agentic Vision as a new capability in Gemini 3 Flash, transforming image understanding from a static analysis task into a dynamic, action-oriented agentic process. Rather than simply analyzing images in isolation, Agentic Vision enables the model to interact with visual content across multiple steps, potentially examining images iteratively and taking actions based on …","tags":["gemini","google","agentic-vision","multimodal","model-release"]},
{"title":"Klaus – a Claude Code native delegating agentic harness","url":"/sources/2026-01-25-klaus-agentic-harness.html","section":"sources","desc":"","content":"Summary Klaus Baudelaire is an open-source agentic harness built entirely on top of Claude Code\u0026amp;rsquo;s native features, designed to automate task delegation and agent routing without external APIs or services. The system addresses the overhead of manually deciding which agent configuration to use for a given prompt by implementing a keyword-based scoring algorithm that evaluates prompt complexity …","tags":["claude-code","agent-workflows","harness-engineering","delegation","tool-routing"]},
{"title":"Claude Skill for Terraform/OpenTofu","url":"/sources/2026-01-19-terraform-skill-claude.html","section":"sources","desc":"","content":"Summary Anton Babenko, a well-known contributor in the Terraform ecosystem and maintainer of many popular terraform-aws-modules, released a Claude Code skill focused on Terraform and OpenTofu best practices. The skill aggregates trusted sources including terraform-best-practices.com and community-tested patterns from over 100 production modules.\nThe skill provides comprehensive guidance across …","tags":["terraform","claude-code","skills","infrastructure-as-code","devops"]}]
