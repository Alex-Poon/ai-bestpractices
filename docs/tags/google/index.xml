<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Google on AI Best Practices Knowledge Base</title>
    <link>/tags/google.html</link>
    <description>Recent content in Google on AI Best Practices Knowledge Base</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 06 Feb 2026 00:00:00 +0000</lastBuildDate>
    <atom:link href="/tags/google/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Apple picks Gemini to power Siri</title>
      <link>/sources/2026-01-12-apple-picks-gemini-siri.html</link>
      <pubDate>Mon, 12 Jan 2026 00:00:00 +0000</pubDate>
      <guid>/sources/2026-01-12-apple-picks-gemini-siri.html</guid>
      <description>&lt;h2 id=&#34;summary&#34;&gt;Summary&lt;/h2&gt;&#xA;&lt;p&gt;Apple announced a partnership with Google to use Gemini as the foundational AI technology powering Siri, marking one of the most significant strategic moves in the AI industry. The deal, reportedly valued near $1 billion, represents Apple&amp;rsquo;s acknowledgment that building competitive frontier AI models in-house is not where their advantage lies.&lt;/p&gt;&#xA;&lt;p&gt;Apple&amp;rsquo;s decision was driven by several practical realities. Despite having world-class edge inference silicon through their Neural Engine, Apple has effectively zero presence in training datacenters &amp;ndash; lacking the TPU pods or GPU clusters needed to train frontier models from scratch. Google, by contrast, has deep pockets, enterprise infrastructure experience, and diversified revenue streams that make them a stable long-term partner.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Gemini CLI</title>
      <link>/tools/gemini-cli.html</link>
      <pubDate>Fri, 06 Feb 2026 00:00:00 +0000</pubDate>
      <guid>/tools/gemini-cli.html</guid>
      <description>&lt;p&gt;Gemini CLI is Google&amp;rsquo;s terminal-based coding agent, powered by Gemini models. It offers a similar interaction model to Claude Code &amp;ndash; describe a task in natural language, and the agent reads files, generates code, and iterates &amp;ndash; but draws on Google&amp;rsquo;s model family and ecosystem. It is notable for its generous free tier, massive context windows, and the polarized practitioner reception that consistently separates benchmark performance from real-world experience.&lt;/p&gt;&#xA;&lt;h2 id=&#34;what-gemini-cli-offers&#34;&gt;What Gemini CLI Offers&lt;/h2&gt;&#xA;&lt;p&gt;&lt;strong&gt;Free tier access.&lt;/strong&gt; Through Google&amp;rsquo;s AI Studio, Gemini CLI provides a free tier that makes it one of the most accessible coding agents available. For developers who want to experiment with agentic coding without committing to a $100-200/month subscription, this is a meaningful advantage. The free tier has rate limits, but for personal projects and learning, it works.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Introducing Agentic Vision in Gemini 3 Flash</title>
      <link>/sources/2026-02-03-gemini-3-agentic-vision.html</link>
      <pubDate>Tue, 03 Feb 2026 00:00:00 +0000</pubDate>
      <guid>/sources/2026-02-03-gemini-3-agentic-vision.html</guid>
      <description>&lt;h2 id=&#34;summary&#34;&gt;Summary&lt;/h2&gt;&#xA;&lt;p&gt;Google introduced Agentic Vision as a new capability in Gemini 3 Flash, transforming image understanding from a static analysis task into a dynamic, action-oriented agentic process. Rather than simply analyzing images in isolation, Agentic Vision enables the model to interact with visual content across multiple steps, potentially examining images iteratively and taking actions based on what it observes.&lt;/p&gt;&#xA;&lt;p&gt;The feature represents a broader trend in AI development where multimodal capabilities are being enhanced with agentic behaviors. Instead of one-shot image analysis where a user submits a photo and receives a description, Agentic Vision allows the model to autonomously decide what to examine more closely, what questions to ask, and what actions to take based on visual inputs. This is particularly relevant for developer tools and automation workflows where visual understanding needs to be combined with decision-making.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
