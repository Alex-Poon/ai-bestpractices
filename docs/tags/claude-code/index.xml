<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Claude-Code on AI Best Practices Knowledge Base</title>
    <link>//localhost:1313/tags/claude-code.html</link>
    <description>Recent content in Claude-Code on AI Best Practices Knowledge Base</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 05 Feb 2026 00:00:00 +0000</lastBuildDate>
    <atom:link href="//localhost:1313/tags/claude-code/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>What 100 Parallel AI Agents Taught Us About Harness Engineering</title>
      <link>//localhost:1313/deep-dives/parallel-compiler-lessons.html</link>
      <pubDate>Thu, 05 Feb 2026 00:00:00 +0000</pubDate>
      <guid>//localhost:1313/deep-dives/parallel-compiler-lessons.html</guid>
      <description>&lt;p&gt;If you want to understand the future of autonomous AI development, ignore the code and study the harness.&lt;/p&gt;&#xA;&lt;p&gt;Nicholas Carlini&amp;rsquo;s compiler project &amp;ndash; 16 parallel Claude instances producing a 100,000-line Rust-based C compiler over two weeks &amp;ndash; is the most data-rich public experiment in parallel agentic coding to date. The compiler passed 99% of the GCC torture test suite and successfully compiled the Linux 6.9 kernel across x86, ARM, and RISC-V architectures. But the real value of the project lies not in the artifact it produced. It lies in what it revealed about how autonomous agents succeed and fail at scale.&lt;/p&gt;</description>
    </item>
    <item>
      <title>AI Coding Tools: A Practitioner&#39;s Comparison</title>
      <link>//localhost:1313/references/tool-landscape.html</link>
      <pubDate>Thu, 05 Feb 2026 00:00:00 +0000</pubDate>
      <guid>//localhost:1313/references/tool-landscape.html</guid>
      <description>&lt;h2 id=&#34;use-a-decision-framework-not-rankings&#34;&gt;Use a Decision Framework, Not Rankings&lt;/h2&gt;&#xA;&lt;p&gt;The AI coding tool landscape changes faster than any comparison can keep up with. Models improve monthly. Pricing shifts quarterly. New entrants appear constantly. Nearly $2 billion has been raised by coding agent startups in just the past five months.&lt;/p&gt;&#xA;&lt;p&gt;Rather than ranking tools, this page provides a decision framework based on factors that are relatively stable: workflow type, pricing model, extensibility, and integration approach. Use these to evaluate any tool, including ones that do not exist yet.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
