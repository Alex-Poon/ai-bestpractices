<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Model Deep Dive | AI Best Practices Knowledge Base</title>
  <meta name="description" content="Variant-level analysis of every major coding model — pricing, benchmarks, reasoning configs, and 605 practitioner reports from HN.">
  <meta name="color-scheme" content="dark light">

  
  <meta property="og:title" content="Model Deep Dive">
  <meta property="og:description" content="Variant-level analysis of every major coding model — pricing, benchmarks, reasoning configs, and 605 practitioner reports from HN.">
  <meta property="og:type" content="article">

  
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

  
  <link rel="stylesheet" href="../css/style.css">

  
  <link rel="icon" href="../favicon.svg" type="image/svg+xml">
</head>
<body class="section-tools"><header class="site-header">
  <nav class="nav-container">
    <a class="nav-logo" href="../index.html">
      <svg width="22" height="22" viewBox="0 0 24 24" fill="none" xmlns="http://www.w3.org/2000/svg">
        <circle cx="12" cy="12" r="10" stroke="currentColor" stroke-width="2"/>
        <circle cx="12" cy="8" r="2" fill="currentColor"/>
        <circle cx="7" cy="15" r="2" fill="currentColor"/>
        <circle cx="17" cy="15" r="2" fill="currentColor"/>
        <line x1="12" y1="10" x2="7" y2="13" stroke="currentColor" stroke-width="1.5"/>
        <line x1="12" y1="10" x2="17" y2="13" stroke="currentColor" stroke-width="1.5"/>
        <line x1="7" y1="15" x2="17" y2="15" stroke="currentColor" stroke-width="1.5"/>
      </svg>
      <span>AI Best Practices</span>
    </a>
    <div class="nav-links">
      <a class="nav-link" data-section="guide" href="../guide/index.html">Guide</a>
      <a class="nav-link" data-section="practices" href="../practices/index.html">Practices</a>
      <a class="nav-link" data-section="debates" href="../debates/index.html">Debates</a>
      <a class="nav-link" data-section="tools" href="../tools/index.html">Tools</a>
      <a class="nav-link" data-section="evidence" href="../evidence/index.html">Evidence</a>
      <a class="nav-link" data-section="voices" href="../voices/index.html">Voices</a>
      <a class="nav-link" data-section="sources" href="../sources/index.html">Sources</a>
    </div>
    <div class="nav-actions">
      <button class="nav-search-btn" type="button" aria-label="Search" id="searchTrigger">
        <svg width="14" height="14" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"><circle cx="11" cy="11" r="8"/><line x1="21" y1="21" x2="16.65" y2="16.65"/></svg>
        <span>Search</span>
        <kbd>/</kbd>
      </button>
      <button class="dark-mode-toggle" aria-label="Toggle dark mode" type="button">
        <svg class="icon-sun" width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round">
          <circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/>
        </svg>
        <svg class="icon-moon" width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round">
          <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"/>
        </svg>
      </button>
      <button class="nav-toggle" aria-label="Toggle menu" type="button">
        <span></span>
      </button>
    </div>
  </nav>
</header>

    <div class="search-overlay" id="searchOverlay">
      <div class="search-container">
        <div class="search-input-wrap">
          <svg width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"><circle cx="11" cy="11" r="8"/><line x1="21" y1="21" x2="16.65" y2="16.65"/></svg>
          <input type="text" class="search-input" id="searchInput" placeholder="Search articles, tools, patterns..." autocomplete="off">
        </div>
        <div class="search-results" id="searchResults"></div>
        <div class="search-hint">
          <span><kbd>Esc</kbd> close</span>
          <span><kbd>&uarr;</kbd><kbd>&darr;</kbd> navigate</span>
          <span><kbd>Enter</kbd> open</span>
        </div>
      </div>
    </div>

    <main>
<div class="page-container">
  
<nav class="breadcrumbs" aria-label="Breadcrumb">
  <ol>
    <li><a href="../index.html">Home</a></li>
    
      
    
      
        <li><a href="../tools/index.html">Tool Landscape</a></li>
      
    
    <li aria-current="page">Model Deep Dive</li>
  </ol>
</nav>



  <div class="article-layout has-toc">
    <article class="article">
      <header class="article-header">
        <h1 class="article-title">Model Deep Dive</h1>
        <div class="meta-bar">
  
  <time class="meta-date" datetime="2026-02-06">
    February 6, 2026
  </time>
  

  

  <span class="meta-reading-time">23 min read</span>

  

  <div class="meta-links">
    
    
    
    
  </div>

  
  
  
  
</div>

        
<div class="tag-pills">
  
  
  
  <a href="../tags/models.html" class="tag-pill">models</a>
  
  
  
  
  <a href="../tags/model-selection.html" class="tag-pill">model-selection</a>
  
  
  
  
  <a href="../tags/comparison.html" class="tag-pill">comparison</a>
  
  
  
  
  <a href="../tags/costs.html" class="tag-pill">costs</a>
  
  
  
  
  <a href="../tags/reasoning.html" class="tag-pill">reasoning</a>
  
  
</div>


      </header>

      <div class="article-content">
        <p>The <a href="../practices/model-selection.html">model selection</a> practice page covers the principles: start cheap, escalate on failure, use multiple models, ignore benchmarks in favor of your own workload. This page is the complement &ndash; a granular look at every model variant, what it costs, how it performs in real-world usage, and exactly how to configure reasoning levels across providers and tools.</p>
<p>The data here comes from three sources: Amp Code&rsquo;s published model evaluation data (including their distinctive &ldquo;off-the-rails&rdquo; metric), official provider documentation, and 605 model variant mentions extracted from over 6,000 Hacker News comments across 20+ discussion threads.</p>
<h2 id="practitioner-mention-heatmap">Practitioner Mention Heatmap</h2>
<p>Before diving into individual models, here is what practitioners actually talk about. The top 20 most-discussed model variants across HN, with sentiment:</p>
<table>
  <thead>
      <tr>
          <th>Model Variant</th>
          <th>Mentions</th>
          <th>Positive</th>
          <th>Negative</th>
          <th>Neutral</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>Opus 4.5</td>
          <td>158</td>
          <td>43</td>
          <td>32</td>
          <td>83</td>
      </tr>
      <tr>
          <td>GPT-5 (family)</td>
          <td>72</td>
          <td>10</td>
          <td>21</td>
          <td>41</td>
      </tr>
      <tr>
          <td>GPT-5.2</td>
          <td>39</td>
          <td>7</td>
          <td>15</td>
          <td>17</td>
      </tr>
      <tr>
          <td>Gemini 3</td>
          <td>39</td>
          <td>8</td>
          <td>10</td>
          <td>21</td>
      </tr>
      <tr>
          <td>Opus 4.6</td>
          <td>38</td>
          <td>9</td>
          <td>4</td>
          <td>25</td>
      </tr>
      <tr>
          <td>Sonnet 4 / 4.5</td>
          <td>64</td>
          <td>15</td>
          <td>15</td>
          <td>34</td>
      </tr>
      <tr>
          <td>Gemini 3 Pro</td>
          <td>13</td>
          <td>1</td>
          <td>3</td>
          <td>9</td>
      </tr>
      <tr>
          <td>GPT-OSS</td>
          <td>10</td>
          <td>2</td>
          <td>0</td>
          <td>8</td>
      </tr>
      <tr>
          <td>GPT-5.2 Codex</td>
          <td>9</td>
          <td>1</td>
          <td>2</td>
          <td>6</td>
      </tr>
      <tr>
          <td>GPT-5.1</td>
          <td>9</td>
          <td>1</td>
          <td>4</td>
          <td>4</td>
      </tr>
      <tr>
          <td>Kimi K2 / K2.5</td>
          <td>14</td>
          <td>1</td>
          <td>2</td>
          <td>11</td>
      </tr>
      <tr>
          <td>Gemini 2.5</td>
          <td>8</td>
          <td>2</td>
          <td>1</td>
          <td>5</td>
      </tr>
      <tr>
          <td>Haiku 4.5</td>
          <td>5</td>
          <td>0</td>
          <td>1</td>
          <td>4</td>
      </tr>
      <tr>
          <td>GPT-5.3 Codex</td>
          <td>5</td>
          <td>1</td>
          <td>0</td>
          <td>4</td>
      </tr>
      <tr>
          <td>Qwen 3</td>
          <td>5</td>
          <td>1</td>
          <td>0</td>
          <td>4</td>
      </tr>
  </tbody>
</table>
<p>Opus 4.5 dominates with 4x more mentions than any other single variant. The GPT-5 family is the second most discussed but skews notably negative (21 negative vs 10 positive). Opus 4.6 has the best positive-to-negative ratio among frequently mentioned models.</p>
<hr>
<h2 id="claude-variants-anthropic">Claude Variants (Anthropic)</h2>
<h3 id="claude-opus-46">Claude Opus 4.6</h3>
<table>
  <thead>
      <tr>
          <th>Spec</th>
          <th>Detail</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td><strong>Release</strong></td>
          <td>February 5, 2026</td>
      </tr>
      <tr>
          <td><strong>Model ID</strong></td>
          <td><code>claude-opus-4-6</code></td>
      </tr>
      <tr>
          <td><strong>Context</strong></td>
          <td>1M tokens (beta) / 200K standard</td>
      </tr>
      <tr>
          <td><strong>Max output</strong></td>
          <td>128K tokens</td>
      </tr>
      <tr>
          <td><strong>Pricing</strong></td>
          <td>$5/M input, $25/M output</td>
      </tr>
      <tr>
          <td><strong>1M pricing</strong></td>
          <td>$10/M input, $37.50/M output</td>
      </tr>
      <tr>
          <td><strong>Cache discount</strong></td>
          <td>90% on cached input</td>
      </tr>
  </tbody>
</table>
<p>The current Anthropic frontier. The headline feature is the 1M token context window &ndash; the first Opus-class model to support it. In needle-in-haystack testing (MRCR v2), Opus 4.6 retrieves information at 93% accuracy at 256K tokens and 76% at 1M. For comparison, Sonnet 4.5 scores only 10.8% at 256K &ndash; making Opus 4.6 roughly 4-9x more reliable at deep context retrieval.</p>
<p><strong>Benchmarks vs Opus 4.5:</strong></p>
<table>
  <thead>
      <tr>
          <th>Benchmark</th>
          <th>Opus 4.5</th>
          <th>Opus 4.6</th>
          <th>Delta</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>SWE-bench Verified</td>
          <td>80.9%</td>
          <td>80.8%</td>
          <td>-0.1%</td>
      </tr>
      <tr>
          <td>Terminal Bench</td>
          <td>59.8%</td>
          <td>65.4%</td>
          <td>+5.6%</td>
      </tr>
      <tr>
          <td>OSWorld (computer use)</td>
          <td>66.3%</td>
          <td>72.7%</td>
          <td>+6.4%</td>
      </tr>
      <tr>
          <td>MRCR v2 8-needle @256K</td>
          <td>N/A</td>
          <td>93%</td>
          <td>New</td>
      </tr>
      <tr>
          <td>MRCR v2 8-needle @1M</td>
          <td>N/A</td>
          <td>76%</td>
          <td>New</td>
      </tr>
  </tbody>
</table>
<p><strong>New capabilities:</strong> Adaptive thinking (model decides when to reason deeply vs respond quickly), effort parameter with exclusive <code>max</code> level, agent teams for multi-instance parallel work, interleaved thinking between tool calls, and improved self-correction during code review.</p>
<p><strong>Practitioner reception (38 HN mentions, 9 positive, 4 negative):</strong> Mixed but leaning positive. One practitioner sent 900 poems in Portuguese and received &ldquo;an impeccable analysis.&rdquo; Another described one-shotting a Gameboy emulator. The negative reports focus on writing quality regression &ndash; one user called it &ldquo;nerfed&rdquo; for prose tasks. The practical advice from practitioners: use 4.6 for coding and agentic work, keep 4.5 available for writing-heavy tasks.</p>
<p><strong>1M context availability:</strong> API and Claude Code pay-as-you-go users only at launch. Not available for Pro, Max, Teams, or Enterprise subscription users initially. Enable with <code>[1m]</code> suffix: <code>/model opus[1m]</code>.</p>
<p><strong>Tool access:</strong> Default model on Claude Code Max and Teams plans. <a href="../tools/claude-code.html">Claude Code</a> automatically falls back to Sonnet when Opus usage threshold is hit on Pro plans.</p>
<h3 id="claude-opus-45">Claude Opus 4.5</h3>
<table>
  <thead>
      <tr>
          <th>Spec</th>
          <th>Detail</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td><strong>Release</strong></td>
          <td>November 24, 2025</td>
      </tr>
      <tr>
          <td><strong>Model ID</strong></td>
          <td><code>claude-opus-4-5-20251101</code></td>
      </tr>
      <tr>
          <td><strong>Context</strong></td>
          <td>200K tokens</td>
      </tr>
      <tr>
          <td><strong>Max output</strong></td>
          <td>64K tokens</td>
      </tr>
      <tr>
          <td><strong>Pricing</strong></td>
          <td>$5/M input, $25/M output (67% cut from Opus 4.1&rsquo;s $15/$75)</td>
      </tr>
  </tbody>
</table>
<p>The model that changed practitioner sentiment. First model to exceed 80% on SWE-bench Verified (80.9%). Best across 7 of 8 programming languages on SWE-bench Multilingual. The 67% price reduction from Opus 4.1 was arguably more disruptive than the capability gains &ndash; it made frontier-class coding available at mainstream pricing.</p>
<p><strong>Key features:</strong> Effort parameter (low/medium/high) for trading thoroughness for speed. At medium effort, matches Sonnet 4.5&rsquo;s SWE-bench score using 76% fewer output tokens. Tool Search discovers tools on-demand, reducing context overhead by 85%. Auto-compaction at 95% context window effectively removes the context limit.</p>
<p><strong>Amp&rsquo;s evaluation data (Opus 4.5 as Smart mode):</strong></p>
<table>
  <thead>
      <tr>
          <th>Metric</th>
          <th>Opus 4.5</th>
          <th>vs Sonnet 4.5</th>
          <th>vs Gemini 3 Pro</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>Internal Evals</td>
          <td>57.3%</td>
          <td>+20.2%</td>
          <td>+3.6%</td>
      </tr>
      <tr>
          <td>Avg Thread Cost</td>
          <td>$2.05</td>
          <td>-$0.70 cheaper</td>
          <td>+$0.01</td>
      </tr>
      <tr>
          <td>Off-the-Rails Cost</td>
          <td>2.4%</td>
          <td>3.5x less waste</td>
          <td>7.4x less waste</td>
      </tr>
      <tr>
          <td>Speed (p50)</td>
          <td>3.5 min</td>
          <td>+1.1 min slower</td>
          <td>0.8 min faster</td>
      </tr>
  </tbody>
</table>
<p><strong>Practitioner reception (158 HN mentions &ndash; by far the most discussed model):</strong> The most polarizing model in recent memory. Champions describe it as &ldquo;an inflection point&rdquo; and &ldquo;a visible step change.&rdquo; One practitioner reported 259 PRs and 497 commits in 30 days. Another built twelve iOS apps in two weeks. Critics report it &ldquo;ate through my Copilot quota,&rdquo; produces &ldquo;baffling architecture decisions,&rdquo; and &ldquo;is not much different than any previous models.&rdquo; The split appears to correlate with domain: React/Rust/C# developers skew positive, while those working with legacy codebases, data science, or novel domains report more frustration.</p>
<h3 id="claude-sonnet-45">Claude Sonnet 4.5</h3>
<table>
  <thead>
      <tr>
          <th>Spec</th>
          <th>Detail</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td><strong>Release</strong></td>
          <td>September 29, 2025</td>
      </tr>
      <tr>
          <td><strong>Model ID</strong></td>
          <td><code>claude-sonnet-4-5-20250929</code></td>
      </tr>
      <tr>
          <td><strong>Context</strong></td>
          <td>200K tokens (1M available via API)</td>
      </tr>
      <tr>
          <td><strong>Max output</strong></td>
          <td>64K tokens</td>
      </tr>
      <tr>
          <td><strong>Pricing</strong></td>
          <td>$3/M input, $15/M output</td>
      </tr>
  </tbody>
</table>
<p>Anthropic&rsquo;s most popular model by volume. The workhorse for daily coding where speed matters more than peak capability. SWE-bench Verified: 77.2%. Handles 90% of coding tasks without difficulty, per Claude Code documentation.</p>
<p><strong>The counterintuitive cost finding:</strong> Despite being cheaper per token, Amp found Sonnet actually costs more per completed thread ($2.75) than Opus ($2.05). It uses more tokens, makes more mistakes, and requires more human intervention. The cheapest model per token is not always the cheapest model per completed task.</p>
<p><strong>Where it fits:</strong> Default execution model in <code>opusplan</code> mode (Opus plans, Sonnet executes). Claude Code falls back to Sonnet when Opus quota is hit on Pro plans. First non-frontier model with strong extended thinking capabilities. Best suited for implementation tasks within an already-defined architecture.</p>
<p><strong>Extended thinking:</strong> Supports manual extended thinking with <code>budget_tokens</code>. Interleaved thinking available with beta header <code>interleaved-thinking-2025-05-14</code>.</p>
<h3 id="claude-haiku-45">Claude Haiku 4.5</h3>
<table>
  <thead>
      <tr>
          <th>Spec</th>
          <th>Detail</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td><strong>Release</strong></td>
          <td>October 15, 2025</td>
      </tr>
      <tr>
          <td><strong>Model ID</strong></td>
          <td><code>claude-haiku-4-5-20251001</code></td>
      </tr>
      <tr>
          <td><strong>Context</strong></td>
          <td>200K tokens</td>
      </tr>
      <tr>
          <td><strong>Max output</strong></td>
          <td>64K tokens</td>
      </tr>
      <tr>
          <td><strong>Pricing</strong></td>
          <td>$1/M input, $5/M output</td>
      </tr>
  </tbody>
</table>
<p>The speed tier. Fastest model in the Claude family, optimized for lowest initial latency. SWE-bench Verified: 73.3% &ndash; within 5 points of best-in-class at 1/3 the cost. First &ldquo;small&rdquo; model to support extended thinking.</p>
<p><strong>Where practitioners use it:</strong> Subagent tasks (summarization, validation, entity extraction), codebase exploration in Claude Code (powers the Explore subagent), background token operations in Claude Code, and as the junior developer in multi-agent teams. Amp uses it for Rush mode (small, well-defined tasks) and Titling (thread title generation).</p>
<p><strong>Rush mode performance (Amp data):</strong> Token-by-token, 67% cheaper and 50% faster than Smart mode. A small task: 37 seconds at $0.12 (44% faster, 77% cheaper than Smart). But on a complex refactor: 2x longer than Smart and only 19% cheaper &ndash; it spends more tokens fixing its own mistakes.</p>
<p><strong>vs Gemini Flash:</strong> Haiku excels at agent orchestration and tool use. Gemini Flash offers broader capabilities and larger context (1M). For live coding, Haiku&rsquo;s total cost-to-solution tends to be lower due to fewer retries and stalls.</p>
<hr>
<h2 id="gpt-variants-openai">GPT Variants (OpenAI)</h2>
<h3 id="gpt-53-codex">GPT-5.3 Codex</h3>
<table>
  <thead>
      <tr>
          <th>Spec</th>
          <th>Detail</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td><strong>Release</strong></td>
          <td>February 5, 2026</td>
      </tr>
      <tr>
          <td><strong>Context</strong></td>
          <td>400K tokens</td>
      </tr>
      <tr>
          <td><strong>Pricing</strong></td>
          <td>~$1.75/M input, ~$14/M output (API pricing TBD)</td>
      </tr>
      <tr>
          <td><strong>Reasoning levels</strong></td>
          <td>low, medium, high, xhigh</td>
      </tr>
      <tr>
          <td><strong>Speed</strong></td>
          <td>~233 tokens/sec at high reasoning</td>
      </tr>
  </tbody>
</table>
<p>The first unified model combining the Codex and GPT-5 training stacks. 25% faster than GPT-5.2-Codex while using fewer tokens for equivalent tasks. Can be steered interactively while working without losing context.</p>
<p><strong>Benchmarks:</strong></p>
<table>
  <thead>
      <tr>
          <th>Benchmark</th>
          <th>GPT-5.2-Codex</th>
          <th>GPT-5.3-Codex</th>
          <th>Delta</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>SWE-Bench Pro</td>
          <td>56.4%</td>
          <td>56.8%</td>
          <td>+0.4%</td>
      </tr>
      <tr>
          <td>Terminal-Bench 2.0</td>
          <td>64.0%</td>
          <td>77.3%</td>
          <td>+13.3%</td>
      </tr>
      <tr>
          <td>OSWorld-Verified</td>
          <td>&ndash;</td>
          <td>64.7%</td>
          <td>New</td>
      </tr>
      <tr>
          <td>Cybersecurity CTF</td>
          <td>79-80%</td>
          <td>77.6%</td>
          <td>~same</td>
      </tr>
  </tbody>
</table>
<p>The Terminal-Bench jump from 64% to 77.3% is the standout number &ndash; a massive gain in real-world terminal/CLI task completion. First model rated &ldquo;High capability&rdquo; in cybersecurity under OpenAI&rsquo;s Preparedness Framework.</p>
<p><strong>Key improvements over 5.2-Codex:</strong> Fixed lint loops (models getting stuck in lint-fix cycles), better bug explanations, fixed flaky-test premature completion, improved codebase coherence, deep diffs for reasoning transparency.</p>
<p><strong>Practitioner reception (5 HN mentions, early):</strong> One practitioner noted both GPT-5.3-Codex and Opus 4.6 &ldquo;one shot a Gameboy emulator.&rdquo; Another observed the Terminal-Bench lead lasted only 35 minutes before Opus 4.6 launched.</p>
<h3 id="gpt-52-codex">GPT-5.2 Codex</h3>
<table>
  <thead>
      <tr>
          <th>Spec</th>
          <th>Detail</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td><strong>Release</strong></td>
          <td>January 14, 2026</td>
      </tr>
      <tr>
          <td><strong>Context</strong></td>
          <td>400K tokens</td>
      </tr>
      <tr>
          <td><strong>Pricing</strong></td>
          <td>$1.75/M input, $14/M output</td>
      </tr>
      <tr>
          <td><strong>Cached input</strong></td>
          <td>90% discount</td>
      </tr>
      <tr>
          <td><strong>Reasoning levels</strong></td>
          <td>low, medium, high, xhigh</td>
      </tr>
  </tbody>
</table>
<p>The model that made Codex CLI competitive again. Context compaction automatically compresses older context into semantically faithful summaries, enabling coherent work across millions of tokens. Optimized for long-horizon tasks like refactors and migrations.</p>
<p><strong>Role in Amp:</strong> Serves as both Deep mode (autonomous coding with extended thinking, works silently for 5-15 minutes) and Oracle (read-only planning/debugging advisor at medium reasoning). Amp chose medium reasoning for Oracle &ndash; balancing analytical depth against speed and cost. GPT-5.2&rsquo;s different training lineage makes it complementary to Claude as a second-opinion model.</p>
<p><strong>Practitioner reception (9 HN mentions):</strong> &ldquo;Reliably good&rdquo; in Codex CLI, earning &ldquo;the default position&rdquo; for one practitioner. Another found GPT-5.2 High and Opus 4.5 to be complementary: &ldquo;they find different things.&rdquo; The negative signal: reports of progressive quality degradation over time, and some developers shifting to Claude or Gemini in response.</p>
<h3 id="gpt-52-base">GPT-5.2 (Base)</h3>
<table>
  <thead>
      <tr>
          <th>Spec</th>
          <th>Detail</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td><strong>Release</strong></td>
          <td>December 11, 2025</td>
      </tr>
      <tr>
          <td><strong>Pricing</strong></td>
          <td>$1.75/M input, $14/M output</td>
      </tr>
      <tr>
          <td><strong>Cached input</strong></td>
          <td>90% discount ($0.175/M)</td>
      </tr>
      <tr>
          <td><strong>Speed</strong></td>
          <td>92 tokens/sec at xhigh reasoning</td>
      </tr>
  </tbody>
</table>
<p>Three-tier architecture: Instant (fast, 200-800ms responses), Thinking (professional work, configurable reasoning), and Pro (mission-critical, ~10x cost of Thinking). AIME 2025: 100% without tools. GDPval: 70.9% wins/ties vs top industry professionals across 44 occupations.</p>
<p><strong>Practitioner reception (39 HN mentions, 7 positive, 15 negative):</strong> The most negatively-received GPT variant. Practitioners describe a gap between strong benchmarks and user experience. One called it &ldquo;everything I hate about 5 and 5.1, but worse.&rdquo; Reports of hallucination and progressive quality decline. However, those who use it at high reasoning for specific tasks &ndash; particularly code review and debugging &ndash; report strong results. The consensus: GPT-5.2 is strong at analysis but weaker than Opus at autonomous coding.</p>
<h3 id="gpt-51">GPT-5.1</h3>
<table>
  <thead>
      <tr>
          <th>Spec</th>
          <th>Detail</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td><strong>Release</strong></td>
          <td>November 12, 2025</td>
      </tr>
      <tr>
          <td><strong>Pricing</strong></td>
          <td>$1.25/M input, $10/M output</td>
      </tr>
      <tr>
          <td><strong>Cached input</strong></td>
          <td>90% discount</td>
      </tr>
  </tbody>
</table>
<p>Warmer personality than 5.0, with adaptive reasoning and customizable personalities (8 options). Pricing unchanged from GPT-5.</p>
<p><strong>GPT-5.1-Codex-Max (November 19, 2025):</strong> The agentic specialist. SWE-Bench Verified: 77.9% (with xhigh reasoning). Observed working autonomously on tasks for 24+ hours. Supports low, medium, high, and xhigh reasoning levels. Terminal-Bench 2.0: 58.1%.</p>
<p><strong>Practitioner reception (9 HN mentions, skewing negative):</strong> GPT-5.1 was caught hallucinating its own product roadmap. Some developers reported quality degradation across the 5.x line.</p>
<h3 id="gpt-50">GPT-5.0</h3>
<table>
  <thead>
      <tr>
          <th>Spec</th>
          <th>Detail</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td><strong>Release</strong></td>
          <td>August 7, 2025</td>
      </tr>
      <tr>
          <td><strong>Context</strong></td>
          <td>400K tokens</td>
      </tr>
      <tr>
          <td><strong>Pricing</strong></td>
          <td>$1.25/M input, $10/M output</td>
      </tr>
      <tr>
          <td><strong>Variants</strong></td>
          <td>gpt-5, gpt-5-mini ($0.25/$2), gpt-5-nano ($0.05/$0.40)</td>
      </tr>
  </tbody>
</table>
<p>The unified model that replaced GPT-4, GPT-4o, GPT-4.1, GPT-4.5, o3, and o4-mini. Internal router system automatically chooses between fast and deep thinking modes. Hallucination rate: 9.6% (down from GPT-4o&rsquo;s 12.9%).</p>
<p><strong>Reception was notably rough:</strong> Users had no control over the router at launch. Many preferred GPT-4o&rsquo;s warmer tone. The 5.0 release had a 15% failure rate on structured formatting tasks where Mistral achieved 0.1%, and timeout rates above 50% with six-minute limits. Amp evaluated GPT-5 as a primary agent and rejected it after one week, citing slow reasoning, research loops, poor tool selection, and invalid JSON for tool arguments.</p>
<h3 id="gpt-41-family">GPT-4.1 Family</h3>
<p>Still relevant for cost-sensitive production workloads needing massive context.</p>
<table>
  <thead>
      <tr>
          <th>Variant</th>
          <th>Input $/M</th>
          <th>Output $/M</th>
          <th>Context</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>GPT-4.1</td>
          <td>$2.00</td>
          <td>$8.00</td>
          <td>1M tokens</td>
      </tr>
      <tr>
          <td>GPT-4.1 mini</td>
          <td>$0.40</td>
          <td>$1.60</td>
          <td>1M tokens</td>
      </tr>
      <tr>
          <td>GPT-4.1 nano</td>
          <td>$0.10</td>
          <td>$0.40</td>
          <td>1M tokens</td>
      </tr>
  </tbody>
</table>
<p>No reasoning levels (pre-dates the reasoning effort system). GPT-4.1 nano at $0.10/$0.40 remains one of the cheapest capable models available for classification and extraction tasks.</p>
<hr>
<h2 id="gemini-variants-google">Gemini Variants (Google)</h2>
<h3 id="gemini-3-pro">Gemini 3 Pro</h3>
<table>
  <thead>
      <tr>
          <th>Spec</th>
          <th>Detail</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td><strong>Release</strong></td>
          <td>November 18, 2025</td>
      </tr>
      <tr>
          <td><strong>Context</strong></td>
          <td>1M input / 64K output</td>
      </tr>
      <tr>
          <td><strong>Pricing (standard)</strong></td>
          <td>$2.00/M input, $12/M output</td>
      </tr>
      <tr>
          <td><strong>Pricing (&gt;200K)</strong></td>
          <td>$4.00/M input, $18/M output</td>
      </tr>
      <tr>
          <td><strong>Thinking</strong></td>
          <td><code>thinkingLevel</code>: high (default), low</td>
      </tr>
      <tr>
          <td><strong>Deep Think</strong></td>
          <td>Available to AI Ultra subscribers</td>
      </tr>
  </tbody>
</table>
<p>Top of the LMSYS Chatbot Arena at 1501 Elo. AIME 2025: 100% with code execution. SWE-Bench Verified: 76.2%. Described by Amp as having &ldquo;impressively clever&rdquo; tool deployment and &ldquo;uncannily good&rdquo; prose writing.</p>
<p><strong>The reliability problem:</strong> Despite top benchmark scores, Gemini 3 Pro exhibited serious production issues when Amp adopted it as their primary agent. Documented problems included infinite thinking loops (3-5% of requests at scale), thinking prose leaking into outputs, control character corruption, reluctance to execute bash commands, unrequested git commits despite explicit instructions, and using relative paths instead of absolute. Amp switched away after approximately one week.</p>
<p><strong>Amp&rsquo;s evaluation:</strong></p>
<table>
  <thead>
      <tr>
          <th>Metric</th>
          <th>Gemini 3 Pro</th>
          <th>vs Opus 4.5</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>Internal Evals</td>
          <td>53.7%</td>
          <td>-3.6%</td>
      </tr>
      <tr>
          <td>Thread Cost</td>
          <td>$2.04</td>
          <td>-$0.01 cheaper</td>
      </tr>
      <tr>
          <td>Off-the-Rails</td>
          <td>17.8%</td>
          <td>7.4x more waste</td>
      </tr>
      <tr>
          <td>Speed (p50)</td>
          <td>4.3 min</td>
          <td>0.8 min slower</td>
      </tr>
  </tbody>
</table>
<p>The off-the-rails metric tells the story: nearly 1 in 5 dollars went to problematic outputs. The model is capable at its best but unpredictable at its worst.</p>
<p><strong>Current Amp role:</strong> Review agent (code review with agentic depth). Also available via <a href="../tools/gemini-cli.html">Gemini CLI</a> as <code>gemini-3-pro-preview</code>.</p>
<h3 id="gemini-3-flash">Gemini 3 Flash</h3>
<table>
  <thead>
      <tr>
          <th>Spec</th>
          <th>Detail</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td><strong>Release</strong></td>
          <td>December 17, 2025</td>
      </tr>
      <tr>
          <td><strong>Context</strong></td>
          <td>1M input / 64K output</td>
      </tr>
      <tr>
          <td><strong>Pricing</strong></td>
          <td>$0.50/M input, $3/M output</td>
      </tr>
      <tr>
          <td><strong>Speed</strong></td>
          <td>218 tokens/sec</td>
      </tr>
      <tr>
          <td><strong>Thinking</strong></td>
          <td><code>thinkingLevel</code>: high (default), medium, low, minimal</td>
      </tr>
  </tbody>
</table>
<p>The parallel processing specialist. Achieves ~8 parallel tool calls per iteration versus Haiku 4.5&rsquo;s ~2.5, completing searches in ~3 turns instead of ~9. This makes it 3x faster than Haiku for codebase search at the same quality.</p>
<p><strong>Amp roles:</strong> Search subagent (codebase retrieval) and Look At system (image/PDF/media analysis). Replaced Haiku 4.5 as Search agent in December 2025.</p>
<p><strong>Agentic Vision (February 2026):</strong> Converts image understanding from static analysis to an agentic process. The model formulates a plan for how to inspect an image, combining visual reasoning with Python code execution. Delivers 5-10% quality boost across vision benchmarks.</p>
<p><strong>Known issues:</strong> Can experience infinite reasoning loops at scale (3-5% of requests). Latency degrades significantly with &gt;500K token prompts (from 1-2s to 8-12s). 22% slower raw token output than Gemini 2.5 Flash.</p>
<h3 id="gemini-25-flash">Gemini 2.5 Flash</h3>
<table>
  <thead>
      <tr>
          <th>Spec</th>
          <th>Detail</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td><strong>Context</strong></td>
          <td>1M tokens</td>
      </tr>
      <tr>
          <td><strong>Pricing</strong></td>
          <td>$0.15/M input, $0.60/M output (non-thinking), $3.50/M output (thinking)</td>
      </tr>
      <tr>
          <td><strong>Speed</strong></td>
          <td>392.8 tokens/sec, 0.29s time-to-first-token</td>
      </tr>
      <tr>
          <td><strong>Thinking</strong></td>
          <td><code>thinkingBudget</code>: 0 to 24,576 tokens; default: dynamic (-1)</td>
      </tr>
  </tbody>
</table>
<p>One of the fastest production-grade models available. Still used by Amp as the Handoff system model (context analysis for task continuation). Default model for simple prompts in Gemini CLI auto-routing.</p>
<p><strong>Cost warning:</strong> Thinking mode makes it dramatically more expensive &ndash; 150x versus Gemini 2.0 Flash due to 9x more expensive output tokens combined with 17x higher token usage.</p>
<h3 id="gemini-25-flash-lite">Gemini 2.5 Flash-Lite</h3>
<table>
  <thead>
      <tr>
          <th>Spec</th>
          <th>Detail</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td><strong>Release</strong></td>
          <td>July 22, 2025</td>
      </tr>
      <tr>
          <td><strong>Context</strong></td>
          <td>1M tokens</td>
      </tr>
      <tr>
          <td><strong>Pricing</strong></td>
          <td>$0.10/M input, $0.40/M output</td>
      </tr>
      <tr>
          <td><strong>Speed</strong></td>
          <td>392.8 tokens/sec</td>
      </tr>
      <tr>
          <td><strong>Thinking</strong></td>
          <td>Off by default (can be enabled)</td>
      </tr>
  </tbody>
</table>
<p>Google&rsquo;s cheapest model in the 2.5 family. 50% less verbose output than standard Flash. Used by Amp as the Topics system model (thread categorization for indexing and analytics). Ideal for high-volume classification and routing tasks.</p>
<h3 id="gemini-3-pro-image">Gemini 3 Pro Image</h3>
<table>
  <thead>
      <tr>
          <th>Spec</th>
          <th>Detail</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td><strong>Pricing</strong></td>
          <td>$0.134/image (2K), $0.24/image (4K)</td>
      </tr>
      <tr>
          <td><strong>Capabilities</strong></td>
          <td>Generation, editing, text rendering, physics control</td>
      </tr>
      <tr>
          <td><strong>Resolution</strong></td>
          <td>Up to 4K output</td>
      </tr>
  </tbody>
</table>
<p>Separate model optimized for image generation and editing, built on Gemini 3 Pro&rsquo;s reasoning. Supports up to 14 reference images. Used by Amp as the Painter system model. Preview-stage reliability: ~45% of API calls during peak hours result in errors.</p>
<hr>
<h2 id="open-weight-models">Open-Weight Models</h2>
<h3 id="deepseek-v3--v31">DeepSeek V3 / V3.1</h3>
<table>
  <thead>
      <tr>
          <th>Spec</th>
          <th>Detail</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td><strong>Parameters</strong></td>
          <td>671B total (MoE), 37B active</td>
      </tr>
      <tr>
          <td><strong>Context</strong></td>
          <td>128K tokens</td>
      </tr>
      <tr>
          <td><strong>API pricing (official)</strong></td>
          <td>$0.07/M input (cache hit), $0.56/M (miss), $1.68/M output</td>
      </tr>
      <tr>
          <td><strong>Training cost</strong></td>
          <td>~$5.6M (2.788M H800 GPU hours)</td>
      </tr>
  </tbody>
</table>
<p>The most capable open-weight option for general coding. SWE-bench Verified: 45.4% (V3-0324). The V3-0324 update brought major gains: AIME jumped from 39.6% to 59.4% (+19.8), LiveCodeBench from 39.2% to 49.2%.</p>
<p><strong>Self-hosting economics:</strong> Running the full 671B model on consumer hardware (dual RTX 5090s) costs roughly $30K over three years including electricity, yielding $30-60 per million tokens &ndash; more expensive than cloud API pricing. The breakeven only works at sustained high volume or when data sovereignty is the primary concern.</p>
<p><strong>Hardware requirements:</strong> FP8 (recommended): ~750 GB VRAM, minimum 8x H100 80GB. Consumer-grade: not feasible for full model; distilled variants (7B, 16B) run on RTX 4090.</p>
<h3 id="deepseek-r1">DeepSeek R1</h3>
<table>
  <thead>
      <tr>
          <th>Spec</th>
          <th>Detail</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td><strong>Parameters</strong></td>
          <td>671B total (MoE), 37B active</td>
      </tr>
      <tr>
          <td><strong>Context</strong></td>
          <td>128K tokens</td>
      </tr>
      <tr>
          <td><strong>Architecture</strong></td>
          <td>MoE with RL-based reasoning training</td>
      </tr>
  </tbody>
</table>
<p>The reasoning specialist. Differs from V3 in purpose: V3 is the fast general-purpose model, R1 is the specialized reasoner. Exposes step-by-step chain-of-thought in <code>&lt;think&gt;</code> tags, enabling verification.</p>
<p><strong>Distilled variants for local use:</strong></p>
<table>
  <thead>
      <tr>
          <th>Variant</th>
          <th>Parameters</th>
          <th>Hardware Minimum</th>
          <th>Best For</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>R1-Distill-Qwen-7B</td>
          <td>7B</td>
          <td>8GB VRAM (INT4)</td>
          <td>Consumer GPU, rapid iteration</td>
      </tr>
      <tr>
          <td>R1-Distill-Qwen-14B</td>
          <td>14B</td>
          <td>16GB VRAM (INT4)</td>
          <td>Mid-range, balanced</td>
      </tr>
      <tr>
          <td>R1-Distill-Qwen-32B</td>
          <td>32B</td>
          <td>32GB VRAM (INT4)</td>
          <td>Best reasoning among mid-tier</td>
      </tr>
      <tr>
          <td>R1-Distill-Llama-70B</td>
          <td>70B</td>
          <td>70GB VRAM (INT4)</td>
          <td>Maximum distilled performance</td>
      </tr>
  </tbody>
</table>
<h3 id="llama-4-scout--maverick">Llama 4 Scout / Maverick</h3>
<table>
  <thead>
      <tr>
          <th>Variant</th>
          <th>Total Params</th>
          <th>Active Params</th>
          <th>Context</th>
          <th>Experts</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>Scout</td>
          <td>109B</td>
          <td>17B</td>
          <td>10M tokens</td>
          <td>16</td>
      </tr>
      <tr>
          <td>Maverick</td>
          <td>400B</td>
          <td>17B</td>
          <td>1M tokens</td>
          <td>128</td>
      </tr>
  </tbody>
</table>
<p>Released April 5, 2025. Scout runs on a single H100 (INT4 quantized) with an industry-leading 10M token context window. Maverick needs multi-GPU but delivers substantially higher quality with 128 experts. Both use early fusion multimodality.</p>
<p><strong>API pricing:</strong> Scout at $0.11/M (Groq) is among the cheapest capable models. Maverick at $0.50/M (Groq) trades cost for quality. Behemoth (~2T parameters) exists only in limited research preview.</p>
<h3 id="qwen-3-family">Qwen 3 Family</h3>
<p>The efficiency story: Qwen3-8B outperforms Qwen2.5-14B. Qwen3-32B matches Qwen2.5-72B. A generational improvement in parameter efficiency through strong-to-weak distillation.</p>
<table>
  <thead>
      <tr>
          <th>Variant</th>
          <th>Params</th>
          <th>Active</th>
          <th>LiveCodeBench</th>
          <th>Best For</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>Qwen3-8B</td>
          <td>8B dense</td>
          <td>8B</td>
          <td>60.2</td>
          <td>RTX 3060, rapid iteration</td>
      </tr>
      <tr>
          <td>Qwen3-30B-A3B (MoE)</td>
          <td>30.5B</td>
          <td>3.3B</td>
          <td>&ndash;</td>
          <td>Local inference, consumer hardware</td>
      </tr>
      <tr>
          <td>Qwen3-32B</td>
          <td>32B dense</td>
          <td>32B</td>
          <td>&ndash;</td>
          <td>Single H100 balance of quality/speed</td>
      </tr>
      <tr>
          <td>Qwen3-Coder-480B-A35B</td>
          <td>480B</td>
          <td>35B</td>
          <td>&ndash;</td>
          <td>Maximum open-weight coding (SWE: 66.5%)</td>
      </tr>
  </tbody>
</table>
<p>Qwen3-30B-A3B is the standout for local use: outperforms every dense 72B-110B model on coding and math while running on 32GB RAM (INT4). Achieves 100+ tokens/sec on Apple M4 Max.</p>
<h3 id="mistral">Mistral</h3>
<table>
  <thead>
      <tr>
          <th>Variant</th>
          <th>Params</th>
          <th>Input $/M</th>
          <th>Output $/M</th>
          <th>SWE-bench</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>Medium 3</td>
          <td>MoE</td>
          <td>$0.40</td>
          <td>$2.00</td>
          <td>&ndash;</td>
      </tr>
      <tr>
          <td>Small 3</td>
          <td>24B</td>
          <td>&ndash;</td>
          <td>&ndash;</td>
          <td>&ndash;</td>
      </tr>
      <tr>
          <td>Devstral 2</td>
          <td>123B dense</td>
          <td>$0.40</td>
          <td>$2.00</td>
          <td>72.2%</td>
      </tr>
      <tr>
          <td>Devstral Small 2</td>
          <td>24B</td>
          <td>$0.10</td>
          <td>$0.30</td>
          <td>68.0%</td>
      </tr>
  </tbody>
</table>
<p>Mistral&rsquo;s standout claim: a 0.1% structured output failure rate on formatting tasks where GPT-5 failed 15% of the time. Devstral Small 2 is the best open-weight coding model at its size (24B) &ndash; runs on a single RTX 4090 with 68% SWE-bench.</p>
<h3 id="kimi-k25">Kimi K2.5</h3>
<table>
  <thead>
      <tr>
          <th>Spec</th>
          <th>Detail</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td><strong>Parameters</strong></td>
          <td>1T total (MoE), 32B active</td>
      </tr>
      <tr>
          <td><strong>Context</strong></td>
          <td>256K tokens</td>
      </tr>
      <tr>
          <td><strong>API pricing</strong></td>
          <td>$0.60/M input, $3/M output</td>
      </tr>
      <tr>
          <td><strong>SWE-bench Verified</strong></td>
          <td>76.8%</td>
      </tr>
      <tr>
          <td><strong>LiveCodeBench</strong></td>
          <td>85.0%</td>
      </tr>
  </tbody>
</table>
<p>From Moonshot AI. The LiveCodeBench score of 85.0% significantly exceeds Opus 4.5&rsquo;s 64.0%, though Opus leads on SWE-bench Verified (80.9% vs 76.8%). Specialized strength in visual coding &ndash; generating code from UI designs and wireframes. Can self-direct up to 100 sub-agents with 1,500 tool calls.</p>
<p><strong>Cost position:</strong> 76% lower than Opus 4.5 for comparable coding tasks. Best suited for cost-sensitive deployments, parallel workflows, and vision-based frontend development.</p>
<hr>
<h2 id="reasoning-level-deep-dive">Reasoning Level Deep Dive</h2>
<p>All three major providers now offer configurable reasoning depth. Understanding these settings is one of the highest-leverage optimizations for both cost and quality.</p>
<h3 id="cross-provider-comparison">Cross-Provider Comparison</h3>
<table>
  <thead>
      <tr>
          <th>Provider</th>
          <th>Parameter</th>
          <th>Levels</th>
          <th>Default</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>OpenAI</td>
          <td><code>reasoning.effort</code></td>
          <td>none, minimal, low, medium, high, xhigh</td>
          <td>medium (pre-5.1), none (5.1+)</td>
      </tr>
      <tr>
          <td>Anthropic</td>
          <td><code>effort</code></td>
          <td>low, medium, high, max</td>
          <td>high</td>
      </tr>
      <tr>
          <td>Anthropic (legacy)</td>
          <td><code>budget_tokens</code></td>
          <td>1,024 to 128K</td>
          <td>31,999 (Claude Code)</td>
      </tr>
      <tr>
          <td>Google (2.5 models)</td>
          <td><code>thinkingBudget</code></td>
          <td>0 to 32,768</td>
          <td>8,192 (Pro), -1/dynamic (Flash)</td>
      </tr>
      <tr>
          <td>Google (3.x models)</td>
          <td><code>thinkingLevel</code></td>
          <td>minimal, low, medium, high</td>
          <td>high</td>
      </tr>
  </tbody>
</table>
<h3 id="openai-reasoning-effort">OpenAI Reasoning Effort</h3>
<p>Six levels from <code>none</code> (traditional LLM, sub-second responses) through <code>xhigh</code> (maximum compute, available on GPT-5.2-Codex and GPT-5.2 Pro). Reasoning tokens are billed as output tokens at the standard rate &ndash; no separate multiplier, but higher effort means more tokens consumed.</p>
<p><strong>Latency scaling:</strong> Low is often under 1 second. Medium is ~3x longer. High is ~3x longer than medium (~9x longer than low). xhigh adds further latency on top.</p>
<p><strong>Amp&rsquo;s Oracle configuration:</strong> GPT-5.2 with medium reasoning &ndash; found to balance analytical depth against speed and cost for planning and debugging tasks.</p>
<h3 id="claude-effort-parameter">Claude Effort Parameter</h3>
<p>Four levels: low, medium, high (default), and max (Opus 4.6 exclusive). The effort parameter is a behavioral signal, not a strict token budget. At lower effort, Claude will still think on sufficiently difficult problems &ndash; it just thinks less.</p>
<p><strong>Adaptive thinking (Opus 4.6):</strong> <code>thinking: {type: &quot;adaptive&quot;}</code> lets the model dynamically allocate reasoning based on task difficulty. Recommended over manual <code>budget_tokens</code> on Opus 4.6. Combined with the effort parameter for best results.</p>
<p><strong>Extended thinking costs:</strong> Thinking tokens are billed at the model&rsquo;s output token rate ($25/M for Opus, $15/M for Sonnet, $5/M for Haiku). You are charged for full thinking tokens, not the summarized version &ndash; billed output count will not match visible token count.</p>
<h3 id="gemini-thinking-configuration">Gemini Thinking Configuration</h3>
<p>Two systems depending on model generation. Gemini 2.5 models use <code>thinkingBudget</code> (0 to 32,768 tokens, or -1 for dynamic). Gemini 3 models use <code>thinkingLevel</code> (high, medium, low, minimal). Cannot disable thinking entirely on Gemini 3 Pro. Gemini 3 Flash&rsquo;s <code>minimal</code> level means the model likely will not think but still can.</p>
<h3 id="per-task-reasoning-recommendations">Per-Task Reasoning Recommendations</h3>
<table>
  <thead>
      <tr>
          <th>Task Type</th>
          <th>OpenAI</th>
          <th>Claude</th>
          <th>Gemini</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>Quick edits, formatting</td>
          <td>none or low</td>
          <td>low</td>
          <td>minimal or low</td>
      </tr>
      <tr>
          <td>Daily coding (default)</td>
          <td>medium</td>
          <td>medium or high</td>
          <td>medium (Flash) or high</td>
      </tr>
      <tr>
          <td>Complex debugging, architecture</td>
          <td>high</td>
          <td>high</td>
          <td>high</td>
      </tr>
      <tr>
          <td>Benchmarks, critical reviews</td>
          <td>xhigh</td>
          <td>max (Opus 4.6)</td>
          <td>high + Deep Think</td>
      </tr>
      <tr>
          <td>Simple completions</td>
          <td>none (GPT-5.1+)</td>
          <td>low</td>
          <td>minimal</td>
      </tr>
  </tbody>
</table>
<h3 id="tool-specific-configuration">Tool-Specific Configuration</h3>
<table>
  <thead>
      <tr>
          <th>Tool</th>
          <th>Model Config</th>
          <th>Reasoning Config</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td><a href="../tools/codex.html">Codex CLI</a></td>
          <td>config.toml profiles, <code>/model</code></td>
          <td><code>model_reasoning_effort</code> in config</td>
      </tr>
      <tr>
          <td><a href="../tools/claude-code.html">Claude Code</a></td>
          <td><code>/model</code> with effort slider</td>
          <td><code>effortLevel</code> in settings, <code>CLAUDE_CODE_EFFORT_LEVEL</code> env var</td>
      </tr>
      <tr>
          <td><a href="../tools/gemini-cli.html">Gemini CLI</a></td>
          <td>settings.json, <code>--model</code> flag</td>
          <td><code>thinkingBudget</code> / <code>thinkingLevel</code> in settings</td>
      </tr>
      <tr>
          <td><a href="../tools/cursor.html">Cursor</a></td>
          <td>Settings &gt; Models</td>
          <td>Thinking toggle, MAX mode, Auto-select</td>
      </tr>
      <tr>
          <td><a href="../tools/copilot.html">Copilot</a></td>
          <td>Model dropdown, Auto mode</td>
          <td>Think Mode toggle (some models)</td>
      </tr>
      <tr>
          <td><a href="../tools/windsurf.html">Windsurf</a></td>
          <td>Cascade model selector</td>
          <td>Reasoning effort on GPT-5.2-Codex (low/medium/high/xhigh)</td>
      </tr>
      <tr>
          <td><a href="../tools/aider.html">Aider</a></td>
          <td><code>--model</code> flag, config YAML</td>
          <td><code>--reasoning-effort</code>, <code>--thinking-tokens</code></td>
      </tr>
      <tr>
          <td><a href="../tools/cline.html">Cline</a></td>
          <td>BYOK per provider</td>
          <td>UI reasoning effort config, Plan/Act workflow</td>
      </tr>
  </tbody>
</table>
<hr>
<h2 id="amps-complete-model-roster">Amp&rsquo;s Complete Model Roster</h2>
<p><a href="../tools/amp.html">Amp Code</a> provides the most concrete example of production multi-model routing, using 15 distinct model deployments across 5 vendors.</p>
<h3 id="user-facing-modes">User-Facing Modes</h3>
<table>
  <thead>
      <tr>
          <th>Mode</th>
          <th>Model</th>
          <th>Purpose</th>
          <th>Performance</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td><strong>Smart</strong></td>
          <td>Claude Opus 4.6</td>
          <td>Collaborative pair-programming</td>
          <td>Default; highest first-try success</td>
      </tr>
      <tr>
          <td><strong>Rush</strong></td>
          <td>Claude Haiku 4.5</td>
          <td>Small, well-defined tasks</td>
          <td>67% cheaper, 50% faster than Smart</td>
      </tr>
      <tr>
          <td><strong>Deep</strong></td>
          <td>GPT-5.2 Codex</td>
          <td>Extended thinking, autonomous</td>
          <td>Works silently 5-15 min before changes</td>
      </tr>
  </tbody>
</table>
<h3 id="feature--subagent-models">Feature &amp; Subagent Models</h3>
<table>
  <thead>
      <tr>
          <th>Role</th>
          <th>Model</th>
          <th>Purpose</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td><strong>Review</strong></td>
          <td>Gemini 3 Pro</td>
          <td>Agentic bug identification and code review</td>
      </tr>
      <tr>
          <td><strong>Search</strong></td>
          <td>Gemini 3 Flash</td>
          <td>Codebase retrieval (~8 parallel tool calls/iteration)</td>
      </tr>
      <tr>
          <td><strong>Oracle</strong></td>
          <td>GPT-5.2 (medium reasoning)</td>
          <td>Read-only planning, debugging, code review</td>
      </tr>
      <tr>
          <td><strong>Librarian</strong></td>
          <td>Claude Sonnet 4.5</td>
          <td>External code research, GitHub repo search</td>
      </tr>
  </tbody>
</table>
<h3 id="system-models-auxiliary">System Models (Auxiliary)</h3>
<table>
  <thead>
      <tr>
          <th>Role</th>
          <th>Model</th>
          <th>Purpose</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td><strong>Look At</strong></td>
          <td>Gemini 3 Flash</td>
          <td>Image, PDF, and media file analysis</td>
      </tr>
      <tr>
          <td><strong>Painter</strong></td>
          <td>Gemini 3 Pro Image</td>
          <td>Image generation and editing</td>
      </tr>
      <tr>
          <td><strong>Handoff</strong></td>
          <td>Gemini 2.5 Flash</td>
          <td>Context analysis for task continuation</td>
      </tr>
      <tr>
          <td><strong>Topics</strong></td>
          <td>Gemini 2.5 Flash-Lite</td>
          <td>Thread categorization for analytics</td>
      </tr>
      <tr>
          <td><strong>Titling</strong></td>
          <td>Claude Haiku 4.5</td>
          <td>Fast title generation</td>
      </tr>
      <tr>
          <td><strong>Amp Tab</strong></td>
          <td>Custom fine-tuned model</td>
          <td>Code completion (SFT + DPO, TensorRT-LLM)</td>
      </tr>
  </tbody>
</table>
<h3 id="the-off-the-rails-metric">The Off-the-Rails Metric</h3>
<p>Amp&rsquo;s most original contribution to model evaluation &ndash; the percentage of total spend wasted on problematic outputs. This captures something benchmarks miss entirely: how much a model costs when it fails.</p>
<table>
  <thead>
      <tr>
          <th>Model</th>
          <th>Internal Evals</th>
          <th>Thread Cost</th>
          <th>Off-the-Rails</th>
          <th>Speed (p50)</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>Claude Sonnet 4.5</td>
          <td>37.1%</td>
          <td>$2.75</td>
          <td>8.4%</td>
          <td>2.4 min</td>
      </tr>
      <tr>
          <td>Gemini 3 Pro</td>
          <td>53.7%</td>
          <td>$2.04</td>
          <td>17.8%</td>
          <td>4.3 min</td>
      </tr>
      <tr>
          <td>Claude Opus 4.5</td>
          <td>57.3%</td>
          <td>$2.05</td>
          <td>2.4%</td>
          <td>3.5 min</td>
      </tr>
  </tbody>
</table>
<p>Gemini 3 Pro wasted nearly 1 in 5 dollars on bad outputs. Opus wasted 1 in 40. When you factor in developer time spent identifying, reverting, and re-doing failed work, the real-world cost difference is larger than the numbers suggest.</p>
<p><strong>The principle:</strong> Waste percentage matters as much as peak capability. A model that scores 10 points lower on benchmarks but wastes 7x less on dead ends is the better engineering choice.</p>
<h3 id="model-switching-timeline">Model Switching Timeline</h3>
<p>Amp switched primary models six times in twelve months, demonstrating that model selection is a dynamic engineering decision:</p>
<table>
  <thead>
      <tr>
          <th>Date</th>
          <th>Primary Model</th>
          <th>Reason for Switch</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>Feb 2025</td>
          <td>Claude Sonnet 3.5/3.7</td>
          <td>Launch</td>
      </tr>
      <tr>
          <td>Jun 2025</td>
          <td>Claude Sonnet 4</td>
          <td>Faster, more stable, better tool calling</td>
      </tr>
      <tr>
          <td>Sep 2025</td>
          <td>Claude Sonnet 4.5</td>
          <td>Reduced sycophancy, better debugging</td>
      </tr>
      <tr>
          <td>Dec 2025</td>
          <td>Gemini 3 Pro</td>
          <td>+17 points on Terminal-Bench 2.0</td>
      </tr>
      <tr>
          <td>Jan 2026</td>
          <td>Claude Opus 4.5</td>
          <td>Gemini&rsquo;s 17.8% off-the-rails rate</td>
      </tr>
      <tr>
          <td>Feb 2026</td>
          <td>Claude Opus 4.6</td>
          <td>Current</td>
      </tr>
  </tbody>
</table>
<h3 id="models-evaluated-and-rejected">Models Evaluated and Rejected</h3>
<p>Amp tested but did not adopt as primary: Kimi K2, Qwen3-Coder, GLM-4.5, GPT-OSS, Grok, Gemini 2.5. GPT-5 was evaluated for one week as a primary agent and repositioned to Oracle role due to slow reasoning, research loops, and poor tool selection.</p>
<hr>
<h2 id="multi-model-routing-strategies">Multi-Model Routing Strategies</h2>
<h3 id="the-dreamteam-concept">The DreamTeam Concept</h3>
<p>A practitioner-reported configuration pairing models from all three major providers by specialization:</p>
<table>
  <thead>
      <tr>
          <th>Role</th>
          <th>Model</th>
          <th>Reasoning</th>
          <th>Rationale</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>Architect/Planner</td>
          <td>Claude Opus 4.6 or GPT-5.2</td>
          <td>max / xhigh</td>
          <td>Deep multi-step reasoning</td>
      </tr>
      <tr>
          <td>Implementer</td>
          <td>Claude Sonnet 4.5 or GPT-5.1</td>
          <td>medium / none</td>
          <td>Speed; plan already exists</td>
      </tr>
      <tr>
          <td>Reviewer</td>
          <td>Claude Opus 4.6 or GPT-5.2</td>
          <td>high / high</td>
          <td>Thorough analysis, complementary perspectives</td>
      </tr>
      <tr>
          <td>Security</td>
          <td>Gemini 3 Pro</td>
          <td>high</td>
          <td>Broad pattern recognition</td>
      </tr>
      <tr>
          <td>Test Writer</td>
          <td>Gemini 3 Flash or Haiku 4.5</td>
          <td>low / medium</td>
          <td>Formulaic work</td>
      </tr>
      <tr>
          <td>Codebase Search</td>
          <td>Gemini 3 Flash</td>
          <td>&ndash;</td>
          <td>3x faster with parallel tool calls</td>
      </tr>
      <tr>
          <td>Legacy Code Reader</td>
          <td>Grok</td>
          <td>&ndash;</td>
          <td>Used as &ldquo;archaeologist&rdquo; agent</td>
      </tr>
  </tbody>
</table>
<p>One practitioner reported getting the best code quality with &ldquo;a full project team using opencode with multiple sub agents which are all managed by a single Opus instance&rdquo; &ndash; giving each subagent a specialized role (coder, reviewer, tester, documentation writer) using different models.</p>
<h3 id="cross-model-review-pattern">Cross-Model Review Pattern</h3>
<p>A specific pattern that emerged from HN discussions: using GPT-5.2 at high reasoning to review code written by Opus 4.5. As one practitioner noted, &ldquo;they find different things.&rdquo; The different training lineages make models complementary rather than redundant for review tasks.</p>
<h3 id="escalate-on-failure-pattern">Escalate-on-Failure Pattern</h3>
<p>The most widely adopted multi-model strategy:</p>
<ol>
<li>Start with low reasoning effort or a cheaper model</li>
<li>If the task fails (test failure, lint error, wrong output), retry with higher reasoning</li>
<li>Continue escalating: low to medium to high to xhigh, or Haiku to Sonnet to Opus</li>
<li>Log escalation events to learn which tasks need higher reasoning by default</li>
</ol>
<p>This naturally optimizes cost while maintaining quality. Both <a href="../tools/codex.html">Codex CLI</a> (via config profiles) and <a href="../tools/claude-code.html">Claude Code</a> (via the <code>/model</code> effort slider) support this workflow.</p>
<hr>
<h2 id="complete-cost-reference">Complete Cost Reference</h2>
<h3 id="frontier-models-per-million-tokens">Frontier Models (Per Million Tokens)</h3>
<table>
  <thead>
      <tr>
          <th>Model</th>
          <th>Input</th>
          <th>Output</th>
          <th>Cache Discount</th>
          <th>Context</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td><strong>Claude Opus 4.6</strong></td>
          <td>$5.00</td>
          <td>$25.00</td>
          <td>90%</td>
          <td>200K / 1M beta</td>
      </tr>
      <tr>
          <td>Claude Opus 4.6 (&gt;200K)</td>
          <td>$10.00</td>
          <td>$37.50</td>
          <td>&ndash;</td>
          <td>1M</td>
      </tr>
      <tr>
          <td><strong>Claude Sonnet 4.5</strong></td>
          <td>$3.00</td>
          <td>$15.00</td>
          <td>90%</td>
          <td>200K / 1M API</td>
      </tr>
      <tr>
          <td><strong>Claude Haiku 4.5</strong></td>
          <td>$1.00</td>
          <td>$5.00</td>
          <td>90%</td>
          <td>200K</td>
      </tr>
      <tr>
          <td><strong>GPT-5.2 / 5.2-Codex</strong></td>
          <td>$1.75</td>
          <td>$14.00</td>
          <td>90%</td>
          <td>400K</td>
      </tr>
      <tr>
          <td>GPT-5.2 Pro</td>
          <td>~$17.50</td>
          <td>~$140.00</td>
          <td>&ndash;</td>
          <td>400K</td>
      </tr>
      <tr>
          <td><strong>GPT-5.3-Codex</strong></td>
          <td>~$1.75</td>
          <td>~$14.00</td>
          <td>TBD</td>
          <td>400K</td>
      </tr>
      <tr>
          <td>GPT-5.0 / 5.1</td>
          <td>$1.25</td>
          <td>$10.00</td>
          <td>90%</td>
          <td>400K</td>
      </tr>
      <tr>
          <td>GPT-5 mini</td>
          <td>$0.25</td>
          <td>$2.00</td>
          <td>90%</td>
          <td>400K</td>
      </tr>
      <tr>
          <td>GPT-5 nano</td>
          <td>$0.05</td>
          <td>$0.40</td>
          <td>90%</td>
          <td>400K</td>
      </tr>
      <tr>
          <td><strong>Gemini 3 Pro</strong></td>
          <td>$2.00</td>
          <td>$12.00</td>
          <td>&ndash;</td>
          <td>1M</td>
      </tr>
      <tr>
          <td>Gemini 3 Pro (&gt;200K)</td>
          <td>$4.00</td>
          <td>$18.00</td>
          <td>&ndash;</td>
          <td>1M</td>
      </tr>
      <tr>
          <td><strong>Gemini 3 Flash</strong></td>
          <td>$0.50</td>
          <td>$3.00</td>
          <td>&ndash;</td>
          <td>1M</td>
      </tr>
      <tr>
          <td>Gemini 2.5 Flash</td>
          <td>$0.15</td>
          <td>$0.60</td>
          <td>&ndash;</td>
          <td>1M</td>
      </tr>
      <tr>
          <td>Gemini 2.5 Flash-Lite</td>
          <td>$0.10</td>
          <td>$0.40</td>
          <td>&ndash;</td>
          <td>1M</td>
      </tr>
      <tr>
          <td><strong>GPT-4.1</strong></td>
          <td>$2.00</td>
          <td>$8.00</td>
          <td>&ndash;</td>
          <td>1M</td>
      </tr>
      <tr>
          <td>GPT-4.1 mini</td>
          <td>$0.40</td>
          <td>$1.60</td>
          <td>&ndash;</td>
          <td>1M</td>
      </tr>
      <tr>
          <td>GPT-4.1 nano</td>
          <td>$0.10</td>
          <td>$0.40</td>
          <td>&ndash;</td>
          <td>1M</td>
      </tr>
  </tbody>
</table>
<h3 id="open-weight--api-models">Open-Weight / API Models</h3>
<table>
  <thead>
      <tr>
          <th>Model</th>
          <th>Input $/M</th>
          <th>Output $/M</th>
          <th>Provider</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>DeepSeek V3 (cache hit)</td>
          <td>$0.07</td>
          <td>$1.68</td>
          <td>Official API</td>
      </tr>
      <tr>
          <td>DeepSeek V3 (cache miss)</td>
          <td>$0.56</td>
          <td>$1.68</td>
          <td>Official API</td>
      </tr>
      <tr>
          <td>Llama 4 Scout</td>
          <td>$0.11</td>
          <td>varies</td>
          <td>Groq</td>
      </tr>
      <tr>
          <td>Llama 4 Maverick</td>
          <td>$0.50</td>
          <td>varies</td>
          <td>Groq</td>
      </tr>
      <tr>
          <td>Devstral Small 2</td>
          <td>$0.10</td>
          <td>$0.30</td>
          <td>Mistral</td>
      </tr>
      <tr>
          <td>Mistral Medium 3</td>
          <td>$0.40</td>
          <td>$2.00</td>
          <td>Mistral</td>
      </tr>
      <tr>
          <td>Kimi K2.5</td>
          <td>$0.60</td>
          <td>$3.00</td>
          <td>Moonshot</td>
      </tr>
      <tr>
          <td>Qwen3 (various)</td>
          <td>$0.20-$1.20</td>
          <td>varies</td>
          <td>Alibaba Cloud</td>
      </tr>
  </tbody>
</table>
<h3 id="subscription-plans">Subscription Plans</h3>
<table>
  <thead>
      <tr>
          <th>Plan</th>
          <th>Monthly</th>
          <th>What You Get</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>Claude Pro</td>
          <td>$20</td>
          <td>Sonnet primary, limited Opus</td>
      </tr>
      <tr>
          <td>Claude Max 5x</td>
          <td>$100</td>
          <td>Full Opus 4.6, 1M context, agent teams</td>
      </tr>
      <tr>
          <td>Claude Max 20x</td>
          <td>$200</td>
          <td>20x Pro usage, everything in 5x</td>
      </tr>
      <tr>
          <td>ChatGPT Plus</td>
          <td>$20</td>
          <td>GPT-5.x access, Codex</td>
      </tr>
      <tr>
          <td>ChatGPT Pro</td>
          <td>$200</td>
          <td>xhigh reasoning, maximum quality</td>
      </tr>
      <tr>
          <td>Gemini CLI</td>
          <td>Free</td>
          <td>1,000 requests/day (mix of Flash and Pro)</td>
      </tr>
  </tbody>
</table>
<h3 id="what-practitioners-actually-spend">What Practitioners Actually Spend</h3>
<ul>
<li><strong>Individual developers:</strong> Claude Max at $100-200/month flat rate. One practitioner described never exceeding 80% of the weekly limit at $200/month. Cursor tracked at $928 over 70 days (~$416/month) with heavy use.</li>
<li><strong>Per-thread (Amp data):</strong> $2.05 with Opus, $2.04 with Gemini 3 Pro, $2.75 with Sonnet. Comparable headline numbers but dramatically different waste profiles.</li>
<li><strong>Annual baseline:</strong> Multiple sources converge on $5-6K per year per developer for AI tooling.</li>
<li><strong>Cache optimization:</strong> One Cursor user achieved 88.8% cache hit rate, bringing effective cost per 1,000 tokens to $0.0009. Understanding your tool&rsquo;s caching behavior can reduce spend by an order of magnitude.</li>
</ul>
<hr>
<h2 id="domain-specific-recommendations-variant-precision">Domain-Specific Recommendations (Variant Precision)</h2>
<p>Based on aggregated practitioner reports across all sources:</p>
<table>
  <thead>
      <tr>
          <th>Domain</th>
          <th>Top Choice</th>
          <th>Config</th>
          <th>Runner-Up</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>Full-stack web (React)</td>
          <td>Opus 4.6 via Claude Code</td>
          <td>high effort</td>
          <td>Opus 4.5</td>
      </tr>
      <tr>
          <td>Rust</td>
          <td>Opus 4.6 + cargo check loop</td>
          <td>high effort</td>
          <td>&ndash;</td>
      </tr>
      <tr>
          <td>C#</td>
          <td>Opus 4.5/4.6</td>
          <td>high effort</td>
          <td>&ndash;</td>
      </tr>
      <tr>
          <td>C++ (mainstream)</td>
          <td>Opus 4.5/4.6</td>
          <td>high effort</td>
          <td>Gemini 3 Pro</td>
      </tr>
      <tr>
          <td>3D / Graphics / Spatial</td>
          <td>Gemini 3 Flash</td>
          <td>high thinking</td>
          <td>&ndash;</td>
      </tr>
      <tr>
          <td>Low-level C / Shaders</td>
          <td>GPT-5.2-Codex</td>
          <td>high reasoning</td>
          <td>&ndash;</td>
      </tr>
      <tr>
          <td>CUDA kernels</td>
          <td>Opus 4.5/4.6</td>
          <td>max effort</td>
          <td>&ndash;</td>
      </tr>
      <tr>
          <td>Transpilation</td>
          <td>GPT-5.2</td>
          <td>high reasoning</td>
          <td>&ndash;</td>
      </tr>
      <tr>
          <td>Structured output</td>
          <td>Mistral Medium 3</td>
          <td>&ndash;</td>
          <td>GPT-4.1 mini</td>
      </tr>
      <tr>
          <td>Architecture guidance</td>
          <td>Opus 4.6 at max effort</td>
          <td>+ Kimi K2.5</td>
          <td>GPT-5.2 xhigh</td>
      </tr>
      <tr>
          <td>Codebase search</td>
          <td>Gemini 3 Flash</td>
          <td>default thinking</td>
          <td>Haiku 4.5</td>
      </tr>
      <tr>
          <td>Code review</td>
          <td>GPT-5.2 high + Opus 4.6</td>
          <td>cross-model</td>
          <td>Gemini 3 Pro</td>
      </tr>
      <tr>
          <td>Complex debugging</td>
          <td>GPT-5.2 Codex xhigh</td>
          <td>autonomous mode</td>
          <td>Opus 4.6 max</td>
      </tr>
      <tr>
          <td>Legacy code reading</td>
          <td>Grok</td>
          <td>&ndash;</td>
          <td>Opus 4.6 (1M context)</td>
      </tr>
      <tr>
          <td>Data science</td>
          <td>Opus 4.5/4.6</td>
          <td>high effort</td>
          <td>GPT-5.2 Thinking</td>
      </tr>
      <tr>
          <td>Batch analysis</td>
          <td>GPT-5.2 Pro</td>
          <td>xhigh</td>
          <td>&ndash;</td>
      </tr>
      <tr>
          <td>Local coding (single GPU)</td>
          <td>Devstral Small 2 (24B)</td>
          <td>&ndash;</td>
          <td>Qwen3-30B-A3B</td>
      </tr>
      <tr>
          <td>Local coding (Mac)</td>
          <td>Qwen3-30B-A3B</td>
          <td>&ndash;</td>
          <td>Devstral Small 2</td>
      </tr>
      <tr>
          <td>Swift</td>
          <td>None excel</td>
          <td>&ndash;</td>
          <td>&ndash;</td>
      </tr>
  </tbody>
</table>
<p>These are practitioner opinions, not controlled evaluations. Your mileage will vary based on codebase, prompt quality, and harness configuration. The most reliable approach remains: test on your own workload, track your own metrics, and maintain the ability to switch.</p>
<p>For comprehensive tool-by-tool comparison, see the <a href="../tools/compare.html">Tool Comparison Matrix</a>. For the principles behind choosing and switching models, see <a href="../practices/model-selection.html">Model Selection</a>.</p>

      </div>

      

      <nav class="article-nav">
        
        <a href="../tools/amp.html" class="article-nav-link article-nav-link--prev">
          <span class="article-nav-direction">Previous</span>
          <span class="article-nav-title">Amp Code</span>
        </a>
        
        
        <a href="../tools/cursor.html" class="article-nav-link article-nav-link--next">
          <span class="article-nav-direction">Next</span>
          <span class="article-nav-title">Cursor</span>
        </a>
        
      </nav>
    </article>

    
<aside class="toc-sidebar" aria-label="Table of Contents">
  <div class="toc-container">
    <h2 class="toc-title">On this page</h2>
    <nav id="TableOfContents">
  <ul>
    <li><a href="#practitioner-mention-heatmap">Practitioner Mention Heatmap</a></li>
    <li><a href="#claude-variants-anthropic">Claude Variants (Anthropic)</a>
      <ul>
        <li><a href="#claude-opus-46">Claude Opus 4.6</a></li>
        <li><a href="#claude-opus-45">Claude Opus 4.5</a></li>
        <li><a href="#claude-sonnet-45">Claude Sonnet 4.5</a></li>
        <li><a href="#claude-haiku-45">Claude Haiku 4.5</a></li>
      </ul>
    </li>
    <li><a href="#gpt-variants-openai">GPT Variants (OpenAI)</a>
      <ul>
        <li><a href="#gpt-53-codex">GPT-5.3 Codex</a></li>
        <li><a href="#gpt-52-codex">GPT-5.2 Codex</a></li>
        <li><a href="#gpt-52-base">GPT-5.2 (Base)</a></li>
        <li><a href="#gpt-51">GPT-5.1</a></li>
        <li><a href="#gpt-50">GPT-5.0</a></li>
        <li><a href="#gpt-41-family">GPT-4.1 Family</a></li>
      </ul>
    </li>
    <li><a href="#gemini-variants-google">Gemini Variants (Google)</a>
      <ul>
        <li><a href="#gemini-3-pro">Gemini 3 Pro</a></li>
        <li><a href="#gemini-3-flash">Gemini 3 Flash</a></li>
        <li><a href="#gemini-25-flash">Gemini 2.5 Flash</a></li>
        <li><a href="#gemini-25-flash-lite">Gemini 2.5 Flash-Lite</a></li>
        <li><a href="#gemini-3-pro-image">Gemini 3 Pro Image</a></li>
      </ul>
    </li>
    <li><a href="#open-weight-models">Open-Weight Models</a>
      <ul>
        <li><a href="#deepseek-v3--v31">DeepSeek V3 / V3.1</a></li>
        <li><a href="#deepseek-r1">DeepSeek R1</a></li>
        <li><a href="#llama-4-scout--maverick">Llama 4 Scout / Maverick</a></li>
        <li><a href="#qwen-3-family">Qwen 3 Family</a></li>
        <li><a href="#mistral">Mistral</a></li>
        <li><a href="#kimi-k25">Kimi K2.5</a></li>
      </ul>
    </li>
    <li><a href="#reasoning-level-deep-dive">Reasoning Level Deep Dive</a>
      <ul>
        <li><a href="#cross-provider-comparison">Cross-Provider Comparison</a></li>
        <li><a href="#openai-reasoning-effort">OpenAI Reasoning Effort</a></li>
        <li><a href="#claude-effort-parameter">Claude Effort Parameter</a></li>
        <li><a href="#gemini-thinking-configuration">Gemini Thinking Configuration</a></li>
        <li><a href="#per-task-reasoning-recommendations">Per-Task Reasoning Recommendations</a></li>
        <li><a href="#tool-specific-configuration">Tool-Specific Configuration</a></li>
      </ul>
    </li>
    <li><a href="#amps-complete-model-roster">Amp&rsquo;s Complete Model Roster</a>
      <ul>
        <li><a href="#user-facing-modes">User-Facing Modes</a></li>
        <li><a href="#feature--subagent-models">Feature &amp; Subagent Models</a></li>
        <li><a href="#system-models-auxiliary">System Models (Auxiliary)</a></li>
        <li><a href="#the-off-the-rails-metric">The Off-the-Rails Metric</a></li>
        <li><a href="#model-switching-timeline">Model Switching Timeline</a></li>
        <li><a href="#models-evaluated-and-rejected">Models Evaluated and Rejected</a></li>
      </ul>
    </li>
    <li><a href="#multi-model-routing-strategies">Multi-Model Routing Strategies</a>
      <ul>
        <li><a href="#the-dreamteam-concept">The DreamTeam Concept</a></li>
        <li><a href="#cross-model-review-pattern">Cross-Model Review Pattern</a></li>
        <li><a href="#escalate-on-failure-pattern">Escalate-on-Failure Pattern</a></li>
      </ul>
    </li>
    <li><a href="#complete-cost-reference">Complete Cost Reference</a>
      <ul>
        <li><a href="#frontier-models-per-million-tokens">Frontier Models (Per Million Tokens)</a></li>
        <li><a href="#open-weight--api-models">Open-Weight / API Models</a></li>
        <li><a href="#subscription-plans">Subscription Plans</a></li>
        <li><a href="#what-practitioners-actually-spend">What Practitioners Actually Spend</a></li>
      </ul>
    </li>
    <li><a href="#domain-specific-recommendations-variant-precision">Domain-Specific Recommendations (Variant Precision)</a></li>
  </ul>
</nav>
  </div>
</aside>


  </div>
</div>

    </main><footer class="site-footer">
  <div class="footer-container">
    <div class="footer-brand">
      <span class="footer-logo">AI Best Practices</span>
      <p class="footer-description">What actually works in AI-assisted development — synthesized from 6,000&#43; practitioner comments across 20 major HN discussions.</p>
    </div>
    <div class="footer-nav">
      <div class="footer-section">
        <h3>Learn</h3>
        <ul>
          <li><a href="../guide/index.html">Guide</a></li>
          <li><a href="../practices/index.html">Practices</a></li>
          <li><a href="../debates/index.html">Debates</a></li>
        </ul>
      </div>
      <div class="footer-section">
        <h3>Explore</h3>
        <ul>
          <li><a href="../tools/index.html">Tools</a></li>
          <li><a href="../evidence/index.html">Evidence</a></li>
          <li><a href="../voices/index.html">Voices</a></li>
          <li><a href="../sources/index.html">Sources</a></li>
        </ul>
      </div>
    </div>
    <div class="footer-bottom">
      <p>Built with Hugo. Synthesized from 32 HN discussions and 6,000+ practitioner comments. 78 pages across 198 topics.</p>
    </div>
  </div>
</footer>
<script src="../js/main.js" defer></script>
  </body>
</html>
