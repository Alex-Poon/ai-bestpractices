<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Deep Dives on AI Best Practices Knowledge Base</title>
    <link>//localhost:1313/deep-dives/index.html</link>
    <description>Recent content in Deep Dives on AI Best Practices Knowledge Base</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 05 Feb 2026 00:00:00 +0000</lastBuildDate>
    <atom:link href="//localhost:1313/deep-dives/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>What 100 Parallel AI Agents Taught Us About Harness Engineering</title>
      <link>//localhost:1313/deep-dives/parallel-compiler-lessons.html</link>
      <pubDate>Thu, 05 Feb 2026 00:00:00 +0000</pubDate>
      <guid>//localhost:1313/deep-dives/parallel-compiler-lessons.html</guid>
      <description>&lt;p&gt;If you want to understand the future of autonomous AI development, ignore the code and study the harness.&lt;/p&gt;&#xA;&lt;p&gt;Nicholas Carlini&amp;rsquo;s compiler project &amp;ndash; 16 parallel Claude instances producing a 100,000-line Rust-based C compiler over two weeks &amp;ndash; is the most data-rich public experiment in parallel agentic coding to date. The compiler passed 99% of the GCC torture test suite and successfully compiled the Linux 6.9 kernel across x86, ARM, and RISC-V architectures. But the real value of the project lies not in the artifact it produced. It lies in what it revealed about how autonomous agents succeed and fail at scale.&lt;/p&gt;</description>
    </item>
    <item>
      <title>What Practitioners Actually Think About AI Coding</title>
      <link>//localhost:1313/deep-dives/practitioner-consensus.html</link>
      <pubDate>Thu, 05 Feb 2026 00:00:00 +0000</pubDate>
      <guid>//localhost:1313/deep-dives/practitioner-consensus.html</guid>
      <description>&lt;p&gt;The Hacker News discussion around Mitchell Hashimoto&amp;rsquo;s AI adoption article drew dozens of experienced practitioners into a remarkably grounded conversation about what actually works, what fails, and what remains genuinely uncertain about AI-assisted development. Unlike most online AI discourse, this thread was dominated by people sharing concrete experiences rather than abstract positions.&lt;/p&gt;&#xA;&lt;p&gt;What follows is a synthesis of where this community of practitioners has converged &amp;ndash; and where honest disagreement remains.&lt;/p&gt;</description>
    </item>
    <item>
      <title>The Multi-Model Agent Landscape</title>
      <link>//localhost:1313/deep-dives/multi-model-agents.html</link>
      <pubDate>Thu, 05 Feb 2026 00:00:00 +0000</pubDate>
      <guid>//localhost:1313/deep-dives/multi-model-agents.html</guid>
      <description>&lt;h2 id=&#34;why-one-model-is-not-enough&#34;&gt;Why One Model Is Not Enough&lt;/h2&gt;&#xA;&lt;p&gt;Every frontier language model has a performance profile. Some excel at planning and reasoning over large contexts. Others are fast and cheap enough for routine lookups. Still others have been tuned for code generation, image understanding, or structured review. No single model is best at everything, and the gap between models on specific tasks can be significant.&lt;/p&gt;&#xA;&lt;p&gt;This is not a theoretical observation. Practitioners who use AI coding agents daily have noticed that the same model that writes excellent code may produce mediocre plans, and the model that reasons carefully through a complex architecture decision may be too slow and expensive for quick file searches. The question is no longer whether to use AI for coding but how to match the right model to the right subtask.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
