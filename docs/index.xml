<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>AI Best Practices on AI Best Practices Knowledge Base</title>
    <link>//localhost:1313/index.html</link>
    <description>Recent content in AI Best Practices on AI Best Practices Knowledge Base</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 05 Feb 2026 00:00:00 +0000</lastBuildDate>
    <atom:link href="//localhost:1313/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Daily Agent-Assisted Development Checklist</title>
      <link>//localhost:1313/references/daily-workflow-checklist.html</link>
      <pubDate>Thu, 05 Feb 2026 00:00:00 +0000</pubDate>
      <guid>//localhost:1313/references/daily-workflow-checklist.html</guid>
      <description>&lt;p&gt;A practical checklist for developers using AI coding agents as part of their daily workflow. Derived from practitioner patterns observed across Mitchell Hashimoto&amp;rsquo;s adoption journey and the Hacker News discussion community.&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 id=&#34;before-starting&#34;&gt;Before Starting&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; &lt;strong&gt;Review AGENTS.md / CLAUDE.md&lt;/strong&gt; &amp;ndash; Is anything outdated? Any patterns to add from yesterday&amp;rsquo;s work? Remove entries that no longer apply. Add entries for mistakes you remember correcting but did not document in the moment.&lt;/li&gt;&#xA;&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; &lt;strong&gt;Identify 2-3 tasks suitable for agent delegation&lt;/strong&gt; &amp;ndash; These should have clear scope, verifiable output, and no architectural decisions embedded in them. If you cannot describe what &amp;ldquo;done&amp;rdquo; looks like in two sentences, the task is not ready.&lt;/li&gt;&#xA;&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; &lt;strong&gt;Check if any overnight agent results are ready for review&lt;/strong&gt; &amp;ndash; If you launched exploration or research tasks at end of day yesterday, review the output now while the context is fresh.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 id=&#34;during-work&#34;&gt;During Work&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; &lt;strong&gt;For each agent task: Write the spec/plan yourself, delegate implementation.&lt;/strong&gt; The design is your job. The typing is the agent&amp;rsquo;s job. If you find yourself letting the agent make structural decisions, stop and reclaim the planning step.&lt;/li&gt;&#xA;&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; &lt;strong&gt;Scope check: Can you verify the output in under 2 minutes?&lt;/strong&gt; If not, break the task down further before delegating. Two minutes is the threshold. Beyond that, you are likely approving output you have not fully understood.&lt;/li&gt;&#xA;&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; &lt;strong&gt;Review diffs incrementally.&lt;/strong&gt; Do not let changes accumulate without verification. Each agent-produced change should be reviewed and verified before moving to the next task. Batching review is how drift accumulates undetected.&lt;/li&gt;&#xA;&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; &lt;strong&gt;When an agent makes a mistake: document it in AGENTS.md before moving on.&lt;/strong&gt; This takes thirty seconds and prevents hours of future re-correction. The discipline is in doing it immediately, not later. &amp;ldquo;Later&amp;rdquo; means &amp;ldquo;never.&amp;rdquo;&lt;/li&gt;&#xA;&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; &lt;strong&gt;Control your attention.&lt;/strong&gt; Disable agent notifications. Check agent results on YOUR schedule, not the agent&amp;rsquo;s. The agent works for you, not the other way around. If you are context-switching every time the agent produces output, you are losing more productivity to interruption than you are gaining from delegation.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 id=&#34;end-of-day&#34;&gt;End of Day&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; &lt;strong&gt;Launch 1-2 agents for overnight research, exploration, or issue triage.&lt;/strong&gt; These should be tasks where the output is informational, not code that will be merged without review. Good candidates: investigating a bug, summarizing documentation, exploring library options, triaging open issues.&lt;/li&gt;&#xA;&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; &lt;strong&gt;Queue up &amp;ldquo;slam dunk&amp;rdquo; tasks for tomorrow&amp;rsquo;s delegation.&lt;/strong&gt; Identify tasks that are clearly well-scoped and likely to succeed. Having these ready means tomorrow starts productively instead of with a scoping exercise.&lt;/li&gt;&#xA;&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; &lt;strong&gt;Update AGENTS.md with any new patterns, mistakes, or conventions discovered today.&lt;/strong&gt; End-of-day is the second-best time to document (immediately is best, but a daily sweep catches what you missed).&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 id=&#34;weekly&#34;&gt;Weekly&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; &lt;strong&gt;Review and refine AGENTS.md.&lt;/strong&gt; Remove outdated entries. Consolidate entries that address the same underlying issue. Reorganize if the file has grown unwieldy. The harness should be concise and scannable, not a sprawling document.&lt;/li&gt;&#xA;&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; &lt;strong&gt;Assess: What types of tasks consistently succeed? Which consistently fail?&lt;/strong&gt; Build a mental (or written) model of the agent&amp;rsquo;s reliability frontier. Expand delegation into areas of consistent success. Pull back from areas of consistent failure.&lt;/li&gt;&#xA;&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; &lt;strong&gt;Consider building a new tool/script if you have seen the same agent failure 3+ times.&lt;/strong&gt; Repeated failures in the same category signal a structural problem that documentation alone cannot fix. A purpose-built tool (filtered test runner, output formatter, context assembler) may be warranted.&lt;/li&gt;&#xA;&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; &lt;strong&gt;Check costs.&lt;/strong&gt; Are you getting value proportional to spend? If you are on a usage-based plan, review the past week&amp;rsquo;s spending. If you are on a flat rate, assess whether you are using the tool enough to justify the subscription. Nobody should be paying for AI tools they are not actively using.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 id=&#34;signs-you-are-doing-it-right&#34;&gt;Signs You Are Doing It Right&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;You spend more time reading and verifying than writing prompts.&lt;/strong&gt; The bottleneck has shifted from generation to verification. This is the correct state.&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Your AGENTS.md is growing steadily.&lt;/strong&gt; Not explosively (that suggests too many mistakes) and not stagnating (that suggests you are not documenting). Steady growth means you are capturing institutional knowledge.&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Agent mistakes are NEW mistakes, not repeated ones.&lt;/strong&gt; The harness is working. Old failure modes are documented and prevented. Errors that occur are genuinely novel.&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;You have a backlog of tasks ready for delegation.&lt;/strong&gt; You are not scrambling to find things for the agent to do. You have a pipeline of well-scoped tasks identified in advance.&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;You control your attention.&lt;/strong&gt; The agent does not interrupt you. You check results when you are ready. Your focus time is protected.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 id=&#34;signs-you-are-doing-it-wrong&#34;&gt;Signs You Are Doing It Wrong&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;You are &amp;ldquo;drawing the owl.&amp;rdquo;&lt;/strong&gt; Giant tasks with vague goals handed to the agent as a single prompt. The output is impressive-looking but subtly wrong in ways you discover too late.&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;You are re-explaining the same constraints every session.&lt;/strong&gt; If you have told the agent the same thing three times across different sessions, it should be in AGENTS.md. You are burning time on repetition instead of building persistent infrastructure.&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;You are not verifying output before moving on.&lt;/strong&gt; Approving agent diffs without reading them, running tests, or checking behavior. This builds accumulated drift that compounds into serious problems.&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;The agent is driving your attention.&lt;/strong&gt; Notifications are on. You context-switch every time the agent produces output. Your workflow is reactive instead of deliberate.&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;You do not know what your monthly AI spend is.&lt;/strong&gt; You are making investment decisions (time, money, workflow changes) without data. Track your costs.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;This checklist puts into daily practice the principles from &lt;a href=&#34;./patterns/task-scoping.html&#34;&gt;Task Scoping&lt;/a&gt;, &lt;a href=&#34;./patterns/harness-engineering.html&#34;&gt;Harness Engineering&lt;/a&gt;, and &lt;a href=&#34;./guides/adoption-stages.html&#34;&gt;The Six Stages of AI Adoption&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Pattern: Task Scoping for AI Agents</title>
      <link>//localhost:1313/patterns/task-scoping.html</link>
      <pubDate>Thu, 05 Feb 2026 00:00:00 +0000</pubDate>
      <guid>//localhost:1313/patterns/task-scoping.html</guid>
      <description>&lt;h2 id=&#34;problem&#34;&gt;Problem&lt;/h2&gt;&#xA;&lt;p&gt;Agent produces unusable output. Either the task is so trivial that delegation is pointless (writing a for loop, renaming a variable), or so broad that the output is confidently wrong and must be discarded entirely. In both cases, the time spent prompting and reviewing exceeds the time it would have taken to do the work manually.&lt;/p&gt;&#xA;&lt;p&gt;This is the single most common reason practitioners abandon agent-assisted development during the adoption valley.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Start Here: AI-Assisted Development</title>
      <link>//localhost:1313/guides/getting-started.html</link>
      <pubDate>Thu, 05 Feb 2026 00:00:00 +0000</pubDate>
      <guid>//localhost:1313/guides/getting-started.html</guid>
      <description>&lt;p&gt;This site is a practitioner reference for developers who use AI coding agents in their daily work. It is built from real-world experience, community discussion, and empirical evidence &amp;mdash; not marketing claims. If you use tools like Claude Code, Cursor, Copilot, or Amp to write and modify code, the patterns and frameworks here will help you get better results and waste less time.&lt;/p&gt;&#xA;&lt;h2 id=&#34;the-core-loop&#34;&gt;The Core Loop&lt;/h2&gt;&#xA;&lt;p&gt;Effective AI-assisted development follows a four-phase loop. Every technique on this site maps back to one of these phases.&lt;/p&gt;</description>
    </item>
    <item>
      <title>The State of Agentic Coding Practice (Feb 2026)</title>
      <link>//localhost:1313/synthesis/state-of-practice-feb-2026.html</link>
      <pubDate>Thu, 05 Feb 2026 00:00:00 +0000</pubDate>
      <guid>//localhost:1313/synthesis/state-of-practice-feb-2026.html</guid>
      <description>&lt;p&gt;A synthesis of where serious practitioners have landed on AI-assisted development, drawn from Mitchell Hashimoto&amp;rsquo;s adoption journey, its Hacker News discussion (147+ points), and the emerging multi-model agent landscape (Amp Code and similar tools).&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 id=&#34;1-where-practitioners-actually-are&#34;&gt;1. Where Practitioners Actually Are&lt;/h2&gt;&#xA;&lt;p&gt;Most serious practitioners have converged on a remarkably similar workflow loop:&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;Plan in chat -&amp;gt; Execute narrow diffs via agent -&amp;gt; Verify fast -&amp;gt; Tighten harness&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;p&gt;The variation between practitioners is in degree, not in kind. Whether someone uses Claude Code, Cursor, Amp, or Copilot, the operational rhythm looks the same. The key shift that experienced users describe is that the bottleneck has moved from &lt;em&gt;writing&lt;/em&gt; code to &lt;em&gt;reading and verifying&lt;/em&gt; code. This is a fundamental change in the nature of programming work. You spend less time with your hands on the keyboard producing code and more time reviewing, testing, and understanding what the agent produced.&lt;/p&gt;</description>
    </item>
    <item>
      <title>What 100 Parallel AI Agents Taught Us About Harness Engineering</title>
      <link>//localhost:1313/deep-dives/parallel-compiler-lessons.html</link>
      <pubDate>Thu, 05 Feb 2026 00:00:00 +0000</pubDate>
      <guid>//localhost:1313/deep-dives/parallel-compiler-lessons.html</guid>
      <description>&lt;p&gt;If you want to understand the future of autonomous AI development, ignore the code and study the harness.&lt;/p&gt;&#xA;&lt;p&gt;Nicholas Carlini&amp;rsquo;s compiler project &amp;ndash; 16 parallel Claude instances producing a 100,000-line Rust-based C compiler over two weeks &amp;ndash; is the most data-rich public experiment in parallel agentic coding to date. The compiler passed 99% of the GCC torture test suite and successfully compiled the Linux 6.9 kernel across x86, ARM, and RISC-V architectures. But the real value of the project lies not in the artifact it produced. It lies in what it revealed about how autonomous agents succeed and fail at scale.&lt;/p&gt;</description>
    </item>
    <item>
      <title>AI Coding Tools: A Practitioner&#39;s Comparison</title>
      <link>//localhost:1313/references/tool-landscape.html</link>
      <pubDate>Thu, 05 Feb 2026 00:00:00 +0000</pubDate>
      <guid>//localhost:1313/references/tool-landscape.html</guid>
      <description>&lt;h2 id=&#34;use-a-decision-framework-not-rankings&#34;&gt;Use a Decision Framework, Not Rankings&lt;/h2&gt;&#xA;&lt;p&gt;The AI coding tool landscape changes faster than any comparison can keep up with. Models improve monthly. Pricing shifts quarterly. New entrants appear constantly. Nearly $2 billion has been raised by coding agent startups in just the past five months.&lt;/p&gt;&#xA;&lt;p&gt;Rather than ranking tools, this page provides a decision framework based on factors that are relatively stable: workflow type, pricing model, extensibility, and integration approach. Use these to evaluate any tool, including ones that do not exist yet.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Pattern: Harness Engineering</title>
      <link>//localhost:1313/patterns/harness-engineering.html</link>
      <pubDate>Thu, 05 Feb 2026 00:00:00 +0000</pubDate>
      <guid>//localhost:1313/patterns/harness-engineering.html</guid>
      <description>&lt;h2 id=&#34;problem&#34;&gt;Problem&lt;/h2&gt;&#xA;&lt;p&gt;The agent keeps making the same mistakes. You correct it in-session, it complies, and then the next session it makes the identical error because it has no memory of the previous correction. You find yourself re-explaining the same project conventions, re-specifying the same constraints, and re-fixing the same categories of errors across every interaction.&lt;/p&gt;&#xA;&lt;p&gt;There is no institutional memory across sessions. Each session starts from zero.&lt;/p&gt;&#xA;&lt;h2 id=&#34;solution&#34;&gt;Solution&lt;/h2&gt;&#xA;&lt;p&gt;Build persistent infrastructure that constrains and guides agents across sessions. This infrastructure &amp;mdash; the &amp;ldquo;harness&amp;rdquo; &amp;mdash; takes two complementary forms: documentation files that agents read, and purpose-built tools that agents use.&lt;/p&gt;</description>
    </item>
    <item>
      <title>The Six Stages of AI Adoption</title>
      <link>//localhost:1313/guides/adoption-stages.html</link>
      <pubDate>Thu, 05 Feb 2026 00:00:00 +0000</pubDate>
      <guid>//localhost:1313/guides/adoption-stages.html</guid>
      <description>&lt;p&gt;Developers who successfully adopt AI coding agents tend to follow the same progression. Not because someone prescribed it, but because each stage builds the calibration and infrastructure required by the next. Skipping stages leads to frustration, wasted effort, and the conclusion that agents do not work &amp;mdash; when the real problem was missing foundations.&lt;/p&gt;&#xA;&lt;p&gt;This framework synthesizes practitioner experience into six stages. Most developers who abandon AI-assisted development get stuck between Stages 1 and 2. Most developers who succeed credit the investments made in Stages 2 and 5.&lt;/p&gt;</description>
    </item>
    <item>
      <title>What Practitioners Actually Think About AI Coding</title>
      <link>//localhost:1313/deep-dives/practitioner-consensus.html</link>
      <pubDate>Thu, 05 Feb 2026 00:00:00 +0000</pubDate>
      <guid>//localhost:1313/deep-dives/practitioner-consensus.html</guid>
      <description>&lt;p&gt;The Hacker News discussion around Mitchell Hashimoto&amp;rsquo;s AI adoption article drew dozens of experienced practitioners into a remarkably grounded conversation about what actually works, what fails, and what remains genuinely uncertain about AI-assisted development. Unlike most online AI discourse, this thread was dominated by people sharing concrete experiences rather than abstract positions.&lt;/p&gt;&#xA;&lt;p&gt;What follows is a synthesis of where this community of practitioners has converged &amp;ndash; and where honest disagreement remains.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Pattern: Parallel Agent Coordination</title>
      <link>//localhost:1313/patterns/parallel-agent-coordination.html</link>
      <pubDate>Thu, 05 Feb 2026 00:00:00 +0000</pubDate>
      <guid>//localhost:1313/patterns/parallel-agent-coordination.html</guid>
      <description>&lt;h2 id=&#34;problem&#34;&gt;Problem&lt;/h2&gt;&#xA;&lt;p&gt;A single agent is too slow for large projects. Building a 100,000-line codebase one session at a time could take months. The obvious solution &amp;mdash; running multiple agents in parallel &amp;mdash; introduces new failure modes: agents overwrite each other&amp;rsquo;s work, duplicate effort on the same problem, create merge conflicts, and produce inconsistent code because they have no shared context.&lt;/p&gt;&#xA;&lt;p&gt;Naively adding agents does not scale. Without coordination infrastructure, more agents can mean less progress.&lt;/p&gt;</description>
    </item>
    <item>
      <title>The AGENTS.md / CLAUDE.md Guide</title>
      <link>//localhost:1313/references/agents-md-guide.html</link>
      <pubDate>Thu, 05 Feb 2026 00:00:00 +0000</pubDate>
      <guid>//localhost:1313/references/agents-md-guide.html</guid>
      <description>&lt;h2 id=&#34;what-these-files-are&#34;&gt;What These Files Are&lt;/h2&gt;&#xA;&lt;p&gt;AGENTS.md and CLAUDE.md are project-root files that AI coding agents read at the start of every session. They give agents persistent, project-specific context that would otherwise be lost between conversations. Different tools use different filenames &amp;ndash; Claude Code reads CLAUDE.md, Amp reads AGENTS.md, and most tools now support both &amp;ndash; but the purpose is identical: tell the agent how to work in this codebase.&lt;/p&gt;&#xA;&lt;h2 id=&#34;why-they-matter-the-compounding-effect&#34;&gt;Why They Matter: The Compounding Effect&lt;/h2&gt;&#xA;&lt;p&gt;Every time you correct an agent&amp;rsquo;s mistake, you face a choice: fix it once and move on, or document the correction so it never happens again. The AGENTS.md file is where those corrections accumulate.&lt;/p&gt;</description>
    </item>
    <item>
      <title>The Multi-Model Agent Landscape</title>
      <link>//localhost:1313/deep-dives/multi-model-agents.html</link>
      <pubDate>Thu, 05 Feb 2026 00:00:00 +0000</pubDate>
      <guid>//localhost:1313/deep-dives/multi-model-agents.html</guid>
      <description>&lt;h2 id=&#34;why-one-model-is-not-enough&#34;&gt;Why One Model Is Not Enough&lt;/h2&gt;&#xA;&lt;p&gt;Every frontier language model has a performance profile. Some excel at planning and reasoning over large contexts. Others are fast and cheap enough for routine lookups. Still others have been tuned for code generation, image understanding, or structured review. No single model is best at everything, and the gap between models on specific tasks can be significant.&lt;/p&gt;&#xA;&lt;p&gt;This is not a theoretical observation. Practitioners who use AI coding agents daily have noticed that the same model that writes excellent code may produce mediocre plans, and the model that reasons carefully through a complex architecture decision may be too slow and expensive for quick file searches. The question is no longer whether to use AI for coding but how to match the right model to the right subtask.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
