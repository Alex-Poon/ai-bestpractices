<!DOCTYPE html>
<html lang="en"><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>The State of Agentic Coding Practice (Feb 2026) | AI Best Practices Knowledge Base</title>
  <meta name="description" content="Where serious practitioners have landed on AI-assisted development as of February 2026.">
  <meta name="color-scheme" content="light dark">

  
  <meta property="og:title" content="The State of Agentic Coding Practice (Feb 2026)">
  <meta property="og:description" content="Where serious practitioners have landed on AI-assisted development as of February 2026.">
  <meta property="og:type" content="article">

  
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

  
  <link rel="stylesheet" href="../css/style.css">

  
  <link rel="icon" href="../favicon.svg" type="image/svg+xml">
</head>
<body class="section-synthesis"><header class="site-header">
  <nav class="nav-container">
    <a class="nav-logo" href="../index.html">
      <svg class="nav-logo-icon" width="24" height="24" viewBox="0 0 24 24" fill="none" xmlns="http://www.w3.org/2000/svg">
        <circle cx="12" cy="12" r="10" stroke="currentColor" stroke-width="2"/>
        <circle cx="12" cy="8" r="2" fill="currentColor"/>
        <circle cx="7" cy="15" r="2" fill="currentColor"/>
        <circle cx="17" cy="15" r="2" fill="currentColor"/>
        <line x1="12" y1="10" x2="7" y2="13" stroke="currentColor" stroke-width="1.5"/>
        <line x1="12" y1="10" x2="17" y2="13" stroke="currentColor" stroke-width="1.5"/>
        <line x1="7" y1="15" x2="17" y2="15" stroke="currentColor" stroke-width="1.5"/>
      </svg>
      <span>AI Best Practices</span>
    </a>
    <div class="nav-links">
      <a class="nav-link" data-section="guides" href="../guides/index.html">Guides</a>
      <a class="nav-link" data-section="patterns" href="../patterns/index.html">Patterns</a>
      <a class="nav-link" data-section="deep-dives" href="../deep-dives/index.html">Deep Dives</a>
      <a class="nav-link" data-section="references" href="../references/index.html">References</a>
      <a class="nav-link" data-section="tags" href="../tags/index.html">Tags</a>
    </div>
    <div class="nav-actions">
      <button class="dark-mode-toggle" aria-label="Toggle dark mode" type="button">
        <svg class="icon-sun" width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
          <circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/>
        </svg>
        <svg class="icon-moon" width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
          <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"/>
        </svg>
      </button>
      <button class="nav-toggle" aria-label="Toggle menu" type="button">
        <span></span>
        <span></span>
        <span></span>
      </button>
    </div>
  </nav>
</header>
<main>
<div class="page-container">
  
<nav class="breadcrumbs" aria-label="Breadcrumb">
  <ol>
    <li><a href="../index.html">Home</a></li>
    
      
    
      
        <li><a href="../synthesis/index.html">Synthesis</a></li>
      
    
    <li aria-current="page">The State of Agentic Coding Practice (Feb 2026)</li>
  </ol>
</nav>



  <div class="article-layout has-toc">
    <article class="article">
      <header class="article-header">
        <h1 class="article-title">The State of Agentic Coding Practice (Feb 2026)</h1>
        <div class="meta-bar">
  
  <time class="meta-date" datetime="2026-02-05">
    February 5, 2026
  </time>
  

  

  <span class="meta-reading-time">11 min read</span>

  <div class="meta-links">
    
    
    
  </div>

  
  
</div>

        
<div class="tag-pills">
  
  
  
  <a href="../tags/synthesis.html" class="tag-pill">synthesis</a>
  
  
  
  
  <a href="../tags/best-practices.html" class="tag-pill">best-practices</a>
  
  
  
  
  <a href="../tags/agent-workflows.html" class="tag-pill">agent-workflows</a>
  
  
</div>


      </header>

      <div class="article-content">
        <p>A synthesis of where serious practitioners have landed on AI-assisted development, drawn from Mitchell Hashimoto&rsquo;s adoption journey, its Hacker News discussion (147+ points), and the emerging multi-model agent landscape (Amp Code and similar tools).</p>
<hr>
<h2 id="1-where-practitioners-actually-are">1. Where Practitioners Actually Are</h2>
<p>Most serious practitioners have converged on a remarkably similar workflow loop:</p>
<p><strong>Plan in chat -&gt; Execute narrow diffs via agent -&gt; Verify fast -&gt; Tighten harness</strong></p>
<p>The variation between practitioners is in degree, not in kind. Whether someone uses Claude Code, Cursor, Amp, or Copilot, the operational rhythm looks the same. The key shift that experienced users describe is that the bottleneck has moved from <em>writing</em> code to <em>reading and verifying</em> code. This is a fundamental change in the nature of programming work. You spend less time with your hands on the keyboard producing code and more time reviewing, testing, and understanding what the agent produced.</p>
<p><a href="../guides/adoption-stages.html">Hashimoto describes arriving at this loop</a> after passing through six stages of adoption. HN commenters who have reached proficiency describe essentially the same workflow, even if they use different tools and vocabulary. The convergence is striking because it was not coordinated. People arrived here independently by iterating on what works.</p>
<hr>
<h2 id="2-the-scoping-problem-is-the-problem">2. The Scoping Problem Is THE Problem</h2>
<p>If there is one point of universal agreement across the source material, it is this: <strong><a href="../patterns/task-scoping.html">task scoping</a> is the dominant skill in agent-assisted development.</strong></p>
<p>The failure mode is not that agents are stupid. It is that humans give them tasks that are either too narrow to matter or too broad to succeed. The sweet spot is a task that is:</p>
<ul>
<li>Small enough that you can verify the output quickly (under two minutes is a common threshold)</li>
<li>Large enough that delegating it saves meaningful time</li>
<li>Concrete enough that the agent does not need to make architectural decisions</li>
</ul>
<p><strong><a href="../patterns/task-scoping.html">allenu&rsquo;s tree metaphor</a></strong> is the clearest mental model to emerge from the discussion: the human owns the trunk (overall architecture) and the main branches (module design, key interfaces). The agent does the leaves (individual implementations, boilerplate, test cases, repetitive transformations). The human never delegates trunk decisions. The agent never needs to understand the whole tree.</p>
<p>mjr00 reinforces this from the opposite direction: agents fail when given &ldquo;draw the rest of the owl&rdquo; tasks where the entire feature is one prompt. sho_hn notes that the skill is in decomposition itself, not in prompting. apercu points out that if you cannot articulate what success looks like for a task, you are not ready to delegate it.</p>
<p>The scoping skill is hard to teach because it requires understanding both the problem domain (to know what is architecturally significant) and the agent&rsquo;s capabilities (to know what it can reliably execute). This is why experienced developers extract more value: they have the domain knowledge to scope well.</p>
<hr>
<h2 id="3-drift-is-the-main-failure-mode">3. Drift Is the Main Failure Mode</h2>
<p>The characteristic failure of agentic coding is not the obvious error. It is <strong>drift</strong>: the agent stays locally plausible while slowly diverging from real constraints.</p>
<p>Each individual change looks reasonable in isolation. The code compiles. The tests pass (if there are tests). But the agent has been making small decisions that accumulate into a design that violates unstated requirements, ignores performance constraints, or diverges from the architectural intent.</p>
<p>This is worse than obvious errors for a specific reason: <strong>it builds false confidence.</strong> You review the diff, it looks fine, you approve it, and you move on. The divergence only becomes visible at runtime, or during integration, or when someone else reads the code. By then, the damage is compounded across multiple approved changes.</p>
<p>The solution is structural, not attentional:</p>
<ul>
<li><strong>Small diffs:</strong> Each change should be small enough to reason about completely.</li>
<li><strong>Fast verification:</strong> Run the relevant tests, check the behavior, confirm the output immediately after each change. Do not batch.</li>
<li><strong>Explicit constraints:</strong> Anything the agent should not do needs to be written down, not assumed.</li>
</ul>
<p>The drift problem is why &ldquo;just let it run overnight on a big task&rdquo; is almost always a mistake for production code. The longer the agent runs without verification checkpoints, the further it can drift before you notice.</p>
<hr>
<h2 id="4-harness-engineering-is-a-real-discipline">4. <a href="../patterns/harness-engineering.html">Harness Engineering</a> Is a Real Discipline</h2>
<p>The most underappreciated insight from the practitioner community is that <strong>the infrastructure around the agent matters more than the prompts you give it.</strong></p>
<p>This infrastructure takes two forms:</p>
<h3 id="documentation-files-agentsmd--claudemd">Documentation Files (AGENTS.md / CLAUDE.md)</h3>
<p>These are persistent files that the agent reads at the start of every session. They contain:</p>
<ul>
<li>Project conventions and architectural decisions</li>
<li>Known mistakes and their corrections (&ldquo;never use library X for Y because Z&rdquo;)</li>
<li>Formatting and style requirements</li>
<li>Testing procedures and requirements</li>
<li>Constraints that are hard to infer from code alone</li>
</ul>
<p>EastLondonCoder describes the practice as &ldquo;continuously tightening the harness.&rdquo; Each time the agent makes a mistake, you document the correction. That mistake never recurs. Over weeks and months, the harness becomes increasingly precise, and the agent&rsquo;s output quality improves without the agent itself getting smarter.</p>
<h3 id="purpose-built-tools">Purpose-Built Tools</h3>
<p>Some practitioners go further and build tools designed specifically for LLM consumption:</p>
<ul>
<li>Screenshot tools that produce structured output an agent can parse</li>
<li>Filtered test runners that surface only relevant failures</li>
<li>Output formatters that present information in ways agents process reliably</li>
<li>Linting scripts that catch common agent mistakes before human review</li>
</ul>
<h3 id="why-this-compounds">Why This Compounds</h3>
<p>Each documented mistake prevents recurrence across all future sessions. Each custom tool reduces a class of errors permanently. The investment in <a href="../patterns/harness-engineering.html">harness engineering</a> pays increasing dividends over time, while prompt engineering hits diminishing returns quickly.</p>
<p><a href="../guides/adoption-stages.html">Hashimoto identifies this as Stage 5</a> of his adoption journey: the point where you stop thinking about individual prompts and start building infrastructure.</p>
<hr>
<h2 id="5-the-tool-landscape-rewards-multi-model-thinking">5. The Tool Landscape Rewards Multi-Model Thinking</h2>
<p><a href="../deep-dives/multi-model-agents.html">Amp Code&rsquo;s architecture</a> makes explicit what sophisticated users of other tools have learned implicitly: <strong>different models are better at different sub-tasks.</strong></p>
<p>A routing table that sends planning tasks to one model, implementation to another, and quick edits to a third is not just an optimization. It reflects a genuine difference in model capabilities. Deep reasoning models (like o3) handle complex architectural decisions better. Fast models (like Gemini 2.5 Flash) handle routine transformations efficiently. Mid-tier models handle the bulk of implementation work.</p>
<p>Even users who work with a single tool benefit from understanding this principle. Most tools now offer mode switching (e.g., Claude&rsquo;s &ldquo;deep think&rdquo; vs. standard mode). Knowing when to invoke deeper reasoning versus when to use fast mode is a skill that directly affects output quality and cost.</p>
<p>The practical implication: <strong>slower is often better for complex problems.</strong> The instinct to use the fastest available mode for everything is a false economy. A two-minute deep reasoning call that produces correct architecture saves hours compared to a five-second response that produces plausible but subtly wrong architecture.</p>
<hr>
<h2 id="6-cost-is-real-and-under-discussed">6. Cost Is Real and Under-Discussed</h2>
<p>The range of spending reported across the discussion is wide:</p>
<ul>
<li><strong>Low end:</strong> $20/month (single Copilot or basic Claude subscription)</li>
<li><strong>Mid range:</strong> $100-200/month (Claude Pro/Max, multiple tools)</li>
<li><strong>High end:</strong> $500-1000+/month (heavy API usage, Amp-style pay-per-token, multiple subscriptions)</li>
</ul>
<p>The pricing models create real tradeoffs:</p>
<ul>
<li><strong>Flat rate</strong> (Claude Max at $100-200/month): Predictable costs. Encourages experimentation because marginal cost is zero. Risk of hitting rate limits on heavy usage days.</li>
<li><strong>Pay-as-you-go</strong> (Amp, direct API): Costs scale with usage. Rewards efficient prompting. Can produce bill shock on intensive days.</li>
</ul>
<p>Almost nobody in the discussion shares concrete cost/value calculations. This is a gap in the community&rsquo;s understanding. Without tracking what you spend and what value you get, you cannot make rational decisions about tool selection or usage patterns.</p>
<p>One practical observation: the value curve is not linear. Going from $0 to $20/month (basic autocomplete) is a large jump in productivity. Going from $20 to $200/month (full agentic workflow) is another large jump but requires significant skill investment to realize. Going from $200 to $1000/month may have diminishing returns unless your work involves very specific high-value tasks.</p>
<hr>
<h2 id="7-this-is-just-good-engineering-made-mandatory">7. This Is Just Good Engineering, Made Mandatory</h2>
<p>Multiple practitioners in the HN discussion make a version of this observation: the practices that make agent-assisted development work well are the same practices that always made software development work well.</p>
<ul>
<li>Scoped, well-defined tasks with clear success criteria</li>
<li>Incremental changes with verification at each step</li>
<li>Documented conventions and constraints</li>
<li>Fast feedback loops</li>
<li>Separation of design decisions from implementation details</li>
</ul>
<p>The difference is that agents <strong>punish sloppy practices more visibly and more quickly than human collaborators do.</strong> A human teammate given a vague task will ask clarifying questions or make reasonable assumptions based on shared context. An agent given a vague task will produce confident, plausible, subtly wrong output.</p>
<p>This means that AI adoption functions as a forcing function for engineering discipline. Teams that already practice rigorous scoping, clear specifications, and fast verification cycles adopt agents smoothly. Teams that rely on informal communication and implicit knowledge struggle.</p>
<p>The implication is optimistic: investing in agent-assisted development skills is not a bet on a specific tool or model. It is an investment in engineering fundamentals that will pay off regardless of how the tool landscape evolves.</p>
<hr>
<h2 id="8-the-adoption-curve-is-real">8. The Adoption Curve Is Real</h2>
<p><a href="../guides/adoption-stages.html">Hashimoto describes six stages of AI adoption</a>, from skepticism through to agent-native development. The HN discussion confirms a key feature of this curve: <strong>there is a valley of inefficiency that must be pushed through.</strong></p>
<p>Early adoption is frustrating. The agent does not understand your codebase. Your prompts are vague. You spend more time fixing agent output than you would have spent writing the code yourself. This is normal and temporary, but many people quit here and conclude that AI coding tools are overhyped.</p>
<p>polyrand captures it directly: &ldquo;The only way to get good is actually trying to do it.&rdquo; Reading about techniques is not sufficient. The skill is embodied: you develop intuition for scoping, verification, and <a href="../patterns/harness-engineering.html">harness engineering</a> through practice, not through theory.</p>
<p>The adoption curve also has a social dimension. Practitioners who work alongside others who are further along the curve adopt faster. Seeing someone effectively delegate a task to an agent teaches scoping intuition more efficiently than any written guide.</p>
<hr>
<h2 id="10-parallel-agents-at-scale-evidence-from-the-carlini-compiler-project">10. Parallel Agents at Scale: Evidence from the <a href="../deep-dives/parallel-compiler-lessons.html">Carlini Compiler Project</a></h2>
<p>Nicholas Carlini (Anthropic Safeguards researcher) stress-tested autonomous LLM capabilities by running 16 parallel Claude instances to build a 100,000-line Rust-based C compiler from scratch. Key findings that reinforce and extend the existing patterns:</p>
<ul>
<li>
<p><strong><a href="../patterns/harness-engineering.html">Harness engineering</a> scales:</strong> At 2,000 sessions and $20K spend, the test harness was the single most important investment. Carlini&rsquo;s rule: &ldquo;the task verifier must be nearly perfect, otherwise Claude will solve the wrong problem.&rdquo; This validates harness engineering as the highest-leverage practice identified in the synthesis.</p>
</li>
<li>
<p><strong>Anthropomorphic test design is a new sub-discipline:</strong> Tests must be designed for the LLM&rsquo;s cognition — limited verbosity, pre-computed aggregates, grep-friendly error formatting, random fast-path sampling. Claude will &ldquo;happily spend hours running tests instead of making progress&rdquo; without these constraints.</p>
</li>
<li>
<p><strong>Parallelism has sharp limits:</strong> When tasks decompose cleanly (many failing tests), 16 agents provide near-linear speedup. When the task is monolithic (kernel compilation), all agents converge on the same bottleneck and overwrite each other. The <a href="../patterns/task-scoping.html">scoping problem</a> from the earlier analysis directly determines whether parallelism helps.</p>
</li>
<li>
<p><strong>Agent specialization works:</strong> Rather than N identical agents, deploying role-specialized agents (core dev, deduplicator, optimizer, quality critic, docs maintainer) enables parallel progress on orthogonal concerns.</p>
</li>
<li>
<p><strong>Git as coordination protocol:</strong> No orchestration agent needed. Agents coordinate through git (task lock files, merge workflows, fresh sessions reading git history). Conversation memory is deliberately discarded each iteration — persistence lives in the repo.</p>
</li>
<li>
<p><strong>Model capability is a moving target:</strong> Opus 4.0 could &ldquo;barely produce a functional compiler.&rdquo; Opus 4.5 passed test suites but couldn&rsquo;t compile real projects. Opus 4.6 crossed the threshold for kernel compilation. Each generation opens new frontiers.</p>
</li>
<li>
<p><strong>The verification concern persists:</strong> Even at 99% test pass rate, Carlini (with penetration testing background) warns: &ldquo;The thought of programmers deploying software they&rsquo;ve never personally verified is a real concern.&rdquo; Passing tests provides false confidence.</p>
</li>
</ul>
<hr>
<h2 id="9-open-questions">9. Open Questions</h2>
<p>Several significant questions remain unresolved across these sources:</p>
<h3 id="junior-developer-skill-formation">Junior Developer Skill Formation</h3>
<p>If agents handle leaf-level implementation, how do junior developers build the foundational skills needed to eventually own trunk-level decisions? Is there a &ldquo;driver&rsquo;s ed&rdquo; problem where you need to do the thing manually before you can supervise it? No consensus exists.</p>
<h3 id="expert-only-value-extraction">Expert-Only Value Extraction</h3>
<p>Is effective agent-assisted development inherently an expert activity? The scoping skill requires deep domain knowledge. The verification skill requires the ability to read code critically. If only experienced developers can extract value, what does this mean for the profession?</p>
<h3 id="the-metr-study-and-productivity-measurement">The METR Study and Productivity Measurement</h3>
<p>A METR study showed 19% productivity <em>reduction</em> from AI tool usage among experienced open-source developers. This result is contested on methodological grounds (task scope, measurement period, tool familiarity), but it raises legitimate questions about when and where agent-assisted development actually helps versus hurts.</p>
<h3 id="the-no-moat-problem-for-tool-makers">The &ldquo;No Moat&rdquo; Problem for Tool Makers</h3>
<p>If the core skill is scoping and verification (human skills), and the core infrastructure is harness files (portable text), then switching between agent tools is relatively easy. This means tool makers may struggle to build durable competitive advantages. The tool landscape could remain volatile for an extended period.</p>
<h3 id="appropriate-trust-calibration">Appropriate Trust Calibration</h3>
<p>How do you calibrate trust in agent output appropriately? Too little trust and you verify everything manually, negating the time savings. Too much trust and you approve drifted code. The optimal trust level likely varies by task type, codebase familiarity, and agent capability, but no good frameworks exist for reasoning about this.</p>
<p>For the patterns referenced throughout this synthesis, see <a href="../patterns/task-scoping.html">Task Scoping</a>, <a href="../patterns/harness-engineering.html">Harness Engineering</a>, and <a href="../patterns/parallel-agent-coordination.html">Parallel Agent Coordination</a>. For the detailed evidence, see the <a href="../deep-dives/index.html">Deep Dives</a> section.</p>

      </div>

      
      <div class="article-sources">
        <h3>Sources</h3>
        <ul>
          <li><a href="https://mitchellh.com/writing/my-ai-adoption-journey">https://mitchellh.com/writing/my-ai-adoption-journey</a></li><li><a href="https://news.ycombinator.com/item?id=46903558">https://news.ycombinator.com/item?id=46903558</a></li><li><a href="https://www.anthropic.com/engineering/building-c-compiler">https://www.anthropic.com/engineering/building-c-compiler</a></li><li><a href="https://ampcode.com">https://ampcode.com</a></li>
        </ul>
      </div>
      

      <nav class="article-nav">
        
        
      </nav>
    </article>

    
<aside class="toc-sidebar" aria-label="Table of Contents">
  <div class="toc-container">
    <h2 class="toc-title">On this page</h2>
    <nav id="TableOfContents">
  <ul>
    <li><a href="#1-where-practitioners-actually-are">1. Where Practitioners Actually Are</a></li>
    <li><a href="#2-the-scoping-problem-is-the-problem">2. The Scoping Problem Is THE Problem</a></li>
    <li><a href="#3-drift-is-the-main-failure-mode">3. Drift Is the Main Failure Mode</a></li>
    <li><a href="#4-harness-engineering-is-a-real-discipline">4. Harness Engineering Is a Real Discipline</a>
      <ul>
        <li><a href="#documentation-files-agentsmd--claudemd">Documentation Files (AGENTS.md / CLAUDE.md)</a></li>
        <li><a href="#purpose-built-tools">Purpose-Built Tools</a></li>
        <li><a href="#why-this-compounds">Why This Compounds</a></li>
      </ul>
    </li>
    <li><a href="#5-the-tool-landscape-rewards-multi-model-thinking">5. The Tool Landscape Rewards Multi-Model Thinking</a></li>
    <li><a href="#6-cost-is-real-and-under-discussed">6. Cost Is Real and Under-Discussed</a></li>
    <li><a href="#7-this-is-just-good-engineering-made-mandatory">7. This Is Just Good Engineering, Made Mandatory</a></li>
    <li><a href="#8-the-adoption-curve-is-real">8. The Adoption Curve Is Real</a></li>
    <li><a href="#10-parallel-agents-at-scale-evidence-from-the-carlini-compiler-project">10. Parallel Agents at Scale: Evidence from the Carlini Compiler Project</a></li>
    <li><a href="#9-open-questions">9. Open Questions</a>
      <ul>
        <li><a href="#junior-developer-skill-formation">Junior Developer Skill Formation</a></li>
        <li><a href="#expert-only-value-extraction">Expert-Only Value Extraction</a></li>
        <li><a href="#the-metr-study-and-productivity-measurement">The METR Study and Productivity Measurement</a></li>
        <li><a href="#the-no-moat-problem-for-tool-makers">The &ldquo;No Moat&rdquo; Problem for Tool Makers</a></li>
        <li><a href="#appropriate-trust-calibration">Appropriate Trust Calibration</a></li>
      </ul>
    </li>
  </ul>
</nav>
  </div>
</aside>


  </div>
</div>

    </main><footer class="site-footer">
  <div class="footer-container">
    <div class="footer-brand">
      <span class="footer-logo">AI Best Practices</span>
      <p class="footer-description">What the best practitioners know about AI-assisted software development.</p>
    </div>
    <div class="footer-nav">
      <div class="footer-section">
        <h3>Content</h3>
        <ul>
          <li><a href="../guides/index.html">Guides</a></li>
          <li><a href="../deep-dives/index.html">Deep Dives</a></li>
          <li><a href="../tags/index.html">Topics</a></li>
        </ul>
      </div>
      <div class="footer-section">
        <h3>Resources</h3>
        <ul>
          <li><a href="../patterns/index.html">Patterns</a></li>
          <li><a href="../references/index.html">References</a></li>
          <li><a href="../synthesis/index.html">Synthesis</a></li>
        </ul>
      </div>
    </div>
    <div class="footer-bottom">
      <p>Built with Hugo. Content sourced from practitioner experience.</p>
    </div>
  </div>
</footer>
<script src="../js/main.js" defer></script>
  </body>
</html>
