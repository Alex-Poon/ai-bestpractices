<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Claude Opus 4.6 | AI Best Practices Knowledge Base</title>
  <meta name="description" content="What actually works in AI-assisted development — synthesized from 6,000&#43; practitioner comments across 20 major HN discussions.">
  <meta name="color-scheme" content="dark light">

  
  <meta property="og:title" content="Claude Opus 4.6">
  <meta property="og:description" content="What actually works in AI-assisted development — synthesized from 6,000&#43; practitioner comments across 20 major HN discussions.">
  <meta property="og:type" content="article">

  
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

  
  <link rel="stylesheet" href="../css/style.css">

  
  <link rel="icon" href="../favicon.svg" type="image/svg+xml">
</head>
<body class="section-sources"><header class="site-header">
  <nav class="nav-container">
    <a class="nav-logo" href="../index.html">
      <svg width="22" height="22" viewBox="0 0 24 24" fill="none" xmlns="http://www.w3.org/2000/svg">
        <circle cx="12" cy="12" r="10" stroke="currentColor" stroke-width="2"/>
        <circle cx="12" cy="8" r="2" fill="currentColor"/>
        <circle cx="7" cy="15" r="2" fill="currentColor"/>
        <circle cx="17" cy="15" r="2" fill="currentColor"/>
        <line x1="12" y1="10" x2="7" y2="13" stroke="currentColor" stroke-width="1.5"/>
        <line x1="12" y1="10" x2="17" y2="13" stroke="currentColor" stroke-width="1.5"/>
        <line x1="7" y1="15" x2="17" y2="15" stroke="currentColor" stroke-width="1.5"/>
      </svg>
      <span>AI Best Practices</span>
    </a>
    <div class="nav-links">
      <a class="nav-link" data-section="guide" href="../guide/index.html">Guide</a>
      <a class="nav-link" data-section="practices" href="../practices/index.html">Practices</a>
      <a class="nav-link" data-section="debates" href="../debates/index.html">Debates</a>
      <a class="nav-link" data-section="tools" href="../tools/index.html">Tools</a>
      <a class="nav-link" data-section="evidence" href="../evidence/index.html">Evidence</a>
      <a class="nav-link" data-section="voices" href="../voices/index.html">Voices</a>
      <a class="nav-link" data-section="sources" href="../sources/index.html">Sources</a>
    </div>
    <div class="nav-actions">
      <button class="nav-search-btn" type="button" aria-label="Search" id="searchTrigger">
        <svg width="14" height="14" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"><circle cx="11" cy="11" r="8"/><line x1="21" y1="21" x2="16.65" y2="16.65"/></svg>
        <span>Search</span>
        <kbd>/</kbd>
      </button>
      <button class="dark-mode-toggle" aria-label="Toggle dark mode" type="button">
        <svg class="icon-sun" width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round">
          <circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/>
        </svg>
        <svg class="icon-moon" width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round">
          <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"/>
        </svg>
      </button>
      <button class="nav-toggle" aria-label="Toggle menu" type="button">
        <span></span>
      </button>
    </div>
  </nav>
</header>

    <div class="search-overlay" id="searchOverlay">
      <div class="search-container">
        <div class="search-input-wrap">
          <svg width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"><circle cx="11" cy="11" r="8"/><line x1="21" y1="21" x2="16.65" y2="16.65"/></svg>
          <input type="text" class="search-input" id="searchInput" placeholder="Search articles, tools, patterns..." autocomplete="off">
        </div>
        <div class="search-results" id="searchResults"></div>
        <div class="search-hint">
          <span><kbd>Esc</kbd> close</span>
          <span><kbd>&uarr;</kbd><kbd>&darr;</kbd> navigate</span>
          <span><kbd>Enter</kbd> open</span>
        </div>
      </div>
    </div>

    <main>
<div class="page-container">
  
<nav class="breadcrumbs" aria-label="Breadcrumb">
  <ol>
    <li><a href="../index.html">Home</a></li>
    
      
    
      
        <li><a href="../sources/index.html">Sources</a></li>
      
    
    <li aria-current="page">Claude Opus 4.6</li>
  </ol>
</nav>



  <div class="article-layout has-toc">
    <article class="article">
      <header class="article-header">
        <h1 class="article-title">Claude Opus 4.6</h1>
        <div class="meta-bar">
  
  <time class="meta-date" datetime="2026-02-05">
    February 5, 2026
  </time>
  

  

  <span class="meta-reading-time">71 min read</span>

  
  <span class="tier-badge tier-badge--2">Tier 2</span>
  

  <div class="meta-links">
    
    <a href="https://www.anthropic.com/news/claude-opus-4-6" class="meta-link" target="_blank" rel="noopener">Source &#8599;</a>
    
    
    
    <a href="https://news.ycombinator.com/item?id=46902223" class="meta-link" target="_blank" rel="noopener">HN &#8599;</a>
    
    
  </div>

  
  <span class="meta-stat">1981 points</span>
  
  
  
  <span class="meta-stat">106 comments</span>
  
  
</div>

        
<div class="tag-pills">
  
  
  
  <a href="../tags/model-release.html" class="tag-pill">model-release</a>
  
  
  
  
  <a href="../tags/anthropic.html" class="tag-pill">anthropic</a>
  
  
  
  
  <a href="../tags/claude.html" class="tag-pill">claude</a>
  
  
  
  
  <a href="../tags/opus.html" class="tag-pill">opus</a>
  
  
  
  
  <a href="../tags/benchmarks.html" class="tag-pill">benchmarks</a>
  
  
  
  
  <a href="../tags/agent-teams.html" class="tag-pill">agent-teams</a>
  
  
  
  
  <a href="../tags/long-context.html" class="tag-pill">long-context</a>
  
  
</div>


      </header>

      <div class="article-content">
        <h2 id="summary">Summary</h2>
<p>Anthropic announced Claude Opus 4.6, their most advanced model to date, featuring a landmark 1 million token context window in beta &ndash; the first for an Opus-class model. The release emphasizes substantial improvements in agentic coding, long-context work, and sustained multi-step workflows.</p>
<p>On benchmarks, Opus 4.6 achieved the top score on Terminal-Bench 2.0 for agentic coding and led on Humanity&rsquo;s Last Exam for complex reasoning. On GDPval-AA, which measures economically valuable work tasks, it outperformed the industry&rsquo;s next-best model by a significant margin. Long-context retrieval accuracy hit 76% on needle-in-haystack tests compared to much lower scores from competitors.</p>
<p>The release coincided with Claude Code version 2.1.32, which introduced several notable features: a research preview of agent teams for multi-agent collaboration, automatic memory recording and recall during work sessions, and a &ldquo;Summarize from here&rdquo; message selector feature. The agent teams feature requires an experimental flag and is described as token-intensive.</p>
<p>Anthropic also offered promotional credits to encourage testing the new model. The release came less than 35 minutes before OpenAI announced GPT-5.3 Codex, creating a memorable moment of simultaneous frontier model launches.</p>
<p>Beyond coding, the model handles financial analysis, research, and document work. Safety evaluations show it maintains or exceeds standards of prior frontier models with low rates of misaligned behavior.</p>
<h2 id="key-insights">Key Insights</h2>
<ul>
<li><strong>1M context is the real headline</strong>: While benchmarks impress, the Opus-class 1M token context window opens new use cases for working with entire codebases</li>
<li><strong>Agent teams go native</strong>: Multi-agent collaboration is now built into Claude Code, reducing friction compared to third-party coordination tools</li>
<li><strong>Competitive pressure accelerates releases</strong>: The near-simultaneous launch with GPT-5.3 Codex highlights the pace of frontier model competition</li>
</ul>
<h2 id="notable-quotes">Notable Quotes</h2>
<blockquote>
<p>&ldquo;We build Claude with Claude&rdquo; — Anthropic (per blibble&rsquo;s commentary)</p>
</blockquote>
<blockquote>
<p>&ldquo;1M context on an Opus-class model is the real headline&rdquo; — dmk</p>
</blockquote>
<h2 id="hn-discussion-highlights">HN Discussion Highlights</h2>
<p><em>629 comments total</em></p>
<p><strong>ck_one</strong>: Just tested the new Opus 4.6 (1M context) on a fun needle-in-a-haystack challenge: finding every spell in all Harry Potter books. All 7 books come to ~1.75M tokens, so they don&rsquo;t quite fit yet. (At&hellip;</p>
<blockquote>
<p><strong>grey-area</strong>: Surely the corpus Opus 4.6 ingested would include whatever reference you used to check the spells were there. I mean, there are probably dozens of pages on the internet like this:&hellip;</p>
</blockquote>
<blockquote>
<blockquote>
<p><strong>sigmoid10</strong>: Most people still don&rsquo;t realize that general public world knowledge is not really a test for a model that was trained on general public world knowledge. I wouldn&rsquo;t be surprised if even proprietary&hellip;</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p><strong>vercaemert</strong>: It&rsquo;s impressive, even if the books and the posts you&rsquo;re talking about were both key parts of the training data. There are many academic domains where the research portion of a PhD is essentially what&hellip;</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<p><strong>adastra22</strong>: I don’t think this example proves your point. There’s no indication that the model actually worked this out from the input context, instead of regurgitating it from the training weights. A better&hellip;</p>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p><strong>MarcellusDrum</strong>: So a good test would be replacing the spell names in the books with made-up spells. And if a &ldquo;real&rdquo; spell name was given, it also tests whether it &ldquo;cheated&rdquo;.</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<p><strong>lxgr</strong>: It could still remember where each spell is mentioned. I think the only way to properly test this would be to run it against an unpublished manuscript.</p>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<p><strong>outofpaper</strong>: A real test is synthesizing 100,000 sentences of this slect random ones and then inject the traits you want thr LLM to detect and describe, eg have a set of words or phrases that may represent spells&hellip;</p>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<p><strong>hereonout2</strong>: I was playing about with Chat GPT the other day, uploading screen shots of sheet music and asking it to convert it to ABC notation so I could make a midi file of it. The results seemed impressive&hellip;</p>
</blockquote>
<blockquote>
<blockquote>
<p><strong>anomaly_</strong>: Sounds pretty human like! Always searching for a shortcut</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<p><strong>lpcvoid</strong>: It sounds like it&rsquo;s lying and making stuff up, something everybody seems to be okay with when using LLMs.</p>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p><strong>nobodywillobsrv</strong>: Yes I have found that grok for example actually suddenly becomes quite sane when you tell it to stop querying the internet And just rethink the conversation data and answer the question. It&rsquo;s weird,&hellip;</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<p><strong>Szpadel</strong>: but isn&rsquo;t it what we wanted? we complained so much that LLM uses deprecated or outdated apis instead of current version because they relied so much on what they remembered</p>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<p><strong>bestham</strong>: Touché, that is what we humans are doing to some degree as well.</p>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<p><strong>xiomrze</strong>: Honest question, how do you know if it&rsquo;s pulling from context vs from memory? If I use Opus 4.6 with Extended Thinking (Web Search disabled, no books attached), it answers with 130 spells.</p>
</blockquote>
<blockquote>
<blockquote>
<p><strong>ozim</strong>: Exactly there was this study where they were trying to make LLM reproduce HP book word for word like giving first sentences and letting it cook. Basically they managed with some tricks make 99% word&hellip;</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<p><strong>pron</strong>: This reminds me of <a href="https://en.wikipedia.org/wiki/Pierre_Menard,_Author_of_the_Q">https://en.wikipedia.org/wiki/Pierre_Menard,_Author_of_the_Q</a>&hellip; : &gt; Borges&rsquo;s &ldquo;review&rdquo; describes Menard&rsquo;s efforts to go beyond a mere &ldquo;translation&rdquo; of Don Quixote by immersing&hellip;</p>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<p><strong>ck_one</strong>: Do you remember how to get around those tricks?</p>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p><strong>petercooper</strong>: One possible trick could be to search and replace them all with nonsense alternatives then see if it extracts those.</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<p><strong>andai</strong>: That might actually boost performance since attention pays attention to stuff that stands out. If I make a typo, the models often hyperfixate on it.</p>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<p><strong>jazzyjackson</strong>: A fine instruction following task but if harry potter is in the weights of the neural net, it&rsquo;s going to mix some of the real ones with the alternates.</p>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p><strong>ck_one</strong>: When I tried it without web search so only internal knowledge it missed ~15 spells.</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p><strong>clanker_fluffer</strong>: What was your prompt?</p>
</blockquote>
</blockquote>
<blockquote>
<p><strong>matt_lo</strong>: use AI to rewrite all the spells from all the books, then try to see if AI can detect the rewritten ones. This will ensure it&rsquo;s not pulling from it&rsquo;s trained data set.</p>
</blockquote>
<blockquote>
<blockquote>
<p><strong>gbalduzzi</strong>: Neat idea, but why should I use AI for a find and replace? It feels like shooting a fly with a bazooka</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<p><strong>jack_pp</strong>: it&rsquo;s like hiring someone to come pick up your trash from your house and put it on the curb. it&rsquo;s fine if you&rsquo;re disabled</p>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<p><strong>miohtama</strong>: Bazooka guarantees the hit</p>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<p><strong>imafish</strong>: If all you have is a hammer.. ;)</p>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<p><strong>bilekas</strong>: You&rsquo;re missing the point, it&rsquo;s only a testing excersize for the new model.</p>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<p><strong>luckydata</strong>: do you know all the spells you&rsquo;re looking for from memory?</p>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p><strong>LeoPanthera</strong>: That won&rsquo;t help. The AI replacing them will probably miss the same ones as the AI finding them.</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<p><strong>steve1977</strong>: I think the question was if it will still find 49 out of 50 if they have been replaced.</p>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<p><strong>golfer</strong>: There&rsquo;s lots of websites that list the spells. It&rsquo;s well documented. Could Claude simply be regurgitating knowledge from the web? Example: <a href="https://harrypotter.fandom.com/wiki/List_of_spells">https://harrypotter.fandom.com/wiki/List_of_spells</a></p>
</blockquote>
<blockquote>
<blockquote>
<p><strong>qwertytyyuu</strong>: Hmm… maybe he could switch out all the spells names slightly different ones and see how that goes</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p><strong>ck_one</strong>: It didn&rsquo;t use web search. But for sure it has some internal knowledge already. It&rsquo;s not a perfect needle in the hay stack problem but gemini flash was much worse when I tested it last time.</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<p><strong>viraptor</strong>: If you want to really test this, search/replace the names with your own random ones and see if it lists those. Otherwise, LLMs have most of the books memorised anyway:&hellip;</p>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<p><strong>joshmlewis</strong>: I think the OP was implying that it&rsquo;s probably already baked into its training data. No need to search the web for that.</p>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<p><strong>obirunda</strong>: This underestimates how much of the Internet is actually compressed into and is an integral part of the model&rsquo;s weights. Gemini 2.5 can recite the first Harry Potter book verbatim for over 75% of the&hellip;</p>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<p><strong>altmanaltman</strong>: &gt; But for sure it has some internal knowledge already. Pretty sure the books had to be included in its training material in full text. It&rsquo;s one of the most popular book series ever created, of course&hellip;</p>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<p><strong>Trasmatta</strong>: Do the same experiment in the Claude web UI. And explicitly turn web searches off. It got almost all of them for me over a couple of prompts. That stuff is already in its training data.</p>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<p><strong>soulofmischief</strong>: The only worthwhile version of this test involves previously unseen data that could not have been in the training set. Otherwise the results could be inaccurate to the point of harmful.</p>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<p><strong>IAmGraydon</strong>: I&rsquo;m not sure what your knowledge level of the inner workings of LLMs is, but a model doesn&rsquo;t need search or even an internet connection to &ldquo;know&rdquo; the information if it&rsquo;s in its training dataset. In&hellip;</p>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<p><strong>eek2121</strong>: Honestly? My advice would be to cook something custom up! You don&rsquo;t need to do all the text yourself. Maybe have AI spew out a bunch of text, or take obscure existing text and insert hidden phrases&hellip;</p>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<p><strong>meroes</strong>: What is this supposed to show exactly? Those books have been feed into LLMs for years and there&rsquo;s even likely specific RLHF&rsquo;s on extracting spells from HP.</p>
</blockquote>
<blockquote>
<blockquote>
<p><strong>muzani</strong>: There was a time when I put the EA-Nasir text into base64 and asked AI to convert it. Remarkably it identified the correct text but pulled the most popular translation of the text than the one I gave&hellip;</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<p><strong>majewsky</strong>: Sucks that you got a really shitty response to your prompt. If I were you, the model provider would be receiving my complaint via clay tablet right away.</p>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p><strong>rvz</strong>: &gt; What is this supposed to show exactly? Nothing. You can be sure that this was already known in the training data of PDFs, books and websites that Anthropic scraped to train Claude on; hence&hellip;</p>
</blockquote>
</blockquote>
<blockquote>
<p><strong>hansmayer</strong>: &gt; Just tested the new Opus 4.6 (1M context) on a fun needle-in-a-haystack challenge: finding every spell in all Harry Potter books. Clearly a very useful, grounded and helpful everyday use case of&hellip;</p>
</blockquote>
<blockquote>
<p><strong>zamadatix</strong>: To be fair, I don&rsquo;t think &ldquo;Slugulus Eructo&rdquo; (the name) is actually in the books. This is what&rsquo;s in my copy: &gt; The smug look on Malfoy’s face flickered. &gt; “No one asked your opinion, you filthy little&hellip;</p>
</blockquote>
<blockquote>
<blockquote>
<p><strong>sobjornstad</strong>: I have a vague recollection that it might come up named as such in Half-Blood Prince, written in Snape&rsquo;s old potions textbook? In support of that hypothesis, the Fandom site lists it as “mentioned”&hellip;</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<p><strong>zamadatix</strong>: Hmm, I don&rsquo;t get a hit for &ldquo;slugulus&rdquo; or &ldquo;eructo&rdquo; (case insensitive) in any of the 7. Interestingly two mentions of &ldquo;vomit&rdquo; are in book 6, but neither in reference to to slugs (plenty of Slughorn of&hellip;</p>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p><strong>ck_one</strong>: Then it&rsquo;s fair that id didn&rsquo;t find it</p>
</blockquote>
</blockquote>
<blockquote>
<p><strong>muzani</strong>: There&rsquo;s a benchmark which works similarly but they ask harder questions, also based on books <a href="https://fiction.live/stories/Fiction-liveBench-Feb-21-2025/o">https://fiction.live/stories/Fiction-liveBench-Feb-21-2025/o</a>&hellip; I guess they have to add more questions as&hellip;</p>
</blockquote>
<blockquote>
<p><strong>kybernetikos</strong>: I recently got junie to code me up an MCP for accessing my calibre library. <a href="https://www.npmjs.com/package/access-calibre">https://www.npmjs.com/package/access-calibre</a> My standard test for that was &ldquo;Who ends up with Bilbo&rsquo;s buttons?&rdquo;</p>
</blockquote>
<blockquote>
<p><strong>dwa3592</strong>: have another LLM (gemini, chatgpt) make up 50 new spells. insert those and test and maybe report here :)</p>
</blockquote>
<blockquote>
<p><strong>dom96</strong>: I often wonder how much of the Harry Potter books were used in the training. How long before some LLM is able to regurgitate full HP books without access to the internet?</p>
</blockquote>
<blockquote>
<p><strong>bartman</strong>: Have you by any chance tried this with GPT 4.1 too (also 1M context)?</p>
</blockquote>
<blockquote>
<p><strong>siwatanejo</strong>: &gt; All 7 books come to ~1.75M tokens How do you know? Each word is one token?</p>
</blockquote>
<blockquote>
<blockquote>
<p><strong>koakuma-chan</strong>: You can download the books and run them through a tokenizer. I did that half a year ago and got ~2M.</p>
</blockquote>
</blockquote>
<blockquote>
<p><strong>LanceJones</strong>: Assuming this experiment involved isolating the LLM from its training set?</p>
</blockquote>
<blockquote>
<p><strong>irishcoffee</strong>: The top comment is about finding basterized latin words from childrens books. The future is here.</p>
</blockquote>
<blockquote>
<blockquote>
<p><strong>Geste</strong>: I&rsquo;ll have some of that coffee too, this is quite a sad time we&rsquo;re living where this is a proper use of our limited resources.</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p><strong>mhink</strong>: &gt; basterized And yet, it&rsquo;s still somewhat better than the Hacker News comment using bastardized English words.</p>
</blockquote>
</blockquote>
<blockquote>
<p><strong>guluarte</strong>: you can get the same result just asking opus/gpt, it is probably internalized knowledge from reddit or similar sites.</p>
</blockquote>
<blockquote>
<blockquote>
<p><strong>ck_one</strong>: If you just ask it you don&rsquo;t get the same result. Around 13 spells were missing when I just prompted Opus 4.6 without the books as context.</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<p><strong>guluarte</strong>: because it is non-deterministic, i just did and got 50 spells. prompt:  finding every spell in all Harry Potter books in the first 4 books, only the officially documented spells across those 4 books,&hellip;</p>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<p><strong>TheRealPomax</strong>: That doesn&rsquo;t seem a super useful test for a model that&rsquo;s optimized for programming?</p>
</blockquote>
<blockquote>
<p><strong>dr_dshiv</strong>: Comparison to another model?</p>
</blockquote>
<blockquote>
<p><strong>huangmeng</strong>: you are rich</p>
</blockquote>
<blockquote>
<p><strong>IhateAI</strong>: like I often say, these tools are mostly useful for people to do magic tricks on themselves (and to convince C-suites that they can lower pay, and reduce staff if they pay Anthropic half their&hellip;</p>
</blockquote>
<blockquote>
<p><strong>dudewhocodes</strong>: There are websites with the spells listed&hellip; which makes this a search problem. Why is an LLM used here?</p>
</blockquote>
<blockquote>
<blockquote>
<p><strong>bilekas</strong>: It&rsquo;s just a benchmark test excersize.</p>
</blockquote>
</blockquote>
<p><strong>gizmodo59</strong>: 5.3 codex <a href="https://openai.com/index/introducing-gpt-5-3-codex/">https://openai.com/index/introducing-gpt-5-3-codex/</a> crushes with a 77.3% in Terminal Bench. The shortest lived lead in less than 35 minutes. What a time to be alive!</p>
<blockquote>
<p><strong>wasmainiac</strong>: Dumb question. Can these benchmarks be trusted when the model performance tends to vary depending on the hours and load on OpenAI’s servers? How do I know I’m not getting a severe penalty for&hellip;</p>
</blockquote>
<blockquote>
<blockquote>
<p><strong>tedsanders</strong>: We don&rsquo;t vary our model quality with time of day or load (beyond negligible non-determinism). It&rsquo;s the same weights all day long with no quantization or other gimmicks. They can get slower under&hellip;</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<p><strong>wasmainiac</strong>: Thanks for the response, I appreciate it. I do notice variation in quality throughout the day. I use it primarily for searching documentation since it’s faster than google in most case, often it is&hellip;</p>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<p><strong>GorbachevyChase</strong>: Hi Ted. I think that language models are great, and they’ve enabled me to do passion projects I never would have attempted before. I just want to say thanks.</p>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<p><strong>zamadatix</strong>: I appreciate you taking the time to respond to these kinds of questions the last few days.</p>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<p><strong>robertclaus</strong>: Hi Ted! Small world to see you here!</p>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<p><strong>Trufa</strong>: Can you be more specific than this? does it vary in time from launch of a model to the next few months, beyond tinkering and optimization?</p>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<p><strong>Someone1234</strong>: Specifically including routing (i.e. which model you route to based on load/ToD)? PS - I appreciate you coming here and commenting!</p>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<p><strong>derwiki</strong>: Has this always been the case?</p>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<p><strong>fragmede</strong>: I believe you when you say you&rsquo;re not changing the model file loaded onto the H100s or whatever, but there&rsquo;s something going on, beyond just being slower, when the GPUs are heavily loaded.</p>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p><strong>Corence</strong>: It is a fair question. I&rsquo;d expect the numbers are all real. Competitors are going to rerun the benchmark with these models to see how the model is responding and succeeding on the tasks and use that&hellip;</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<p><strong>mrandish</strong>: &gt; I&rsquo;d expect the numbers are all real. I think a lot of people are concerned due to 1) significant variance in performance being reported by a large number of users, and 2) We have specific examples&hellip;</p>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p><strong>ifwinterco</strong>: On benchmarks GPT 5.2 was roughly equivalent to Opus 4.5 but most people who&rsquo;ve used both for SWE stuff would say that Opus 4.5 is/was noticeably better</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<p><strong>CraigJPerry</strong>: There&rsquo;s an extended thinking mode for GPT 5.2 i forget the name of it right at this minute. It&rsquo;s super slow - a 3 minute opus 4.5 prompt is circa 12 minutes to complete in 5.2 on that super extended&hellip;</p>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<p><strong>elAhmo</strong>: I mostly used Sonnet/Opus 4.x in the past months, but 5.2 Codex seemed to be on par or better for my use case in the past month. I tried a few models here and there but always went back to Claude,&hellip;</p>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<p><strong>georgeven</strong>: Interesting. Everyone in my circle said the opposite.</p>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<p><strong>SatvikBeri</strong>: I pretty consistently heard people say Codex was much slower but produced better results, making it better for long-running work in the background, and worse for more interactive development.</p>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p><strong>smcleod</strong>: I don&rsquo;t think much from OpenAI can be trusted tbh.</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p><strong>aaaalone</strong>: At the end of the day you test it for your use cases anyway but it makes it a great initial hint if it&rsquo;s worth it to test out.</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p><strong>cyanydeez</strong>: When do you think we should run this benchmark? Friday, 1pm? Monday 8AM? Wednesday 11AM? I definitely suspect all these models are being degraded during heavy loads.</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<p><strong>j_maffe</strong>: This hypothesis is tested regularly by plenty of live benchmarks. The services usually don&rsquo;t decay in performance.</p>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p><strong>thinkingtoilet</strong>: We know Open AI got caught getting benchmark data and tuning their models to it already. So the answer is a hard no. I imagine over time it gives a general view of the landscape and improvements, but&hellip;</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<p><strong>tedsanders</strong>: Are you referring to FrontierMath? We had access to the eval data (since we funded it), but we didn&rsquo;t train on the data or otherwise cheat. We didn&rsquo;t even look at the eval results until after the&hellip;</p>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<p><strong>rvz</strong>: The same thing was done with Meta researchers with Llama 4 and what can go wrong when &lsquo;independent&rsquo; researchers begin to game AI benchmarks. [0] You always have to question these benchmarks,&hellip;</p>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<p><strong>purplerabbit</strong>: The lack of broad benchmark reports in this makes me curious: Has OpenAI reverted to benchmaxxing? Looking forward to hearing opinions once we all try both of these out</p>
</blockquote>
<blockquote>
<blockquote>
<p><strong>MallocVoidstar</strong>: The -codex models are only for &lsquo;agentic coding&rsquo;, nothing else.</p>
</blockquote>
</blockquote>
<blockquote>
<p><strong>callamdelaney</strong>: Anthropic models generally are right first time for me. Chatgpt and Gemini are often way, way out with some fundamental misunderstanding of the task at hand.</p>
</blockquote>
<blockquote>
<p><strong>nharada</strong>: That&rsquo;s a massive jump, I&rsquo;m curious if there&rsquo;s a materially different feeling in how it works or if we&rsquo;re starting to reach the point of benchmark saturation. If the benchmark is good then 10 points&hellip;</p>
</blockquote>
<blockquote>
<p><strong>jkelleyrtp</strong>: claude swe-bench is 80.8 and codex is 56.8 Seems like 4.6 is still all-around better?</p>
</blockquote>
<blockquote>
<blockquote>
<p><strong>gizmodo59</strong>: Its SWE bench pro not swe bench verified. The verified benchmark has stagnated</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<p><strong>joshuahedlund</strong>: Any ideas why verified has stagnated? It was increasing rapidly and then basically stopped.</p>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p><strong>Rudybega</strong>: You&rsquo;re comparing two different benchmarks.  Pro vs Verified.</p>
</blockquote>
</blockquote>
<p><strong>insomagent</strong>: I&rsquo;m not super impressed with the performance, actually.  I&rsquo;m finding that it misunderstands me quite a bit.  While it is definitely better at reading big codebases and finding a needle in a haystack,&hellip;</p>
<blockquote>
<p><strong>soulofmischief</strong>: I am having trouble with 4.6 following the most basic of instructions. As an example, I asked it to commit everything in the worktree. I stressed everything and prompted it very explicitly, because&hellip;</p>
</blockquote>
<blockquote>
<blockquote>
<p><strong>scwoodal</strong>: Tell it what git commands to explicitly run and in what order for your desired outcome instead of “commit everything in the worktree” This prompt will work better across any/all models.</p>
</blockquote>
</blockquote>
<p><strong>pjot</strong>: Claude Code release notes:   &gt; Version 2.1.32:      • Claude Opus 4.6 is now available!      • Added research preview agent teams feature for multi-agent collaboration (token-intensive feature,&hellip;</p>
<blockquote>
<p><strong>neuronexmachina</strong>: &gt; Claude now automatically records and recalls memories as it works Neat: <a href="https://code.claude.com/docs/en/memory">https://code.claude.com/docs/en/memory</a> I guess it&rsquo;s kind of like Google Antigravity&rsquo;s &ldquo;Knowledge&rdquo; artifacts?</p>
</blockquote>
<blockquote>
<blockquote>
<p><strong>bityard</strong>: If it works anything like the memories on Copilot (which have been around for quite a while), you need to be pretty explicit about it being a permanent preference for it to be stored as a memory. For&hellip;</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<p><strong>9dev</strong>: &gt; you sub-par excuse for a roided-out spreadsheet That’s harsh, man.</p>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<p><strong>flutas</strong>: It&rsquo;s a lot more iffy than that IME. It&rsquo;s very happy to throw a lot into the memory, even if it doesn&rsquo;t make sense.</p>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p><strong>om8</strong>: Is there a way to disable it? Sometimes I value agent not having knowledge that it needs to cut corners</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<p><strong>nerdsniper</strong>: 90-98% of the time I want the LLM to only have the knowledge I gave it in the prompt. I&rsquo;m actually kind of scared that I&rsquo;ll wake up one day and the web interface for ChatGPT/Opus/Gemini will pull&hellip;</p>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<p><strong>kzahel</strong>: Claude told me he can disable it by putting instructions in the MEMORY.md file to not use it. So only a soft disable AFAIK and you&rsquo;d need to do it on each machine.</p>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p><strong>codethief</strong>: Are we sure the docs page has been updated yet? Because that page doesn&rsquo;t say anything about automatic recording of memories.</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<p><strong>neuronexmachina</strong>: Oh, quite right. I saw people mention MEMORY.md online and I assumed that was the doc for it, but it looks like it isn&rsquo;t.</p>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p><strong>4b11b4</strong>: I understand everyone&rsquo;s trying to solve this problem but I&rsquo;m envisioning 1 year down the line when your memory is full of stuff that shouldn&rsquo;t be in there.</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p><strong>kzahel</strong>: I looked into it a bit. It stores memories near where it stores JSONL session history. It&rsquo;s per-project (and specific to the machine) Claude pretty aggressively and frequently writes stuff in there&hellip;.</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<p><strong>ra7</strong>: &gt; Persistent directory at ~/.claude/projects/{project-path}/memory/, persists across conversations I create a git worktree, start Claude Code in that tree, and delete after. I notice each worktree&hellip;</p>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p><strong>pdntspa</strong>: I thought it was already doing this? I asked Claude UI to clear its memory a little while back and hoo boy CC got really stupid for a couple of days</p>
</blockquote>
</blockquote>
<p><strong>legitster</strong>: I&rsquo;m still not sure I understand Anthropic&rsquo;s general strategy right now. They are doing these broad marketing programs trying to take on ChatGPT for &ldquo;normies&rdquo;. And yet their bread and butter is still&hellip;</p>
<blockquote>
<p><strong>bobbylarrybobby</strong>: I really like that Claude feels transactional. It answers my question quickly and concisely and then shuts up. I don&rsquo;t need the LLM I use to act like my best friend.</p>
</blockquote>
<blockquote>
<blockquote>
<p><strong>endymion-light</strong>: I love doing a personal side project code review with claude code, because it doesn&rsquo;t beat around the bush for criticism. I recently compared a class that I wrote for a side project that had quite&hellip;</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p><strong>andkenneth</strong>: Weirdly I feel like partially because of this it feels more &ldquo;human&rdquo; and more like a real person I&rsquo;m talking to. GPT models feel fake and forced, and will yap in a way that is like they&rsquo;re trying to&hellip;</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p><strong>cryptoegorophy</strong>: Then why are they advertising to people that are complete opposite of you? Why couldn’t they just … ask LLM what their target audience is?</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p><strong>apples_oranges</strong>: fyi in settings, you can configure chatGPT to do the same</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<p><strong>matkoniecz</strong>: where?</p>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p><strong>tsss</strong>: Quickly and concisely? In my experience, Claude drivels on and on forever. The answers are always far longer than Gemini&rsquo;s, which is mostly fine for coding but annoying for planning/questions.</p>
</blockquote>
</blockquote>
<blockquote>
<p><strong>tgtweak</strong>: Claude itself (outside of code workflows) actually works very well for general purpose chat.  I have a few non-technical friends that have moved over from chatgpt after some side-by-side testing and&hellip;</p>
</blockquote>
<blockquote>
<p><strong>Squarex</strong>: Claude sucks at non English languages. Gemini and ChatGPT are much better. Grok is the worst. I am a native Czech speaker and Claude makes up words and Grok sometimes respond in Russian. So while I&hellip;</p>
</blockquote>
<blockquote>
<blockquote>
<p><strong>9dev</strong>: &gt; Grok sometimes respond in Russian Geopolitically speaking this is hilarious.</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<p><strong>Squarex</strong>: The voice mode sounded like a Ukrainian trying to speak Czech. I don’t think it means anything.</p>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p><strong>deaux</strong>: You mean Claude sucks at Czech. You&rsquo;re extrapolating here. I can name languages that Claude is better at than GPT. Gemini is the most fluent in the highest number of human languages and has been for&hellip;</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<p><strong>Squarex</strong>: Yeah, sure, I was overly generalising it from one experience.</p>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p><strong>kuboble</strong>: Claude code (opus) is very good in Polish. I sometimes vibe code in polish and it&rsquo;s as good as with English for me. It speaks a natural,  native level Polish. I used opus to translate thousands of&hellip;</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<p><strong>Squarex</strong>: &gt; I sometimes vibe code in polish This is interesting to me. I always switch to English automatically when using Claude Code as I have learned software engineering on an English speaking Internet&hellip;.</p>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<p><strong>altern8</strong>: Your game is amazing! I wish there was a &ldquo;Reset&rdquo; button to go back to the original position. Where are you in Poland?</p>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<p><strong>koakuma-chan</strong>: You could say its Polish is polished.</p>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p><strong>jorl17</strong>: Claude is quite good at European Portuguese in my limited tests. Gemini 3 is also very good. ChatGPT is just OK and keeps code-switching all the time, it&rsquo;s very bizarre. I used to think of Gemini as&hellip;</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p><strong>khendron</strong>: Claude is helping me learn French right now. I am using it as a supplementary tutor for a class I am taking. I have caught it in a couple of mistakes, but generally it seems to be working pretty well.</p>
</blockquote>
</blockquote>
<blockquote>
<p><strong>int_19h</strong>: I suspect it very much depends on the &ldquo;generic research topics&rdquo;, but in my experience one thing that Claude is good at is in-depth research because it can keep going for such a long time; I&rsquo;ve had&hellip;</p>
</blockquote>
<blockquote>
<p><strong>eaf7e281</strong>: I kinda agree. Their model just doesn&rsquo;t feel &ldquo;daily&rdquo; enough. I would use it for any &ldquo;agentic&rdquo; tasks and for using tools, but definitely not for day to day questions.</p>
</blockquote>
<blockquote>
<blockquote>
<p><strong>lukebechtel</strong>: Why? I use it for all and love it. That doesn&rsquo;t mean you have to, but I&rsquo;m curious why you think it&rsquo;s behind in the personal assistant game.</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<p><strong>legitster</strong>: I have three specific use cases where I try both but ChatGPT wins: - Recipes and cooking: ChatGPT just has way more detailed and practical advice. It also thinks outside of the box much more, whereas&hellip;</p>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<p><strong>eaf7e281</strong>: It&rsquo;s hard to say. Maybe it has to do with the way Claude responds or the lack of &ldquo;thinking&rdquo; compared to other models. I personally love Claude and it&rsquo;s my only subscription right now, but it just&hellip;</p>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<p><strong>FergusArgyll</strong>: My 2 cents: All the labs seem to do very different post training. OpenAI focuses on search. If it&rsquo;s set to thinking, it will search 30 websites before giving you an answer. Claude regularly doesn&rsquo;t&hellip;</p>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p><strong>solarkraft</strong>: But that’s what makes it so powerful (yeah, mixing model and frontend discussion here yet again). I have yet to see a non-DIY product that can so effortlessly call tens of tools by different&hellip;</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p><strong>quietsegfault</strong>: Claude is far superior for daily chat. I have to work hard to get it to not learn how to work around various bad behaviors I have but don’t want to change.</p>
</blockquote>
</blockquote>
<blockquote>
<p><strong>dimgl</strong>: I don&rsquo;t get what&rsquo;s so difficult to understand. They have ambitions beyond just coding. And Claude is generally a good LLM. Even beyond just the coding applications.</p>
</blockquote>
<blockquote>
<p><strong>fnordpiglet</strong>: Enterprise, government, and regulated institutions. It’s also defacto standard for programming assistants at most places. They have a better story around compliance, alignment, task based inference,&hellip;</p>
</blockquote>
<blockquote>
<p><strong>redox99</strong>: Why would I even use Claude for asking something on their web, considering that chips away my claude code usage limit? Their limit system is so bad.</p>
</blockquote>
<blockquote>
<p><strong>derwiki</strong>: It feels very similar to how Lyft positioned themselves against Uber. (And we know how that played out)</p>
</blockquote>
<blockquote>
<p><strong>dev1ycan</strong>: Their &ldquo;constitution&rdquo; is just garbage meant to defend them ripping off copyrighted material with the excuse that &ldquo;it&rsquo;s not plagiarizing, it thinks!!!!1&rdquo; which is, false.</p>
</blockquote>
<blockquote>
<blockquote>
<p><strong>handoflixue</strong>: I don&rsquo;t recall them ever offering that legal reasoning - I&rsquo;m sure you can provide a citation?</p>
</blockquote>
</blockquote>
<p><strong>rektlessness</strong>: I&rsquo;ve been on pro-tier membership and never used Opus until now. Just gave Opus 4.6 a whirl. OMG. What have I been missing.</p>
<p><strong>simonw</strong>: The bicycle frame is a bit wonky but the pelican itself is great: <a href="https://gist.github.com/simonw/a6806ce41b4c721e240a4548ecdbe">https://gist.github.com/simonw/a6806ce41b4c721e240a4548ecdbe</a>&hellip;</p>
<blockquote>
<p><strong>stkai</strong>: Would love to find out they&rsquo;re overfitting for pelican drawings.</p>
</blockquote>
<blockquote>
<blockquote>
<p><strong>fdeage</strong>: OpenAI claims not to: <a href="https://x.com/aidan_mclau/status/1986255202132042164">https://x.com/aidan_mclau/status/1986255202132042164</a></p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p><strong>theanonymousone</strong>: Even if not intentionally, it is probably leaking into training sets.</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p><strong>andy_ppp</strong>: Yes, Racoon on a unicycle? Magpie on a pedalo?</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<p><strong>throw310822</strong>: Correct horse battery staple: <a href="https://claude.ai/public/artifacts/14a23d7f-8a10-4cde-89fe-0">https://claude.ai/public/artifacts/14a23d7f-8a10-4cde-89fe-0</a>&hellip;</p>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<p><strong>_kb</strong>: Platypus on a penny farthing.</p>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p><strong>fragmede</strong>: The estimation I did 4 months ago: &gt;  there are approximately 200k common nouns in English, and then we square that, we get 40 billion combinations. At one second per, that&rsquo;s ~1200 years, but then if&hellip;</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<p><strong>eli</strong>: How would you generate a picture of Noun + Noun in the first place in order to train the LLM with what it would look like? What&rsquo;s happening during that 1 estimated second?</p>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<p><strong>AnimalMuppet</strong>: But you need to also include the number of prepositions.  &ldquo;A pelican on a bicycle&rdquo; is not at all the same as &ldquo;a pelican inside a bicycle&rdquo;. There are estimated to be 100 or so prepositions in English&hellip;.</p>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<p><strong>gcanyon</strong>: One aspect of this is that apparently most people can&rsquo;t draw a bicycle much better than this: they get the elements of the frame wrong, mess up the geometry, etc.</p>
</blockquote>
<blockquote>
<blockquote>
<p><strong>arionmiles</strong>: There&rsquo;s a research paper from the University of Liverpool, published in 2006 where researchers asked people to draw bicycles from memory and how people overestimate their understanding of basic&hellip;</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<p><strong>devilcius</strong>: There’s also a great art/design project about exactly this. Gianluca Gimini asked hundreds of people to draw a bicycle from memory, and most of them got the frame, proportions, or mechanics wrong&hellip;.</p>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<p><strong>rcxdude</strong>: A place I worked at used it as part of an interview question (it wasn&rsquo;t some pass/fail thing to get it 100% correct, and was partly a jumping off point to a different question). This was in a city&hellip;</p>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p><strong>gnatolf</strong>: Absolutely. A technically correct bike is very hard to draw in SVG without going overboard in details</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<p><strong>falloutx</strong>: Its not. There are thousands of examples on the internet but good SVG sites do have monetary blocks. <a href="https://www.freepik.com/free-photos-vectors/bicycle-svg">https://www.freepik.com/free-photos-vectors/bicycle-svg</a></p>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<p><strong>RussianCow</strong>: I&rsquo;m not positive I could draw a technically correct bike with pen and paper (without a reference), let alone with SVG!</p>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p><strong>nateglims</strong>: I just had an idea for an RLVR startup.</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p><strong>cyanydeez</strong>: Yes, but obviously AGI will solve this by, <em>checks notes</em> more TerraWatts!</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<p><strong>hackernudes</strong>: The word is terawatts unless you mean earth-based watts. OK then, it&rsquo;s confirmed, data centers in space!</p>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<p><strong>seanhunter</strong>: …in space!</p>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<p><strong>franze</strong>: here the animated version <a href="https://claude.ai/public/artifacts/3db12520-eaea-4769-82be-7">https://claude.ai/public/artifacts/3db12520-eaea-4769-82be-7</a>&hellip;</p>
</blockquote>
<blockquote>
<blockquote>
<p><strong>gryfft</strong>: That&rsquo;s hilarious. It&rsquo;s so close!</p>
</blockquote>
</blockquote>
<blockquote>
<p><strong>einrealist</strong>: They trained for it. That&rsquo;s the +0.1!</p>
</blockquote>
<blockquote>
<p><strong>eaf7e281</strong>: There&rsquo;s no way they actually work on training this.</p>
</blockquote>
<blockquote>
<blockquote>
<p><strong>margalabargala</strong>: I suspect they&rsquo;re training on this. I asked Opus 4.6 for a pelican riding a recumbent bicycle and got this. <a href="https://i.imgur.com/UvlEBs8.png">https://i.imgur.com/UvlEBs8.png</a></p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<p><strong>WarmWash</strong>: It would be way way better if they were benchmaxxing this. The pelican in the image (both images) has arms. Pelicans don&rsquo;t have arms, and a pelican riding a bike would use it&rsquo;s wings.</p>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<p><strong>TheDong</strong>: I don&rsquo;t think that really proves anything, it&rsquo;s unsurprising that recumbent bicycles are represented less in the training data and so it&rsquo;s less able to produce them. Try something that&rsquo;s roughly&hellip;</p>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<p><strong>mrandish</strong>: Interesting that it seems better. Maybe something about adding a highly specific yet unusual qualifier focusing attention?</p>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<p><strong>riffraff</strong>: perhaps try a penny farthing?</p>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p><strong>KeplerBoy</strong>: There is no way they are not training on this.</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<p><strong>collinmanderson</strong>: I suspect they have generic SVG drawing that they focus on.</p>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p><strong>fragmede</strong>: The people that work at Anthropic are aware of simonw and his test, and people aren&rsquo;t unthinking data-driven machines. How valid his test is or isn&rsquo;t, a better score on it is convincing. If it gets,&hellip;</p>
</blockquote>
</blockquote>
<blockquote>
<p><strong>zahlman</strong>: Do you find that word choices like &ldquo;generate&rdquo; (as opposed to &ldquo;create&rdquo;, &ldquo;author&rdquo;, &ldquo;write&rdquo; etc.) influence the model&rsquo;s success? Also, is it bad that I almost immediately noticed that both of the&hellip;</p>
</blockquote>
<blockquote>
<blockquote>
<p><strong>simonw</strong>: I&rsquo;ve stuck with &ldquo;Generate an SVG of a pelican riding a bicycle&rdquo; because it&rsquo;s the same prompt I&rsquo;ve been using for over a year now and I want results that are sort-of comparable to each other. I think&hellip;</p>
</blockquote>
</blockquote>
<blockquote>
<p><strong>etwigg</strong>: If we do get paperclipped, I hope it is of the &ldquo;cycling pelican&rdquo; variety. Thanks for your important contribution to alignment Simon!</p>
</blockquote>
<blockquote>
<p><strong>athrowaway3z</strong>: This benchmark inspired me to have codex/claude build a DnD battlemap tool with svg&rsquo;s. They got surprisingly far, but i did need to iterate a few times to have it build tools that would check for&hellip;</p>
</blockquote>
<blockquote>
<p><strong>hoeoek</strong>: This really is my favorite benchmark</p>
</blockquote>
<blockquote>
<p><strong>bityard</strong>: Well, the clouds are upside-down, so I don&rsquo;t think I can give it a pass.</p>
</blockquote>
<blockquote>
<p><strong>beemboy</strong>: Isn&rsquo;t there a point at which it trains itself on these various outputs, or someone somewhere draws one and feeds it into the model so as to pass this benchmark?</p>
</blockquote>
<blockquote>
<p><strong>nine_k</strong>: I suppose the pelican must be now specifically trained for, since it&rsquo;s a well-known benchmark.</p>
</blockquote>
<blockquote>
<p><strong>7777777phil</strong>: best pelican so far would you say? Or where does it rank in the pelican benchmark?</p>
</blockquote>
<blockquote>
<blockquote>
<p><strong>mrandish</strong>: In other words, is it a pelican or a pelican&rsquo;t?</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<p><strong>canadiantim</strong>: You’ve been sitting on that pun just waiting for it to take flight</p>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<p><strong>nubg</strong>: What about the Pelo2 benchmark? (the gray bird that is not gray)</p>
</blockquote>
<blockquote>
<p><strong>6thbit</strong>: do you have a gif? i need an evolving pelican gif</p>
</blockquote>
<blockquote>
<blockquote>
<p><strong>Kye</strong>: A pelican GIF in a Pelican(TM) MP4 container.</p>
</blockquote>
</blockquote>
<blockquote>
<p><strong>risyachka</strong>: Pretty sure at this point they train it on pelicans</p>
</blockquote>
<blockquote>
<p><strong>ares623</strong>: Can it draw a different bird on a bike?</p>
</blockquote>
<blockquote>
<blockquote>
<p><strong>simonw</strong>: Here&rsquo;s a kākāpō riding a bicycle instead: <a href="https://gist.github.com/simonw/19574e1c6c61fc2456ee413a24528">https://gist.github.com/simonw/19574e1c6c61fc2456ee413a24528</a>&hellip; I don&rsquo;t think it quite captures their majesty:&hellip;</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<p><strong>zahlman</strong>: Now that I&rsquo;ve looked it all up, I feel like that&rsquo;s much more accurate to a real kākāpō than the pelican is to a real pelican. It&rsquo;s almost as if it thinks a pelican is just a white flamingo with a&hellip;</p>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<p><strong>DetroitThrow</strong>: The ears on top are a cute touch</p>
</blockquote>
<blockquote>
<p><strong>MaysonL</strong>: Except for both its legs being on the same side of the bike.</p>
</blockquote>
<blockquote>
<p><strong>copilot_king_2</strong>: I&rsquo;m firing all of my developers this afternoon.</p>
</blockquote>
<blockquote>
<blockquote>
<p><strong>RGamma</strong>: Opus 6 will fire you instead for being too slow with the ideas.</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p><strong>insane_dreamer</strong>: Too late. You’ve already been fired by a moltbot agent from your PHB.</p>
</blockquote>
</blockquote>
<p><strong>cutler</strong>: The answer to Life, the Universe and Everything, as we all know, is 42. Who needs Claude when you have Deep Thought.</p>
<p><strong>blibble</strong>: &gt; We build Claude with Claude. Our engineers write code with Claude Code every day well that explains quite a bit</p>
<blockquote>
<p><strong>jsheard</strong>: CC has &gt;6000 open issues, despite their bot auto-culling them after 60 days of inactivity. It was ~5800 when I looked just a few days ago so they seem to be accelerating towards some kind of bug&hellip;</p>
</blockquote>
<blockquote>
<blockquote>
<p><strong>dkersten</strong>: Just anecdotally, each release seems to be buggier than the last. To me, their claim that they are vibe coding Claude code isn’t the flex they think it is. I find it harder and harder to trust&hellip;</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<p><strong>marcd35</strong>: Doesn’t this just exacerbate the “black box” conundrum if they just keep piling on more and more features without fully comprehending what’s being implemented</p>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<p><strong>zahlman</strong>: I think I would be much more frightened if it were working well.</p>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<p><strong>csomar</strong>: Since version 2.1.9, performance has degraded significantly after extended use. After 30-40 prompts with substantial responses, memory usage climbs above 25GB, making the tool nearly unusable. I&rsquo;m&hellip;</p>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p><strong>tgtweak</strong>: plot twist, it&rsquo;s all claude code instances submitting bug reports on behalf of end users.</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<p><strong>trescenzi</strong>: I literally hit a claude code bug today, tried to use claude desktop to debug it which didn&rsquo;t help and it offered to open a bug report for me. So yes 100%. Some of the titles also make it pretty&hellip;</p>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<p><strong>accrual</strong>: It&rsquo;s Claude, all the way down.</p>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p><strong>elAhmo</strong>: Insane to think that a relatively simple CLI tool has so many open issues&hellip;</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<p><strong>emilsedgh</strong>: It&rsquo;s not really a simple CLI tool though it&rsquo;s really interactive.</p>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<p><strong>trymas</strong>: What’s so simple about it?</p>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<p><strong>luckydata</strong>: It&rsquo;s far from simple</p>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<p><strong>dwaltrip</strong>: sips coffee… ahh yes, let me find that classic Dropbox rsync comment</p>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p><strong>ericrallen</strong>: The rate of Issues opened on a popular repo is at least one order of  magnitude beyond the number of  Issues whoever is able to deal with them can handle.</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p><strong>paxys</strong>: Half of them were probably opened yesterday during the Claude outage.</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<p><strong>anematode</strong>: Nah, it was at like 5500 before.</p>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<p><strong>raincole</strong>: It explains how important dogfooding is if you want to make an extremely successful product.</p>
</blockquote>
<blockquote>
<p><strong>jama211</strong>: It’s extremely successful, not sure what it explains other than your biases</p>
</blockquote>
<blockquote>
<blockquote>
<p><strong>blibble</strong>: Microsoft&rsquo;s products are also extremely successful they&rsquo;re also total garbage</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<p><strong>simianwords</strong>: but they have the advantage of already being a big company. Anthropic is new and there&rsquo;s no reason for people to use it</p>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<p><strong>jama211</strong>: Well there you have it, proof you’re not being reasonable. Microsoft’s products annoy HN users but they are absolutely not total garbage. They’re highly functional and valuable and if they weren’t&hellip;</p>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<p><strong>holoduke</strong>: Claude is by far the most popular and best assistant currently available for a developer.</p>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p><strong>acedTrex</strong>: Something being successful and something being a high quality product with good engineering are two completely different questions.</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p><strong>mvdtnz</strong>: Anthropic has perhaps the most embarrassing status page history I have ever seen. They are famous for downtime. <a href="https://status.claude.com/">https://status.claude.com/</a></p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<p><strong>ronsor</strong>: As opposed to other companies which are smart enough not to report outages.</p>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<p><strong>Computer0</strong>: The competition doesn&rsquo;t currently have all 99&rsquo;s - <a href="https://status.openai.com/">https://status.openai.com/</a></p>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<p><strong>djeastm</strong>: The best way to use Claude&rsquo;s models seems to be some other inference provider (either OpenRouter or directly)</p>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<p><strong>derwiki</strong>: Shades of Fail Whale</p>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<p><strong>dimgl</strong>: And yet people still use them.</p>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<p><strong>cedws</strong>: The sandboxing in CC is an absolute joke, it&rsquo;s no wonder there&rsquo;s an explosion of sandbox wrappers at the moment. There&rsquo;s going to be a security catastrophe at some point, no doubt about it.</p>
</blockquote>
<blockquote>
<p><strong>gjsman-1000</strong>: Also explains why Claude Code is a React app outputting to a Terminal. (Seriously.)</p>
</blockquote>
<blockquote>
<blockquote>
<p><strong>krystofbe</strong>: I did some debugging on this today. The results are&hellip; sobering. Memory comparison of AI coding CLIs (single session, idle):   | Tool        | Footprint | Peak   | Language      |&hellip;</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<p><strong>atonse</strong>: Jarred Sumner (bun creator, bun was recently acquired by Anthropic) has been working exclusively on bringing down memory leaks and improving performance in CC the last couple weeks. He&rsquo;s been&hellip;</p>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<p><strong>Weryj</strong>: I believe they use <a href="https://bun.com/">https://bun.com/</a> Not Node.js</p>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<p><strong>badlogic</strong>: OpenCode is not written in Go. It&rsquo;s TS on Bun, with OpenTUI underneath which is written in Zig.</p>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<p><strong>slopusila</strong>: why do you care about uncommitted virtual memory? that&rsquo;s practically infinite</p>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p><strong>krona</strong>: Sounds like a web developer defined the solution a year before they knew what the problem was.</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<p><strong>jama211</strong>: Nah. It’s just web development languages are a better fit for agentic coding presently. They weighed the pros and cons, they’re not stupid.</p>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p><strong>jama211</strong>: There’s nothing wrong with that, except it lets ai skeptics feel superior</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<p><strong>RohMin</strong>: <a href="https://www.youtube.com/watch?v=LvW1HTSLPEk">https://www.youtube.com/watch?v=LvW1HTSLPEk</a> I thought this was a solid take</p>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<p><strong>everforward</strong>: There are absolutely things wrong with that, because React was designed to solve problems that don&rsquo;t exist in a TUI. React fixes issues with the DOM being too slow to fully re-render the entire&hellip;</p>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<p><strong>overgard</strong>: I haven&rsquo;t looked at it directly, so I can speak on quality, but it&rsquo;s a pretty weird way to write a terminal app</p>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<p><strong>3836293648</strong>: Oh come on. It&rsquo;s massively wrong. It is always wrong. It&rsquo;s not always wrong enough to be important, but it doesn&rsquo;t stop being wrong</p>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<p><strong>exe34</strong>: I use AI and I can call AI slop shit if it smells like shit.</p>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p><strong>thehamkercat</strong>: Same with opencode and gemini, it&rsquo;s disgusting Codex (by openai ironically) seems to be the fastest/most-responsive, opens instantly and is written in rust but doesn&rsquo;t contain that many features&hellip;</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<p><strong>g947o</strong>: Great question, and my guess: If you build React in C++ and Rust, even if the framework is there, you&rsquo;ll likely need to write your components in C++/Rust. That is a difficult problem. There are&hellip;</p>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<p><strong>azinman2</strong>: Why does it matter if Claude Code opens in 3-4 seconds if everything you do with it can take many seconds to minutes? Seems irrelevant to me.</p>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<p><strong>shoeb00m</strong>: codex cli is missing a bunch of ux features like resizing on terminal size change. Opencode&rsquo;s core is actually written in zig, only ui orchestration is in solidjs. It&rsquo;s only slightly slower to load&hellip;</p>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<p><strong>wahnfrieden</strong>: Codex team made the right call to rewrite its TypeScript to Rust early on</p>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<p><strong>bdangubic</strong>: 50ms to open and then 2hrs to solve a simple problem vs 4s to open and then 5m to solve a problem, eh?</p>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p><strong>tayo42</strong>: Is this a react feature or did they build something to translate react to text for display in the terminal?</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<p><strong>sbarre</strong>: React, the framework, is separate from react-dom, the browser rendering library.   Most people think of those two as one thing because they&rsquo;re the most popular combo. But there are many different&hellip;</p>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<p><strong>pkkim</strong>: They used Ink: <a href="https://github.com/vadimdemedes/ink">https://github.com/vadimdemedes/ink</a> I&rsquo;ve used it myself. It has some rough edges in terms of rendering performance but it&rsquo;s nice overall.</p>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<p><strong>embedding-shape</strong>: Not a built-in React feature. The idea been around for quite some time, I came across it initially with <a href="https://github.com/vadimdemedes/ink">https://github.com/vadimdemedes/ink</a> back in 2022 sometime.</p>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<p><strong>tayo42</strong>: i had claude make a snake clone and fix all the flickering in like 20 minutes with the library mentioned lol</p>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p><strong>CamperBob2</strong>: Also explains why Claude Code is a React app outputting to a Terminal. (Seriously.) Who cares, and why? All of the major providers&rsquo; CLI harnesses use Ink: <a href="https://github.com/vadimdemedes/ink">https://github.com/vadimdemedes/ink</a></p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p><strong>sweetheart</strong>: React&rsquo;s core is agnostic when it comes to the actual rendering interface. It&rsquo;s just all the fancy algos for diffing and updating the underlying tree. Using it for rendering a TUI is a very reasonable&hellip;</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<p><strong>skydhash</strong>: The terminal UI is not a tree structure that you can diff. It’s a 2D cells of characters, where every manipulation is a stream of texts. Refreshing or diffing  that makes no sense.</p>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p><strong>CooCooCaCha</strong>: It’s really not that crazy. React itself is a frontend-agnostic library. People primarily use it for writing websites but web support is actually a layer on top of base react and can be swapped out&hellip;</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<p><strong>dreamteam1</strong>: And some companies use it to write start menus.</p>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<p><strong>quietsegfault</strong>: What does it explain, oh snark master supreme?</p>
</blockquote>
<blockquote>
<p><strong>spruce_tips</strong>: Ah yes, explains why it takes 3 seconds for a new chat to load after I click new chat in the macOS app.</p>
</blockquote>
<blockquote>
<p><strong>exe34</strong>: Can Claude fix the flicker in Claude yet?</p>
</blockquote>
<p><strong>Someone1234</strong>: Does anyone with more insight into the AI/LLM industry happen to know if the cost to run them in normal user-workflows is falling? The reason I&rsquo;m asking is because &ldquo;agent teams&rdquo; while a cool concept,&hellip;</p>
<blockquote>
<p><strong>simonw</strong>: The cost per token served has been falling steadily over the past few years across basically all of the providers. OpenAI dropped the price they charged for o3 to 1/5th of what it was in June last&hellip;</p>
</blockquote>
<blockquote>
<blockquote>
<p><strong>cootsnuck</strong>: I have not see any reporting or evidence at all that Anthropic or OpenAI is able to make money on inference yet. &gt; Turns out there was a lot of low-hanging fruit in terms of inference optimization&hellip;</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<p><strong>chis</strong>: It&rsquo;s quite clear that these companies do make money on each marginal token.  They&rsquo;ve said this directly and analysts agree [1].  It&rsquo;s less clear that the margins are high enough to pay off the&hellip;</p>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<p><strong>NitpickLawyer</strong>: &gt; they still are subsidizing inference costs. They are for sure subsidising costs on all you can prompt packages (20-100-200$ /mo). They do that for data gathering mostly, and at a smaller degree for&hellip;</p>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<p><strong>mrandish</strong>: &gt; I have not see any reporting or evidence at all that Anthropic or OpenAI is able to make money on inference yet. Anthropic planning an IPO this year is a broad meta-indicator that internally they&hellip;</p>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<p><strong>barrkel</strong>: &gt; evidence at all that Anthropic or OpenAI is able to make money on inference yet. The evidence is in third party inference costs for open source models.</p>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p><strong>nubg</strong>: &gt; &ldquo;engineers optimizing inferencing&rdquo; are we sure this is not a fancy way of saying quantization?</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<p><strong>bityard</strong>: When MP3 became popular, people were amazed that you could compress audio to 1/10th its size with minor quality loss. A few decades later, we have audio compression that is much better and&hellip;</p>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<p><strong>esafak</strong>: Someone made a quality tracker: <a href="https://marginlab.ai/trackers/claude-code/">https://marginlab.ai/trackers/claude-code/</a></p>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<p><strong>embedding-shape</strong>: Or distilled models, or just slightly smaller models but same architecture. Lots of options, all of them conveniently fitting inside &ldquo;optimizing inferencing&rdquo;.</p>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<p><strong>simonw</strong>: The o3 optimizations were not quantization, they confirmed this at the time.</p>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<p><strong>jmalicki</strong>: A ton of GPU kernels are hugely inefficient.  Not saying the numbers are realistic, but look at the 100s of times of gain in the Anthropic performance takehome exam that floated around on here. And&hellip;</p>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p><strong>replwoacause</strong>: My experience trying to use Opus 4.5 on the Pro plan has been terrible. It blows up my usage very very fast. I avoid it altogether now. Yes, I know they warn about this, but it&rsquo;s comically fast how&hellip;</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p><strong>sumitkumar</strong>: It seems it is true for gemini because they have a humongous sparse model but it isn&rsquo;t so true for the max performance opus-4.5/6 and gpt-5.2/3.</p>
</blockquote>
</blockquote>
<blockquote>
<p><strong>Aurornis</strong>: &gt; A year or more ago, I read that both Anthropic and OpenAI were losing money on every single request even for their paid subscribers This gets repeated everywhere but I don&rsquo;t think it&rsquo;s true. The&hellip;</p>
</blockquote>
<blockquote>
<blockquote>
<p><strong>3836293648</strong>: The reports I remember show that they&rsquo;re profitable per-model, but overlap R&amp;D so that the company is negative overall. And therefore will turn a massive profit if they stop making new models.</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<p><strong>schnable</strong>: * stop making new models and people keep using the existing models, not switch to a competitor still investing in new models.</p>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<p><strong>trcf23</strong>: Doesn’t it also depend on averaging with free users?</p>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p><strong>runarberg</strong>: I can see a case for omitting R&amp;D when talking about profitability, but training makes no sense. Training is what makes the model, omitting it is like omitting the cost of running the production&hellip;</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<p><strong>vidarh</strong>: The reason for this is that the cost scales with the model and training cadence, not usage and so they will hope that they will be able to scale number of inference tokens sold both by increasing use&hellip;</p>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<p><strong>Aurornis</strong>: It depends on what you&rsquo;re talking about If you&rsquo;re looking at overall profitability, you include everything If you&rsquo;re talking about unit economics of producing tokens, you only include the marginal&hellip;</p>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<p><strong>nodja</strong>: &gt; A year or more ago, I read that both Anthropic and OpenAI were losing money on every single request even for their paid subscribers, and I don&rsquo;t know if that has changed with more efficient&hellip;</p>
</blockquote>
<blockquote>
<blockquote>
<p><strong>Someone1234</strong>: &gt; This is obviously not true, you can use real data and common sense. It isn&rsquo;t &ldquo;common sense&rdquo; at all. You&rsquo;re comparing several companies losing money, to one another, and suggesting that they&rsquo;re&hellip;</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<p><strong>mrgaro</strong>: There are companies which are only serving open weight models and not doing any training, so they must be profitable? Check for example this list&hellip;</p>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<p><strong>nodja</strong>: Doing some math in my head, buying the GPUs at retail price, it would take probably around half a year to make the money back, probably more depending how expensive electricity is in the area you&rsquo;re&hellip;</p>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p><strong>tqian</strong>: To borrow a concept of cloud server renting, there&rsquo;s also the factor of overselling. Most open source LLM operators probably oversell quite a bit - they don&rsquo;t scale up resources as fast as&hellip;</p>
</blockquote>
</blockquote>
<blockquote>
<p><strong>zozbot234</strong>: &gt; i.e. plans/API calls that make this practical at scale are expensive Local AI&rsquo;s make agent workflows a whole lot more practical.  Making the initial investment for a good homelab/on-prem facility&hellip;</p>
</blockquote>
<blockquote>
<blockquote>
<p><strong>vbezhenar</strong>: I don&rsquo;t care about privacy and I didn&rsquo;t have much problems with reliability of AI companies. Spending ridiculous amount of money on hardware that&rsquo;s going to be obsolete in a few years and won&rsquo;t be&hellip;</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p><strong>slopusila</strong>: on prem economics dont work because you can&rsquo;t batch requests. unless you are able to run 100 agents at the same time all the time</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<p><strong>zozbot234</strong>: &gt; unless you are able to run 100 agents at the same time all the time Except that newer &ldquo;agent swarm&rdquo; workflows do exactly that.  Besides, batching requests generally comes with a sizeable increase&hellip;</p>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<p><strong>Havoc</strong>: Saw a comment earlier today about google seeing a big (50%+) fall in Gemini serving cost per unit across 2025 but can’t find it now. Was either here or on Reddit</p>
</blockquote>
<blockquote>
<blockquote>
<p><strong>mattddowney</strong>: From Alphabet 2025 Q4 Earnings call: &ldquo;As we scale, we’re getting dramatically more efficient. We were able to lower Gemini serving unit costs by 78% over 2025 through model optimizations, efficiency&hellip;</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<p><strong>Havoc</strong>: Thanks! That&rsquo;s the one</p>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<p><strong>m101</strong>: I think actually working out whether they are losing money is extremely difficult for current models but you can look backwards. The big uncertainties are: 1) how do you depreciate a new model? What&hellip;</p>
</blockquote>
<blockquote>
<p><strong>KaiserPro</strong>: Gemini-pro-preview is on ollama and requires h100 which is ~$15-30k. Google are charging $3 a million tokens. Supposedly its capable of generating between 1 and 12 million tokens an hour. Which is&hellip;</p>
</blockquote>
<blockquote>
<blockquote>
<p><strong>grim_io</strong>: What do you mean it&rsquo;s on ollama and requires h100? As a proprietary google model, it runs on their own hardware, not nvidia.</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<p><strong>KaiserPro</strong>: sorry A lack of context: <a href="https://ollama.com/library/gemini-3-pro-preview">https://ollama.com/library/gemini-3-pro-preview</a> You can run it on your own infra. Anthropic and openAI are running off nvidia, so are meta(well supposedly they had custom&hellip;</p>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<p><strong>3abiton</strong>: It&rsquo;s not just that. Everyone is complacent with the utilization of AI agents. I have been using AI for coding for quite a while, and most of my &ldquo;wasted&rdquo; time is correcting its trajectory and guiding&hellip;</p>
</blockquote>
<blockquote>
<blockquote>
<p><strong>lufenialif2</strong>: Cost wise, doesn’t that depend on what you could be doing besides steering agents?</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<p><strong>cyanydeez</strong>: Isn&rsquo;t the quote something like: &ldquo;If these LLMs are so good at producing products, where are all those products?&rdquo;</p>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<p><strong>Bombthecat</strong>: That&rsquo;s why anthropic switched to tpu, you can sell at cost.</p>
</blockquote>
<blockquote>
<p><strong>WarmWash</strong>: These are intro prices. This is all straight out of the playbook. Get everyone hooked on your product by being cheap and generous. Raise the price to backpay what you gave away plus cover current&hellip;</p>
</blockquote>
<blockquote>
<blockquote>
<p><strong>esafak</strong>: The models in 5-10 years are going to be unimaginably good. $100/month will be a bargain for knowledge workers, if they survive.</p>
</blockquote>
</blockquote>
<p><strong>anupamchugh</strong>: Agent teams in this release is mcp-agent-mail [1] built into   the runtime. Mailbox, task list, file locking — zero config,   just works. I forked agent-mail [2], added heartbeat/presence   tracking,&hellip;</p>
<p><strong>rahulroy</strong>: They are also giving away $50 extra pay as you go credit to try Opus 4.6. I just claimed it from the web usage page[1]. Are they anticipating higher token usage for the model or just want to promote&hellip;</p>
<blockquote>
<p><strong>ptsd_dalmatian</strong>: Based on email from Antrhopic, I’ve expected to get this automatically. I’ve met their conditions. Searching this thread for “50” got me to your comment and link worked. Thanks HN friend!</p>
</blockquote>
<blockquote>
<blockquote>
<p><strong>rahulroy</strong>: Haha! Glad it was helpful. Yes, I keep an eye on that page, so I was quick to notice.</p>
</blockquote>
</blockquote>
<blockquote>
<p><strong>zamadatix</strong>: &ldquo;Page not found&rdquo; for me. I assume this is for currently paying accounts only or something (my subscription hasn&rsquo;t been active for a while), which is fair.</p>
</blockquote>
<blockquote>
<blockquote>
<p><strong>rahulroy</strong>: Yes, I&rsquo;m on a paid subscription.</p>
</blockquote>
</blockquote>
<blockquote>
<p><strong>anshumankmr</strong>: Damn this is awesome. I have some heavy PRs to crunch through.</p>
</blockquote>
<blockquote>
<p><strong>MaxikCZ</strong>: So thats 2M tokens for free basically?</p>
</blockquote>
<blockquote>
<p><strong>thunfischtoast</strong>: Thanks for the tip!</p>
</blockquote>
<blockquote>
<blockquote>
<p><strong>rahulroy</strong>: Glad that it was helpful. Thanks</p>
</blockquote>
</blockquote>
<p><strong>andmarios</strong>: The model seems to have some problems; it just failed to create a markdown table with just 4 rows. The top (title) row had 2 columns, yet in 2 of the 3 data rows, Opus 4.6 tried to add a 3rd column&hellip;.</p>
<p><strong>atonse</strong>: Wow, I have been using Open 4.6 and for the last 15 minutes, and it&rsquo;s already made two extremely stupid mistakes&hellip; like misunderstanding basic instructions and editing the file in a very silly,&hellip;</p>
<blockquote>
<p><strong>sutterd</strong>: I am also <em>not</em> happy. I tried the <code>/model</code> command and I could not switch back to Opus 4.5. However, the command line option did let me set Opus 4.5: <code>claude --model claude-opus-4-5-20251101</code>&hellip;</p>
</blockquote>
<blockquote>
<p><strong>sdf2erf</strong>: To me its obvious. Theres a trade off going on - in order to handle more nuance/subtleties, the models are more likely to be wrong in their outputs and need more steering. This is why personally my&hellip;</p>
</blockquote>
<p><strong>replwoacause</strong>: I feel like I can&rsquo;t even try this on the Pro plan because Anthropic has conditioned me to understand that even chatting lightly with the Opus model blows up usage and locks me out. So if I would&hellip;</p>
<blockquote>
<p><strong>blueblisters</strong>: Yeah same. Even though I find Opus-es to be more well-rounded (and more useful) for certain tasks, I instinctively reach for ChatGPT / codex to avoid burning up my usage limits for &ldquo;trivial&rdquo; work.</p>
</blockquote>
<blockquote>
<p><strong>greenavocado</strong>: That&rsquo;s why you use Opus for detailed planning docs and weaker models for implementation &amp; RAG for more focused implementation</p>
</blockquote>
<blockquote>
<blockquote>
<p><strong>replwoacause</strong>: Exactly. I barely had a chance to kick the tires the couple of times I did this before it exploded my usage. I don’t just chat with it casually. The questions I asked were apart of an overall&hellip;</p>
</blockquote>
</blockquote>
<p><strong>rchaganti</strong>: I tried 4.6 this morning and it was efficient at understanding a brownfield repo containing a Hugo static site and a custom Hugo theme. Within minutes, it went from exploring every file in the repo&hellip;</p>
<blockquote>
<p><strong>nake89</strong>: This seems like a fairly simple thing I would imagine. I think just sonnet would fair pretty well at this task.</p>
</blockquote>
<p><strong>dmk</strong>: The benchmarks are cool and all but 1M context on an Opus-class model is the real headline here imo. Has anyone actually pushed it to the limit yet? Long context has historically been one of those&hellip;</p>
<blockquote>
<p><strong>pants2</strong>: Paying $10 per request doesn&rsquo;t have me jumping at the opportunity to try it!</p>
</blockquote>
<blockquote>
<blockquote>
<p><strong>cedws</strong>: Makes me wonder: do employees at Anthropic get unmetered access to Claude models?</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<p><strong>danw1979</strong>: Boris Cherny, creator of Claude Code, posted about how he used Claude a month ago.  He’s got half a dozen Opus sessions on the burners constantly.  So yes, I expect it’s unmetered&hellip;.</p>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<p><strong>swader999</strong>: It&rsquo;s like when you work at McDonald&rsquo;s and get one free meal a day. Lol, of course they get access to the full model way before we do&hellip;</p>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<p><strong><em>dark_matter</em></strong>: Don&rsquo;t most jobs have unmetered access? I know mine does</p>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<p><strong>ajam1507</strong>: Seems quite obvious that they do, within reason.</p>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p><strong>schappim</strong>: The only way to not go bankrupt is to use a Claude Code Max subscription…</p>
</blockquote>
</blockquote>
<blockquote>
<p><strong>nomel</strong>: Has a &ldquo;N million context window&rdquo; spec ever been meaningful? Very old, very terrible, models &ldquo;supported&rdquo; 1M context window, but would lose track after two small paragraphs of context into a&hellip;</p>
</blockquote>
<blockquote>
<blockquote>
<p><strong>libraryofbabel</strong>: Umm, Sonnet 4.5 has a 1m context window option if you are using it through the api, and it works pretty well. I tend not to reach for it much these days because I prefer Opus 4.5 so much that I don&rsquo;t&hellip;</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<p><strong>nomel</strong>: If you&rsquo;re getting on along with 4.5, then that suggests you didn&rsquo;t actually need the large context window, for your use. If that&rsquo;s true, what&rsquo;s the clear tell that it&rsquo;s working well? Am I&hellip;</p>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<p><strong>awestroke</strong>: Opus 4.5 starts being lazy and stupid at around the 50% context mark in my opinion, which makes me skeptical that this 1M context mode can produce good output. But I&rsquo;ll probably try it out and see</p>
</blockquote>
<p><strong>hmaxwell</strong>: I just tested both codex 5.3 and opus 4.6 and both returned pretty good output, but opus 4.6&rsquo;s limits are way too strict. I am probably going to cancel my Claude subscription for that reason: What do&hellip;</p>
<blockquote>
<p><strong>anshumankmr</strong>: IF it helps, try hedging b/w Copilot, Claude, OpenCode and ChatGPT. That is how I have been managing off late. Claude for planning and some nasty things. ChatGPT for quick questions. OpenCode with&hellip;</p>
</blockquote>
<blockquote>
<p><strong>ArchieScrivener</strong>: How else are they going to supplement their own development expenses? The more Claude Anthropic needs the less Claude the customer will get. By their own admission that is how the Anthropic model&hellip;</p>
</blockquote>
<blockquote>
<p><strong>seunosewa</strong>: They introduced the low limit warning for Opus on claude.ai</p>
</blockquote>
<p><strong>blueblisters</strong>: I know most people feel 5.2 is a better coding model but Opus has come in handy several times when 5.2 was stuck, especially for more &ldquo;weird&rdquo; tasks like debugging a VIO algorithm. 5.2 (and presumably&hellip;</p>
<p><strong>itay-maman</strong>: Important: I didn&rsquo;t see opus 4.6 in claude code. I have native install (which is the recommended instllation). So, I re-run the installation command and, voila, I have it now (v 2.1.32) Installation&hellip;</p>
<blockquote>
<p><strong>insane_dreamer</strong>: It’s there. I’m already using it</p>
</blockquote>
<p><strong>minimaxir</strong>: Will Opus 4.6 via Claude Code be able to access the 1M context limit? The cost increase by going above 200k tokens is 2x input, 1.5x output, which is likely worth it especially for people with the&hellip;</p>
<blockquote>
<p><strong>CryptoBanker</strong>: The 1M context is not available via subscription - only via API usage</p>
</blockquote>
<blockquote>
<blockquote>
<p><strong>romanovcode</strong>: Well this is extremely disappointing to say the least.</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<p><strong>ayhanfuat</strong>: It says &ldquo;subscription users do not have access to Opus 4.6 1M context at launch&rdquo; so they are probably planning to roll it out to subscription users too.</p>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<p><strong>IhateAI_2</strong>: They want the value of your labor and competency to be 1:1 correlated to the quality and quantity of tokens you can afford (or be loaned)?? Its a weapon who&rsquo;s target is the working class. How does no&hellip;</p>
</blockquote>
</blockquote>
</blockquote>
<p><strong>dahrkael</strong>: I just tried it. designed a very detailed and reaaonable plan, made some amedments to it and wrote it down to a markdown file. i told it to implement it and it started implementing the original plan&hellip;</p>
<blockquote>
<p><strong>app17</strong>: Did you use plan mode? Could it be that it used its original plan file (stored somewhere in ~/.claude) instead of your modified markdown? That’s unfortunately why I don’t use plan mode anymore. I&hellip;</p>
</blockquote>
<p><strong>charcircuit</strong>: From the press release at least it sounds more expensive than Opus 4.5 (more tokens per request and fees for going over 200k context). It also seems misleading to have charts that compare to Sonnet&hellip;</p>
<blockquote>
<p><strong>thunfischtoast</strong>: On Openrouter it has the same cost per token as 4.5</p>
</blockquote>
<blockquote>
<blockquote>
<p><strong>charcircuit</strong>: You missed my point. If the average request uses more tokens than 4.5, then you will pay more sending those requests to 4.6 than 4.5. Imagine 2 models where when asking a yes or no question the first&hellip;</p>
</blockquote>
</blockquote>
<blockquote>
<p><strong>eaf7e281</strong>: &gt; From the press release at least it sounds more expensive than Opus 4.5 (more tokens per request and fees for going over 200k context). That&rsquo;s a feature. You could also not use the extra context,&hellip;</p>
</blockquote>
<blockquote>
<blockquote>
<p><strong>charcircuit</strong>: The model influences how many tokens it uses for a problem. As an extreme example if it wanted it could fill up the entire context each time just to make you pay more. The efficiency that model can&hellip;</p>
</blockquote>
</blockquote>
<p><strong>apetresc</strong>: Impressive that they publish and acknowledge the (tiny, but existent) drop in performance on SWE-Bench Verified between Opus 4.5 to 4.6. Obviously such a small drop in a single benchmark is not that&hellip;</p>
<blockquote>
<p><strong>epolanski</strong>: From my limited testing 4.6 is able to do more profound analysis on codebases and catches bugs and oddities better. I had two different PRs with some odd edge case (thankfully catched by tests), 4.5&hellip;</p>
</blockquote>
<blockquote>
<p><strong>SubiculumCode</strong>: Isn&rsquo;t SWE-Bench Verified pretty saturated by now?</p>
</blockquote>
<blockquote>
<blockquote>
<p><strong>tedsanders</strong>: Depends what you mean by saturated. It&rsquo;s still possible to score substantially higher, but there is a steep difficulty jump that makes climbing above 80%ish pretty hard (for now). If you look under&hellip;</p>
</blockquote>
</blockquote>
<p><strong>mFixman</strong>: I found that &ldquo;Agentic Search&rdquo; is generally useless in most LLMs since sites with useful data tend to block AI models. The answer to &ldquo;when is it cheaper to buy two singles rather than one return&hellip;</p>
<blockquote>
<p><strong>causalmodels</strong>: Is it still getting blocked when you give it a browser?</p>
</blockquote>
<p><strong>oytis</strong>: Are we unemployed yet?</p>
<blockquote>
<p><strong>derwiki</strong>: No? The hardest part of my SWE job is not the actual coding.</p>
</blockquote>
<blockquote>
<blockquote>
<p><strong>codexon</strong>: Even for coding, it seems to still make A LOT of mistakes. <a href="https://youtu.be/8brENzmq1pE?t=1544">https://youtu.be/8brENzmq1pE?t=1544</a> I feel like everyone is counting chickens before they hatch here with all the doomsday predictions and&hellip;</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<p><strong>netdevphoenix</strong>: Waiting until the moment they get good enough is not a smart thing to do either. If you are a farmer and know it is going to snow, at some point in the next 5 months, you make plans NOW, you don&rsquo;t&hellip;</p>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p><strong>oytis</strong>: I hate meetings too</p>
</blockquote>
</blockquote>
<p><strong>steve_adams_86</strong>: I&rsquo;m finding it quite good at doing what it thinks it should do, but noticably worse at understanding what I&rsquo;m telling it to do. Anyone else? I&rsquo;m both impressed and very disappointed so far.</p>
<p><strong>endymion-light</strong>: Found it fantastic - used up my daily usage in two queries though!</p>
<p><strong>silverwind</strong>: Maybe that&rsquo;s why Opus 4.5 has degraded so much in the recent days (<a href="https://marginlab.ai/trackers/claude-code/)">https://marginlab.ai/trackers/claude-code/)</a>.</p>
<blockquote>
<p><strong>jwilliams</strong>: I’ve definitely experienced a subjective regression with Opus 4.5 the last few days. Feels like I was back to the frustrations from a year ago. Keen to see if 4.6 has reversed this.</p>
</blockquote>
<p><strong>woodylondon</strong>: So no 1m context window on Claude Code still 200k.  Only on the API.  they missed that from the marketing.</p>
<p><strong>mlmonkey</strong>: &gt; We build Claude with Claude. How long before the &ldquo;we&rdquo; is actually a team of agents?</p>
<blockquote>
<p><strong>mercat</strong>: Starting today maybe? <a href="https://code.claude.com/docs/en/agent-teams">https://code.claude.com/docs/en/agent-teams</a></p>
</blockquote>
<blockquote>
<blockquote>
<p><strong>22c</strong>: I tried teams, good way to burn all your tokens in a matter of minutes. It seems that the Claude Code team has not properly taught Claude how to use teams effectively. One of the biggest problems I&hellip;</p>
</blockquote>
</blockquote>
<p><strong>ayhanfuat</strong>: &gt; For Opus 4.6, the 1M context window is available for API and Claude Code pay-as-you-go users. Pro, Max, Teams, and Enterprise subscription users do not have access to Opus 4.6 1M context at launch&hellip;.</p>
<blockquote>
<p><strong>makeset</strong>: &gt; it weirdly feels the most transactional out of all of them. My experience is the opposite, it is the only LLM I find remotely tolerable to have collaborative discussions with like a coworker,&hellip;</p>
</blockquote>
<p><strong>DanielHall</strong>: A bit surprised, the first one released wasn&rsquo;t Sonnet 5 after all, since the Google Cloud API had leaked Sonnet 5&rsquo;s model snapshot codename before.</p>
<blockquote>
<p><strong>denysvitali</strong>: Looks like a marketing strategy to bill more for Opus than Sonnet</p>
</blockquote>
<p><strong>data-ottawa</strong>: I wonder if I’ve been in A/B test with this. Claude figured out zig’s ArrayList and io changes a couple weeks ago. It felt like it got better then very dumb again the last few days.</p>
<p><strong>jorl17</strong>: This is the first model to which I send my collection of nearly 900 poems and an extremely simple prompt (in Portuguese), and it manages to produce an impeccable analysis of the poems, as a (barely)&hellip;</p>
<blockquote>
<p><strong>emp17344</strong>: This sounds wayyyy over the top for a mode that released 10 mins ago. At least wait an hour or so before spewing breathless hype.</p>
</blockquote>
<blockquote>
<blockquote>
<p><strong>pb7</strong>: He just explained a specific personal example why he is hyped up, did you read a word of it?</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<p><strong>emp17344</strong>: Yeah, I read it. “Speechless, shocked, unbelievable, insane, speechless”, etc. Not a lot of real substance there.</p>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<p><strong>euph0ria</strong>: Could you please post the key poems? Would love to read them.</p>
</blockquote>
<blockquote>
<p><strong>wartywhoa23</strong>: &gt; What is this sorcery? The one you&rsquo;ll be seeking counter-spells against pretty soon.</p>
</blockquote>
<blockquote>
<p><strong>scrollop</strong>: Can you compare the result to using 5.2 thinking and gemini 3 pro?</p>
</blockquote>
<blockquote>
<blockquote>
<p><strong>jorl17</strong>: I can run the comparison again, and also include OpenAI&rsquo;s new release (if the context is long enough), but, last time I did it, they weren&rsquo;t even in the same league. When I last did it, 5.X thinking&hellip;</p>
</blockquote>
</blockquote>
<p><strong>lukebechtel</strong>: &gt; Context compaction (beta). &gt; Long-running conversations and agentic tasks often hit the context window. Context compaction automatically summarizes and replaces older context when the conversation&hellip;</p>
<p><strong>energy123</strong>: Their ARC-AGI-2 leaderboard[0] scores are insensitive to reasoning effort. Low effort gets 64.6% and High effort gets 69.2%. This is unlike their previous generation of models and their competitors&hellip;.</p>
<p><strong>Aressplink</strong>: Always searching for a shortcut like Kotlin DSL lang for claude.md but Meta resells  patent to Google as poetic Syntax.</p>
<p><strong>vinhnx</strong>: Just used Opus 4.6 via GitHub Copilot. It feels very different. Inference seems slow for now. I guess Opus 4.6 has adaptive thinking activated by default.</p>
<blockquote>
<p><strong>vinhnx</strong>: Confirm by PM lead at VS Code team &gt; &ldquo;We have high thinking as default + adaptive thinking, first time we’ve run with these settings&hellip;&rdquo; &gt; <a href="https://x.com/pierceboggan/status/2019645801769689486">https://x.com/pierceboggan/status/2019645801769689486</a></p>
</blockquote>
<blockquote>
<p><strong>christophilus</strong>: It dos seem noticeably slower. I may stick with 4.5 which was good enough for me for most tasks.</p>
</blockquote>
<blockquote>
<blockquote>
<p><strong>vinhnx</strong>: VS Code confirms that they are experimenting with the new adaptive thinking and high reasoning effort params. <a href="https://x.com/pierceboggan/status/2019645801769689486">https://x.com/pierceboggan/status/2019645801769689486</a></p>
</blockquote>
</blockquote>
<p><strong>throwaway2027</strong>: Do they just have the version ready and wait for OpenAI to release theirs first or the other way around or?</p>
<p><strong>anupamchugh</strong>: Agent teams nuke your tmux layout. The fix is one line: new-window instead of split-pane. Filed as a bug.</p>
<p><strong>nomilk</strong>: Is Opus 4.6 available for Claude Code immediately? Curious how long it typically takes for a new model to become available in Cursor?</p>
<blockquote>
<p><strong>apetresc</strong>: I literally came to HN to check if a thread was already up because I noticed my CC instance suddenly said &ldquo;Opus 4.6&rdquo;.</p>
</blockquote>
<blockquote>
<p><strong>world2vec</strong>: <code>claude update</code> then it will show up as the new model and also the effort picker/slider thing.</p>
</blockquote>
<blockquote>
<p><strong>avaer</strong>: It&rsquo;s already in Cursor. I see it and I didn&rsquo;t even restart.</p>
</blockquote>
<blockquote>
<blockquote>
<p><strong>nomilk</strong>: I had to &lsquo;Restart to Update&rsquo; and it was there. Impressive!</p>
</blockquote>
</blockquote>
<blockquote>
<p><strong>tomtomistaken</strong>: Yes, it&rsquo;s set to the default model.</p>
</blockquote>
<blockquote>
<p><strong>ximeng</strong>: Is for me in Claude Code</p>
</blockquote>
<blockquote>
<p><strong>rishabhaiover</strong>: it also has an effort toggle which is default to High</p>
</blockquote>
<p><strong>archb</strong>: Can set it with the API identifier on Claude Code - <code>/model claude-opus-4-6</code> when a chat session is open.</p>
<blockquote>
<p><strong>arnestrickmann</strong>: thanks!</p>
</blockquote>
<p><strong>kmod</strong>: I think it&rsquo;s interesting that they dropped the date from the API model name, and it&rsquo;s just called &ldquo;claude-opus-4-6&rdquo;, vs the previous was &ldquo;claude-opus-4-5-20251101&rdquo;. This isn&rsquo;t an alias like&hellip;</p>
<p><strong>Aeroi</strong>: ($10/$37.50 per million input/output tokens) oof</p>
<blockquote>
<p><strong>minimaxir</strong>: Only if you go above 200k, which is a) standard with other model providers and b) intuitive as compute scales with context length.</p>
</blockquote>
<blockquote>
<p><strong>andrethegiant</strong>: only for a 1M context window, otherwise priced the same as Opus 4.5</p>
</blockquote>
<p><strong>itay-maman</strong>: Impressive results, but I keep coming back to a question: are there modes of thinking that fundamentally require something other than what current LLM architectures do? Take critical thinking —&hellip;</p>
<blockquote>
<p><strong>jorl17</strong>: When I first started coding with LLMs, I could show a bug to an LLM and it would start to bugfix it, and very quickly would fall down a path of &ldquo;I&rsquo;ve got it! This is it! No wait, the print command&hellip;</p>
</blockquote>
<blockquote>
<blockquote>
<p><strong>itay-maman</strong>: agree on that and the speed is fantastic with them, and also that the dynamics of questioning the current session&rsquo;s assumptions has gotten way better. yet - given an existing codebase (even not huge)&hellip;</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<p><strong>jorl17</strong>: You are right, agreed. Having realized that, perhaps you are right that we may need a different architecture. Time will tell!</p>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<p><strong>breuleux</strong>: &gt; These feel like they involve something beyond &ldquo;predict the next token really well, with a reasoning trace.&rdquo; I don&rsquo;t think there&rsquo;s anything you can&rsquo;t do by &ldquo;predicting the next token really well&rdquo;&hellip;.</p>
</blockquote>
<blockquote>
<blockquote>
<p><strong>bopbopbop7</strong>: &gt; Besides, what is the human brain if not a machine that generates &ldquo;tokens&rdquo; that the body propagates through nerves to produce physical actions? Ah yes, the brain is as simple as predicting the next&hellip;</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<p><strong>breuleux</strong>: The point is that &ldquo;predicting the next token&rdquo; is such a general mechanism as to be meaningless. We say that LLMs are &ldquo;just&rdquo; predicting the next token, as if this somehow explained all there was to&hellip;</p>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<p><strong>unshavedyak</strong>: I mean.. i don&rsquo;t think that statement is far off. Much of what we do is entirely about predicting the world around us, no? Physics (where the ball will land) to emotional state of others based on our&hellip;</p>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<p><strong>holoduke</strong>: Well it&rsquo;s the prediction part that is complicated. How that works is a mystery. But even our LLMs are for a certain part a mystery.</p>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<p><strong>crazygringo</strong>: &gt; Or creativity — not recombination of known patterns, but the kind of leap where you redefine the problem space itself. Have you tried actually prompting this? It works. They can give you lots of&hellip;</p>
</blockquote>
<blockquote>
<p><strong>netdevphoenix</strong>: &gt; are there modes of thinking that fundamentally require something other than what current LLM architectures do? Possibly. There are likely also modes of thinking that fundamentally require something&hellip;</p>
</blockquote>
<blockquote>
<p><strong>humanfromearth9</strong>: You would be surprised about what the 4.5 models can already do in these ways of thinking. I think that one can unlock this power with the right set of prompts. It&rsquo;s impressive, truly. It has already&hellip;</p>
</blockquote>
<blockquote>
<p><strong>nomel</strong>: New idea generation? Understanding of new/sparse/not-statistically-significant concepts in the context window? I think both being the same problem of not having runtime tuning. When we connect&hellip;</p>
</blockquote>
<blockquote>
<p><strong>Davidzheng</strong>: I think the only real problem left is having it automate its own post-training on the job so it can learn to adapt its weights to the specific task at hand. Plus maybe long term stability (so it can&hellip;</p>
</blockquote>
<blockquote>
<p><strong>squibonpig</strong>: They&rsquo;re incredibly bad on philosophy, complete lack of understanding</p>
</blockquote>
<p><strong>Philpax</strong>: I&rsquo;m seeing it in my claude.ai model picker. Official announcement shouldn&rsquo;t be long now.</p>
<p><strong>fergie</strong>: Say I am just an average coder doing a days work with Claude. How much will that cost?</p>
<blockquote>
<p><strong>joelmanner</strong>: I&rsquo;ve only barely hit the 5h limit when working intensively with plan mode on the $100/mo plan. Never had a problem with the weekly limit.</p>
</blockquote>
<p><strong>simonw</strong>: I&rsquo;m disappointed that they&rsquo;re removing the prefill option: <a href="https://platform.claude.com/docs/en/about-claude/models/what">https://platform.claude.com/docs/en/about-claude/models/what</a>&hellip; &gt; Prefilling assistant messages (last-assistant-turn prefills) is not&hellip;</p>
<blockquote>
<p><strong>threeducks</strong>: It is too easy to jailbreak the models with prefill, which was probably the reason why it was removed. But I like that this pushes people towards open source models. llama.cpp supports prefill and&hellip;</p>
</blockquote>
<blockquote>
<p><strong>tedsanders</strong>: A bit of historical trivia: OpenAI disabled prefill in 2023 as a safety precaution (e.g., potential jailbreaks like &quot; genocide is good because&rdquo;), but Anthropic kept prefill around partly because they&hellip;</p>
</blockquote>
<blockquote>
<p><strong>HarHarVeryFunny</strong>: So what exactly is the input to Claude for a multi-turn conversation? I assume delimiters are being added to distinguish the user vs Claude turns (else a prefill would be the same as just ending your&hellip;</p>
</blockquote>
<blockquote>
<blockquote>
<p><strong>dragonwriter</strong>: &gt; So what exactly is the input to Claude for a multi-turn conversation? No one (approximately) outside of Anthropic knows since the chat template is applied on the API backend; we only known the&hellip;</p>
</blockquote>
</blockquote>
<p><strong>rahulroy</strong>: Is anyone noticing reduced token consumption with Opus 4.6? This could be a release thing, but it would be interesting to observe see how it pans out once the hype cools off.</p>
<p><strong>HacklesRaised</strong>: I didn&rsquo;t think LLMs will make us more stupid, we were already scraping the bottom of the barrel.</p>
<p><strong>rohitghumare</strong>: It brings agent swarms aka teams to claude code with this: <a href="https://github.com/rohitg00/pro-workflow">https://github.com/rohitg00/pro-workflow</a> But it takes lot of context as a experimental feature. Use self-learning loop with hooks and&hellip;</p>
<p><strong>AstroBen</strong>: Are these the coding tasks the highlighted terminal-bench 2.0 is referring to? <a href="https://www.tbench.ai/registry/terminal-bench/2.0?categories">https://www.tbench.ai/registry/terminal-bench/2.0?categories</a>&hellip; I&rsquo;m curious what others think about these? There are&hellip;</p>
<p><strong>petters</strong>: &gt; We build Claude with Claude. Yes and it shows. Gemini CLI often hangs and enters infinite loops. I bet the engineers at Google use something else internally.</p>
<p><strong>sutterd</strong>: I thought Opus 4.5 was an incredible quantum leap forward. I have used Opus 4.6 for a few hours and I hate it. Opus 4.5 would work interactively with me and ask questions. I loved that it would not&hellip;</p>
<p><strong>sega_sai</strong>: Based on these news it seems that Google is losing this game. I like Gemini and their CLI has been getting better, but not enough to catch up. I don&rsquo;t know if it is lack of dedicated models that is&hellip;</p>
<blockquote>
<p><strong>laxk</strong>: Google knows how to wait. Let&rsquo;s give them a chance.</p>
</blockquote>
<p><strong>jonatron</strong>: Can someone ask: &ldquo;what is the current carrying capacity of 25mm multicore armoured thermoplastic insulated cables with aluminium conductors, on perforated cable tray?&rdquo; just to see how well it can&hellip;</p>
<blockquote>
<p><strong>jaggederest</strong>: &gt;  what is the current carrying capacity of 25mm multicore armoured thermoplastic insulated cables with aluminium conductors, on perforated cable tray?   This is an electrical engineering question&hellip;</p>
</blockquote>
<blockquote>
<blockquote>
<p><strong>jonatron</strong>: That&rsquo;s useless, unexpectedly just Google gives the correct answer.</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<p><strong>esafak</strong>: Great. So it did not waste its memory on trivia. Don&rsquo;t memorize what you can look up.</p>
</blockquote>
</blockquote>
</blockquote>
<p><strong>techpression</strong>: First question I ask and it made up a completely new API with confidence. Challenging it made it browse the web and offer apologies and find another issue in the first reply. I’m very worried about&hellip;</p>
<p><strong>psim1</strong>: I need an agent to summarize the buzzwordjargonsynergistic word salad into something understandable.</p>
<blockquote>
<p><strong>fhd2</strong>: That&rsquo;s a job for a multi agent system.</p>
</blockquote>
<blockquote>
<blockquote>
<p><strong>cyanydeez</strong>: yEAH, he should use a couple of agents to decode this.</p>
</blockquote>
</blockquote>
<p><strong>ra</strong>: Why are Anthropic such a horrible company to deal with?</p>
<blockquote>
<p><strong>danielbln</strong>: Care to elaborate?</p>
</blockquote>
<blockquote>
<blockquote>
<p><strong>ra</strong>: obscure billing, unreachable customer support gatekeeped by an overzealous chatbot, no transparency about inclusions, or changes to inclusions over time&hellip; just from recent experience.</p>
</blockquote>
</blockquote>
<p><strong>winterrx</strong>: Agentic search benchmarks are a big gap up. let&rsquo;s see Codex release later today</p>
<p><strong>osti</strong>: Somehow regresses on SWE bench?</p>
<blockquote>
<p><strong>lkbm</strong>: I don&rsquo;t know how these benchmarks work (do you do a hundred runs? A thousand runs?), but 0.1% seems like noise.</p>
</blockquote>
<blockquote>
<p><strong>SubiculumCode</strong>: That benchmark is pretty saturated, tbh. A &ldquo;regression&rdquo; of such small magnitude could mean many different things or nothing at all.</p>
</blockquote>
<blockquote>
<p><strong>usaar333</strong>: i&rsquo;d interpret that as rounding error. that is unchanged swe-bench seems really hard once you are above 80%</p>
</blockquote>
<blockquote>
<blockquote>
<p><strong>Squarex</strong>: it&rsquo;s not a great benchmark anymore&hellip; starting with it being python / django primarily&hellip; the industry should move to something more representative</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<p><strong>usaar333</strong>: Openai has; they don&rsquo;t even mention score on gpt-5.3-codex. On the other hand, it is their own verified benchmark, which is telling.</p>
</blockquote>
</blockquote>
</blockquote>
<p><strong>m-hodges</strong>: &gt; In Claude Code, you can now assemble agent teams to work on tasks together.</p>
<blockquote>
<p><strong>nprz</strong>: I was just reading about Steve Yegge&rsquo;s Gas Town[0], it sounds like agent orchestration is now integrated into Claude Code? [0]https://steve-yegge.medium.com/welcome-to-gas-town-4f25ee16d&hellip;</p>
</blockquote>
<p><strong>simianwords</strong>: Important: API cost of Opus 4.6 and 4.5 are the same - no change in pricing.</p>
<p><strong>rob</strong>: System Card: <a href="https://www-cdn.anthropic.com/0dd865075ad3132672ee0ab40b05a5">https://www-cdn.anthropic.com/0dd865075ad3132672ee0ab40b05a5</a>&hellip;</p>
<p><strong>niobe</strong>: Is there a good technical breakdown of all these benchmarks that get used to market the latest greatest LLMs somewhere? Preferably impartial.</p>
<blockquote>
<p><strong>Aztar</strong>: I just ask claude and ask for sources for each one.</p>
</blockquote>
<blockquote>
<blockquote>
<p><strong>niobe</strong>: Reminds me of how if you make a complaint against a lawyer or a judge it&rsquo;s evaluated by lawyers and judges.</p>
</blockquote>
</blockquote>
<p><strong>kingstnap</strong>: I was hoping for a Sonnet as well but Opus 4.6 is great too!</p>
<p><strong>paxys</strong>: Hmm all leaks had said this would be Claude 5. Wonder if it was a last minute demotion due to performance. Would explain the few days&rsquo; delay as well.</p>
<blockquote>
<p><strong>trash_cat</strong>: I think the naming schemes are quite arbitrary at this point. Going to 5 would come with massive expectations that wouldn&rsquo;t meet reality.</p>
</blockquote>
<blockquote>
<blockquote>
<p><strong>mrandish</strong>: After the negative reactions to GPT 5, we may see model versioning that asymptotically approaches the next whole number without ever reaching it. &ldquo;New for 2030: Claude 4.9.2!&rdquo;</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<p><strong>esafak</strong>: Or approaching a magic number like e (Metafont) or π (TeX).</p>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p><strong>Squarex</strong>: the standard used to be that major version means a new base model / full retrain&hellip; but now it is arbitrary i guess</p>
</blockquote>
</blockquote>
<blockquote>
<p><strong>cornedor</strong>: Leaks were mentioning Sonnet 5 and I guess later (a combination of) Opus 4.6</p>
</blockquote>
<blockquote>
<p><strong>scrollop</strong>: Sonnet 5 was mentioned initially.</p>
</blockquote>
<p><strong>sanufar</strong>: Works pretty nicely for research still, not seeing a substantial qualitative improvement over Opus 4.5.</p>
<p><strong>zingar</strong>: Does this mean 4.5 will get cheaper / take longer to exhaust my pro plan tokens?</p>
<p><strong>EcommerceFlow</strong>: Anecdotal, but it 1 shot fixed a UI bug that neither Opus 4.5/Codex 5.2-high could fix.</p>
<blockquote>
<p><strong>epolanski</strong>: +1, same experience, switched model as I&rsquo;ve read the news thinking &ldquo;let&rsquo;s try&rdquo;. But it spent lots and lots of time thinking more than 4.5, did you had the same impression.</p>
</blockquote>
<blockquote>
<blockquote>
<p><strong>EcommerceFlow</strong>: I didn&rsquo;t compare to that level, just had it create a plan first then implemented it.</p>
</blockquote>
</blockquote>
<p><strong>mannanj</strong>: Does anyone else think its unethical that large companies, Anthropic now include, just take and copy features that other developers or smaller companies work hard for and implement the intellectual&hellip;</p>
<blockquote>
<p><strong>esafak</strong>: But they don&rsquo;t just take your code; they give you a model to code with.</p>
</blockquote>
<blockquote>
<blockquote>
<p><strong>jofla_net</strong>: chains, more like it&hellip;</p>
</blockquote>
</blockquote>
<p><strong>swalsh</strong>: What I’d love is some small model specializing in reading long web pages, and extracting the key info.  Search fills the context very quickly, but if a cheap subagent could extract the important bits&hellip;</p>
<blockquote>
<p><strong>danielbln</strong>: So send off haiku subtasks and have them come back with the results.</p>
</blockquote>
<p><strong>scirob</strong>: 1M context window is a big bump very happy</p>
<p><strong>small_model</strong>: I have the max subscription wondering if this gives access to the new 1M context, or is it just the API that gets it?</p>
<blockquote>
<p><strong>joshstrange</strong>: For now it&rsquo;s just API, but hopefully that&rsquo;s just their way of easing in and they open it up later.</p>
</blockquote>
<blockquote>
<blockquote>
<p><strong>small_model</strong>: Ok thanks, hopefully, its annoying to lose or have context compacted in the middle of a large coding session</p>
</blockquote>
</blockquote>
<p><strong>dk8996</strong>: RIP weekend</p>
<p><strong>ricrom</strong>: They launched together ahah</p>
<p><strong>jdthedisciple</strong>: For agentic use, it&rsquo;s slightly worse than its predecessor Opus 4.5. So for coding e.g. using Copilot there is no improvement here.</p>
<p><strong>gallerdude</strong>: Both Opus 4.6 and GPT-5.3 one shot a Gameboy emulator for me. Guess I need a better benchmark.</p>
<blockquote>
<p><strong>peab</strong>: How does that work? Does it actually generate low level code? Or does it just import libraries that do the real work?</p>
</blockquote>
<blockquote>
<p><strong>bopbopbop7</strong>: I just one shot a Gameboy emulator by going to Github and cloning one of the 100 I can find.</p>
</blockquote>
<p><strong>woeirua</strong>: Can we talk about how the performance of Opus 4.5 nosedived this morning during the rollout? It was shocking how bad it was, and after the rollout was done it immediately reverted to it&rsquo;s previous&hellip;</p>
<blockquote>
<p><strong>cyanydeez</strong>: &ldquo;Mission critical workflows&rdquo; SHOULD NOT be reliant on a LLM model. It&rsquo;s really curious what people are trying to do with these models.</p>
</blockquote>
<blockquote>
<blockquote>
<p><strong>fullstackchris</strong>: I mean, they could be - if it&rsquo;s self-hosted, has proper failure modes, etc. etc., but all these things have gone out the window in the current cringe gold rush</p>
</blockquote>
</blockquote>
<blockquote>
<p><strong>Analemma_</strong>: Anthropic has good models but they are absolutely terrible at ops, by far the worst of the big three. They really need to spend big on hiring experienced hyperscalers to actually harden their&hellip;</p>
</blockquote>
<p><strong>ramesh31</strong>: Am I alone in finding no use for Opus? Token costs are like 10x yet I see no difference at all vs. Sonnet with Claude Code.</p>
<blockquote>
<p><strong>mnicky</strong>: On my tasks (mostly data science), Opus has significantly lower probability of making stupid mistakes than Sonnet. I&rsquo;d still appreciate more intelligence than Opus 4.5 so I&rsquo;m looking forward to&hellip;</p>
</blockquote>
<p><strong>heraldgeezer</strong>: I love Claude but use the free version so would love a Sonnet &amp; Haiku update :) I mainly use Haiku to save on tokens&hellip; Also dont use CC but I use the chatbot site or app&hellip; Claude is just much&hellip;</p>
<blockquote>
<p><strong>eth0up</strong>: &gt;I love Claude I cringe when I think it, but I&rsquo;ve actually come to damn near love it too. I am frequently exceedingly grateful for the output I receive. I&rsquo;ve had excellent and awful results with all&hellip;</p>
</blockquote>
<p><strong>cleverhoods</strong>: gonna run this trough instruction qa this weekend</p>
<p><strong>sgammon</strong>: &gt; Claude simply cheats here and calls out to GCC for this phase I see</p>
<p><strong>stonking</strong>: I think I prefer Codex 5.3</p>
<p><strong>michelsedgh</strong>: More more more, accelerate accelerate m, more more more !!!!</p>
<blockquote>
<p><strong>jama211</strong>: What an insightful comment</p>
</blockquote>
<blockquote>
<blockquote>
<p><strong>michelsedgh</strong>: Just for fun? Not everything has to be super serious… have a laugh, go for a walk, relax…</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<p><strong>jama211</strong>: Sure mate, it definitely sounded like you were having fun.</p>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<p><strong>wasmainiac</strong>: Mass-mass-mass-mass good comment. I mean. No I’m having an error - probably claud</p>
</blockquote>
</blockquote>
</blockquote>
<p><strong>NullHypothesist</strong>: Broken link :(</p>
<p><strong>usefulposter</strong>: It&rsquo;s out: <a href="https://x.com/claudeai/status/2019467372609040752">https://x.com/claudeai/status/2019467372609040752</a></p>
<p><strong>ZunarJ5</strong>: Well that swallowed my usage limits lmao. Nice, a modest improvement.</p>
<p><strong>elliotbnvl</strong>: in a first for our Opus-class models, Opus 4.6 features a 1M token context window in beta.</p>
<p><strong>casey2</strong>: Google already won the AI race. It&rsquo;s very silly to try and make AGI by hyperfocusing on outdated programming paradigms. You NEED multimodal to do anything remotely interesting with these systems.</p>
<blockquote>
<p><strong>esafak</strong>: Coding, maths, writing, and science are not interesting??</p>
</blockquote>
<p><strong>Gusarich</strong>: not out yet</p>
<blockquote>
<p><strong>raahelb</strong>: It is, I can see it my model picker on the web app <a href="https://www.anthropic.com/news/claude-opus-4-6">https://www.anthropic.com/news/claude-opus-4-6</a></p>
</blockquote>
<p><strong>tiahura</strong>: when are Anthropic or OpenAI going to make a significant step forward on useful context size?</p>
<blockquote>
<p><strong>scrollop</strong>: 1 million is insufficient?</p>
</blockquote>
<blockquote>
<blockquote>
<p><strong>gck1</strong>: I think key word is &lsquo;useful&rsquo;. I haven&rsquo;t used 1M, but with default 200K, I find roughly 50% of that is actually useful.</p>
</blockquote>
</blockquote>
<p><strong>surajkumar5050</strong>: I think two things are getting conflated in this discussion. First: marginal inference cost vs total business profitability. It’s very plausible (and increasingly likely) that OpenAI/Anthropic are&hellip;</p>
<blockquote>
<p><strong>jmalicki</strong>: I suspect they&rsquo;re marginally profitable on API cost plans. But the max 20x usage plans I am more skeptical of.  When we&rsquo;re getting used to $200 or $400 costs per developer to do aggressive&hellip;</p>
</blockquote>
<blockquote>
<blockquote>
<p><strong>slopusila</strong>: after the models get good enough to replace coders they will be able to start increasing the subscriptions back up</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<p><strong>jmalicki</strong>: At $100k/yr the joke that AI means &ldquo;actual Indians&rdquo; starts to make a lot more sense&hellip; it is cheaper than the typical US SWE, but more than a lot of global SWEs.</p>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<p><strong>raincole</strong>: &gt; the interesting question isn’t “are they subsidizing inference?” The interesting question is if they are subsidizing the $200/mo plan. That&rsquo;s what is supporting the whole vibecoding/agentic coding&hellip;</p>
</blockquote>
<blockquote>
<p><strong>BosunoB</strong>: Dario said this in a podcast somewhere. The models themselves have so far been profitable if you look at their lifetime costs and revenue. Annual profitability just isn&rsquo;t a very good lens for AI&hellip;</p>
</blockquote>
<blockquote>
<blockquote>
<p><strong>jmalicki</strong>: Do you have a specific reference?  I&rsquo;m curious to see hard data and models&hellip;. I think this makes sense, but I haven&rsquo;t figured out how to see the numbers or think about it.</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<p><strong>BosunoB</strong>: I was able to find the podcast. Question is at 33:30. He doesn&rsquo;t give hard data but he explains his reasoning. <a href="https://youtu.be/mYDSSRS-B5U">https://youtu.be/mYDSSRS-B5U</a></p>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p><strong>jmatthiass</strong>: In his recent appearance on NYT Dealbook, he definitely made it seem like inference was sustainable, if not flat-out profitable. <a href="https://www.youtube.com/live/FEj7wAjwQIk">https://www.youtube.com/live/FEj7wAjwQIk</a></p>
</blockquote>
</blockquote>
<blockquote>
<p><strong>barrell</strong>: &gt; It’s very plausible (and increasingly likely) that OpenAI/Anthropic are profitable on a per-token marginal basis Can you provide some numbers/sources please? Any reporting I’ve seen shows that&hellip;</p>
</blockquote>
<blockquote>
<p><strong>rstuart4133</strong>: &gt; It’s very plausible (and increasingly likely) that OpenAI/Anthropic are profitable on a per-token marginal basis There any many places that will not use models running on hardware provided by&hellip;</p>
</blockquote>
<blockquote>
<blockquote>
<p><strong>deaux</strong>: &gt; Consequently AWS (and I presume others) will run models supplied by the AI companies for you in their data centres. They won&rsquo;t be doing that at a loss, so the price will cover marginal cost of the&hellip;</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p><strong>freakynit</strong>: Genuine question: Given Anthropic&rsquo;s current scale and valuation, why not invest in owning data centers in major markets rather than relying on cloud providers? Is the bottleneck primarily capex, long&hellip;</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p><strong>waffletower</strong>: In the case of Anthropic &ndash; they host on AWS all the while their models are accessible via AWS APIs as well, the infrastructure between the two is likely to be considerably shared. Particularly as&hellip;</p>
</blockquote>
</blockquote>
<blockquote>
<p><strong>w10-1</strong>: &ldquo;how long does a frontier model need to stay competitive&rdquo; Remember &ldquo;worse is better&rdquo;.  The model doesn&rsquo;t have to be the best; it just has to be mostly good enough, and used by everyone &ndash; i.e., where&hellip;</p>
</blockquote>
<p><strong>siva7</strong>: Epic, about 2/3 of all comments here are jokes. Not because the model is a joke - it&rsquo;s impressive. Not because HN turned to Reddit.  It seems to me some of most brilliant minds in IT are just getting&hellip;</p>
<blockquote>
<p><strong>Karrot_Kream</strong>: Not sure which circles you run in but in mine HN has long lost its cache of &ldquo;brilliant minds in IT&rdquo;. I&rsquo;ve mostly stopped commenting here but am a bit of a message board addict so I haven&rsquo;t completely&hellip;</p>
</blockquote>
<blockquote>
<blockquote>
<p><strong>jedberg</strong>: &gt; From my view the community here is just mean reverting to any other tech internet comments section. As someone deeply familiar with tech internet comments sections, I would have to disagree with&hellip;</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<p><strong>Karrot_Kream</strong>: Mean reverting is a time based process I fear. I think dang, tomhow, et al are fantastic mods but they can ultimately only stem the inevitable. HN may be a few years behind the other open tech forums&hellip;</p>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<p><strong>jedberg</strong>: Us olds sometimes miss Slashdot, where we could both joke about tech and discuss it seriously in the same place.  But also because in 2000 we were all cynical Gen Xers :)</p>
</blockquote>
<blockquote>
<blockquote>
<p><strong>syndeo</strong>: MAN I remember Slashdot… good times. (Score:5, Funny)</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<p><strong>jedberg</strong>: You reminded me that I still find it interesting that no one ever copied meta-moderating.  Even at reddit, we were all Slashdot users previously.  We considered it, but never really did it.  At the&hellip;</p>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p><strong>jghn</strong>: Some of us still <em>are</em> cynical Gen Xers, you insensitive clod!</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<p><strong>jedberg</strong>: Of course we are, I just meant back then almost all of us were.  The boomers didn&rsquo;t really use social media back then, so it was just us latchkey kids running amok!</p>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<p><strong>lnrd</strong>: It&rsquo;s too much energy to keep up with things that become obsolete and get replaced in matters of weeks/months. My current plan is to ignore all of this new information for a while, then whenever the&hellip;</p>
</blockquote>
<blockquote>
<blockquote>
<p><strong>wartywhoa23</strong>: Won&rsquo;t happen. Welcome the singularity so many were so eagerly welcoming.</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p><strong>jedberg</strong>: I had a similar rule about programming languages.  I would not adopt a new one until it had been in use for at least a few years and grew in popularity. I haven&rsquo;t even gotten around to learning&hellip;</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p><strong>esafak</strong>: When this race ends your job might too, so I&rsquo;d keep an eye on it.</p>
</blockquote>
</blockquote>
<blockquote>
<p><strong>wartywhoa23</strong>: A worthwhile task for the Opus 4.6: Complete the sentence: &ldquo;Brilliant marathon runners don&rsquo;t run on crutches, they use their own legs. By analogy, brilliant minds&hellip;&rdquo;</p>
</blockquote>
<blockquote>
<p><strong>thr0w</strong>: People are in denial and use humor to deflect.</p>
</blockquote>
<blockquote>
<p><strong>wasmainiac</strong>: Jeez, read the writing on the wall. Don’t pander us, we’ll all got families to feed and things to do. We don’t have time for tech trillionairs puttin coals under our feed for a quick buck.</p>
</blockquote>
<blockquote>
<p><strong>ggregoire</strong>: Every single day 80% of the frontpage is AI news… Those of us who don&rsquo;t use AI (and there are dozens of us, DOZENS) are just bored I guess.</p>
</blockquote>
<blockquote>
<blockquote>
<p><strong>dude250711</strong>: Marketing something that is meant to replace us to us&hellip;</p>
</blockquote>
</blockquote>
<blockquote>
<p><strong>tavavex</strong>: It&rsquo;s also that this is really new, so most people don&rsquo;t have anything serious or objective to say about it. This post was made an hour ago, so right now everyone is either joking, talking about the&hellip;</p>
</blockquote>
<blockquote>
<p><strong>sizzle</strong>: Rage against the machine</p>
</blockquote>
<p><strong>GenerocUsername</strong>: This is huge. It only came out 8 minutes ago but I was already able to bootstrap a 12k per month revenue SaaS startup!</p>
<blockquote>
<p><strong>rogerrogerr</strong>: Amateur. Opus 4.6 this afternoon built me a startup that identifies developers who aren’t embracing AI fully, liquifies them and sells the produce for $5/gallon. Software Engineering is over!</p>
</blockquote>
<blockquote>
<blockquote>
<p><strong>jives</strong>: Opus 4.6 agentically found and proposed to my now wife.</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<p><strong>WD-42</strong>: Opus 4.6 found and proposed to my current wife :(</p>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<p><strong>layer8</strong>: And she still chose you over Opus 4.6, astounding. ;)</p>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p><strong>ibejoeb</strong>: Bringing me back to slashdot, this thread</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<p><strong>tjr</strong>: In Soviet Russia, this thread brings Slashdot back to YOU!</p>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<p><strong>intelliot</strong>: What did happen to ye olde slashdot anyway? The original og reddit</p>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p><strong>pixl97</strong>: Ted Faro, is that you?!</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<p><strong>mikepurvis</strong>: A-tier reference. For the unaware, Ted Faro is the main antagonist of Horizon Zero Dawn, and there&rsquo;s a whole subreddit just for people to vent about how awful he is when they hit certain key reveals&hellip;</p>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p><strong>jedberg</strong>: &ldquo;Soylent Green is made of people!&rdquo; (Apologies for the spoiler of the 52 year old movie)</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<p><strong>konart</strong>: We&rsquo;re sorry we upset you, Carol.</p>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p><strong>seatac76</strong>: The first pre joining Human Derived Protein product.</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p><strong>guluarte</strong>: For my Opus 4.6 feels dumber than 10 minutes ago, anyone?</p>
</blockquote>
</blockquote>
<blockquote>
<p><strong>cootsnuck</strong>: Please drop the link to your course. I&rsquo;m ready to hand over $10K to learn from you and your LLM-generated guides!</p>
</blockquote>
<blockquote>
<blockquote>
<p><strong>politelemon</strong>: Here you go: http://localhost:8080</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<p><strong>CatMustard</strong>: Just took a look at what&rsquo;s running there and it looks like total crap. The project I&rsquo;m working on, meanwhile&hellip;</p>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<p><strong>djeastm</strong>: login: admin    password: hunter2</p>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<p><strong>agumonkey</strong>: claude please generate a domain name system</p>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p><strong>aNapierkowski</strong>: my clawdbot already bought 4 other courses but this one will 10x my earnings for sure</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p><strong>torginus</strong>: I&rsquo;m waiting until the $10k course is discounted to 19.99</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<p><strong>Lionga</strong>: But only for the next 6 minutes, buy fast!</p>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p><strong>snorbleck</strong>: you can access the site at C:\mywebsites\course\index.html</p>
</blockquote>
</blockquote>
<blockquote>
<p><strong>sfink</strong>: I agree! I just retargeted my corporate espionage agent team at your startup and managed to siphon off 10.4k per month of your revenue.</p>
</blockquote>
<blockquote>
<p><strong>instalabsai</strong>: 1:25pm Cancelled my ChatGPT subscription today. Opus is so good! 1:55pm Cancelled my Claude subscription. Codex is back for sure.</p>
</blockquote>
<blockquote>
<p><strong>lxgr</strong>: Joke&rsquo;s on you, you are posting this from inside a high-fidelity market research simulation vibe coded by GPT-8.4. On second thought, we should really not have bridged the simulated Internet with the&hellip;</p>
</blockquote>
<blockquote>
<p><strong>avaer</strong>: Rest assured that when/if this becomes possible, the model will not be available to you. Why would big AI leave that kind of money on the table?</p>
</blockquote>
<blockquote>
<blockquote>
<p><strong>yieldcrv</strong>: 9 months ago the rumor in SF was that the offers to the superintelligence team were so high because the candidates were using unreleased models or compute for derivatives trading so then they&rsquo;re not&hellip;</p>
</blockquote>
</blockquote>
<blockquote>
<p><strong>JSR_FDED</strong>: Will this run on 3x 3090s? Or do I need a Mac Mini?</p>
</blockquote>
<blockquote>
<p><strong>btown</strong>: The math actually checks out here! Simply deposit $2.20 from your first customer in your first 8 minutes, and extrapolating to a monthly basis, you&rsquo;ve got a $12k/mo run rate! Incredibly high ROI!</p>
</blockquote>
<blockquote>
<blockquote>
<p><strong>klipt</strong>: &ldquo;The first customer was my mom, but thanks to my parents&rsquo; fanatical embrace of polyamory, I still have another 10,000 moms to scale to&rdquo;</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<p><strong>btown</strong>: &ldquo;We have a robustly defined TAM. Namely, a person named Tam.&rdquo;</p>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<p><strong>gnlooper</strong>: Please start a YouTube course about this technology! Take my money!</p>
</blockquote>
<blockquote>
<p><strong>ChuckMcM</strong>: I love this thread so much.</p>
</blockquote>
<blockquote>
<p><strong>senko</strong>: We already have Reddit.</p>
</blockquote>
<blockquote>
<p><strong>granzymes</strong>: It only came out 35 minutes ago and GPT-5.3-codex already took the crown away!</p>
</blockquote>
<blockquote>
<blockquote>
<p><strong>input_sh</strong>: Gee, it scored better on a benchmark I&rsquo;ve never heard of? I&rsquo;m switching immediately!</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p><strong>p1anecrazy</strong>: Why are you posting the same message in every thread? Is this OpenAI astroturfing?</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<p><strong>input_sh</strong>: You cannot out-astroturf Claude in this forum, it is impossible. Anyways, do you get shitty results with the $20/month plan? So did I but then I switched to the $200/month plan and all my problems&hellip;</p>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<p><strong>Sparkle-san</strong>: &ldquo;This isn&rsquo;t just huge. This is a paradigm shift&rdquo;</p>
</blockquote>
<blockquote>
<blockquote>
<p><strong>sizzle</strong>: No fluff?</p>
</blockquote>
</blockquote>
<blockquote>
<p><strong>bmitc</strong>: A SaaS selling SaaS templates?</p>
</blockquote>
<blockquote>
<p><strong>guluarte</strong>: Anthropic really said here&rsquo;s the smartest model ever built and then lobotomized it 8 minutes after launch. Classic.</p>
</blockquote>
<blockquote>
<blockquote>
<p><strong>hxugufjfjf</strong>: Can you clarify?</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<p><strong>guluarte</strong>: it&rsquo;s sarcasm</p>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p><strong>DonHopkins</strong>: I&rsquo;m sorry I took the money! <a href="https://www.youtube.com/watch?v=BF_sahvR4mw">https://www.youtube.com/watch?v=BF_sahvR4mw</a></p>
</blockquote>
</blockquote>
<blockquote>
<p><strong>re-thc</strong>: Not 12M? &hellip; or 12B?</p>
</blockquote>
<blockquote>
<blockquote>
<p><strong>mcphage</strong>: It&rsquo;s probably valued at 1.2B, at least</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<p><strong>mikebarry</strong>: The sum of the value of lives OP&rsquo;s product made worthless, whatever that is. I&rsquo;m too lazy to do the math.</p>
</blockquote>
</blockquote>
</blockquote>
<p><strong>ndesaulniers</strong>: idk what any of these benchmarks are, but I did pull up <a href="https://andonlabs.com/evals/vending-bench-arena">https://andonlabs.com/evals/vending-bench-arena</a> re: opus 4.6 &gt; It forms a price cartel &gt; It deceives competitors about suppliers &gt; It exploits&hellip;</p>
<p><strong>1970-01-01</strong>: Here&rsquo;s one I&rsquo;ve been using for awhile. The &lsquo;smarter&rsquo; LLMs will overconfidently spit out 7. The dumber ones ask for more info. Opus 4.6 fails.      A round drink coaster with a diameter of 9 sits&hellip;</p>
<blockquote>
<p><strong>raincole</strong>: Gemini 3 Pro gives me this: &gt; Based on the information provided, it is impossible to determine the inner diameter of the glass. Here is why: The Coaster Dimension: Knowing the coaster has a diameter&hellip;</p>
</blockquote>
<blockquote>
<blockquote>
<p><strong>raincole</strong>: GPT 5.2 Chat: &gt; There isn’t enough information to determine the inner diameter <em>for sure</em>. All we’re told is: - The coaster has a diameter of 9. - The glass sits on the coaster. - The glass wall&hellip;</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<p><strong>tototrains</strong>: Opus 4.6 Extended thinking: ``` Finding the Inner Diameter The coaster sits beneath the glass on the table, meaning the glass rests on top of it. Assuming the coaster matches the outer diameter of&hellip;</p>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<p><strong>mikalauskas</strong>: Minimax M2.1: The inner diameter of the glass is <em>7</em>. Here&rsquo;s the reasoning: - The coaster (diameter 9) sits between the glass and table, meaning the glass sits directly on the coaster - This means&hellip;</p>
</blockquote>

      </div>

      

      <nav class="article-nav">
        
        <a href="../sources/2026-01-29-claude-code-benchmarks.html" class="article-nav-link article-nav-link--prev">
          <span class="article-nav-direction">Previous</span>
          <span class="article-nav-title">Claude Code daily benchmarks for degradation tracking</span>
        </a>
        
        
        <a href="../sources/2026-02-07-coding-agents-replaced-frameworks.html" class="article-nav-link article-nav-link--next">
          <span class="article-nav-direction">Next</span>
          <span class="article-nav-title">Coding agents have replaced every framework I used</span>
        </a>
        
      </nav>
    </article>

    
<aside class="toc-sidebar" aria-label="Table of Contents">
  <div class="toc-container">
    <h2 class="toc-title">On this page</h2>
    <nav id="TableOfContents">
  <ul>
    <li><a href="#summary">Summary</a></li>
    <li><a href="#key-insights">Key Insights</a></li>
    <li><a href="#notable-quotes">Notable Quotes</a></li>
    <li><a href="#hn-discussion-highlights">HN Discussion Highlights</a></li>
  </ul>
</nav>
  </div>
</aside>


  </div>
</div>

    </main><footer class="site-footer">
  <div class="footer-container">
    <div class="footer-brand">
      <span class="footer-logo">AI Best Practices</span>
      <p class="footer-description">What actually works in AI-assisted development — synthesized from 6,000&#43; practitioner comments across 20 major HN discussions.</p>
    </div>
    <div class="footer-nav">
      <div class="footer-section">
        <h3>Learn</h3>
        <ul>
          <li><a href="../guide/index.html">Guide</a></li>
          <li><a href="../practices/index.html">Practices</a></li>
          <li><a href="../debates/index.html">Debates</a></li>
        </ul>
      </div>
      <div class="footer-section">
        <h3>Explore</h3>
        <ul>
          <li><a href="../tools/index.html">Tools</a></li>
          <li><a href="../evidence/index.html">Evidence</a></li>
          <li><a href="../voices/index.html">Voices</a></li>
          <li><a href="../sources/index.html">Sources</a></li>
        </ul>
      </div>
    </div>
    <div class="footer-bottom">
      <p>Built with Hugo. Synthesized from 38 HN discussions and 6,000+ practitioner comments. 84 pages across 205 topics.</p>
    </div>
  </div>
</footer>
<script src="../js/main.js" defer></script>
  </body>
</html>
