<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Mistral 3 family of models released | AI Best Practices Knowledge Base</title>
  <meta name="description" content="What actually works in AI-assisted development — synthesized from 6,000&#43; practitioner comments across 20 major HN discussions.">
  <meta name="color-scheme" content="dark light">

  
  <meta property="og:title" content="Mistral 3 family of models released">
  <meta property="og:description" content="What actually works in AI-assisted development — synthesized from 6,000&#43; practitioner comments across 20 major HN discussions.">
  <meta property="og:type" content="article">

  
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

  
  <link rel="stylesheet" href="../css/style.css">

  
  <link rel="icon" href="../favicon.svg" type="image/svg+xml">
</head>
<body class="section-sources"><header class="site-header">
  <nav class="nav-container">
    <a class="nav-logo" href="../index.html">
      <svg width="22" height="22" viewBox="0 0 24 24" fill="none" xmlns="http://www.w3.org/2000/svg">
        <circle cx="12" cy="12" r="10" stroke="currentColor" stroke-width="2"/>
        <circle cx="12" cy="8" r="2" fill="currentColor"/>
        <circle cx="7" cy="15" r="2" fill="currentColor"/>
        <circle cx="17" cy="15" r="2" fill="currentColor"/>
        <line x1="12" y1="10" x2="7" y2="13" stroke="currentColor" stroke-width="1.5"/>
        <line x1="12" y1="10" x2="17" y2="13" stroke="currentColor" stroke-width="1.5"/>
        <line x1="7" y1="15" x2="17" y2="15" stroke="currentColor" stroke-width="1.5"/>
      </svg>
      <span>AI Best Practices</span>
    </a>
    <div class="nav-links">
      <a class="nav-link" data-section="guide" href="../guide/index.html">Guide</a>
      <a class="nav-link" data-section="practices" href="../practices/index.html">Practices</a>
      <a class="nav-link" data-section="debates" href="../debates/index.html">Debates</a>
      <a class="nav-link" data-section="tools" href="../tools/index.html">Tools</a>
      <a class="nav-link" data-section="evidence" href="../evidence/index.html">Evidence</a>
      <a class="nav-link" data-section="voices" href="../voices/index.html">Voices</a>
      <a class="nav-link" data-section="sources" href="../sources/index.html">Sources</a>
    </div>
    <div class="nav-actions">
      <button class="nav-search-btn" type="button" aria-label="Search" id="searchTrigger">
        <svg width="14" height="14" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"><circle cx="11" cy="11" r="8"/><line x1="21" y1="21" x2="16.65" y2="16.65"/></svg>
        <span>Search</span>
        <kbd>/</kbd>
      </button>
      <button class="dark-mode-toggle" aria-label="Toggle dark mode" type="button">
        <svg class="icon-sun" width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round">
          <circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/>
        </svg>
        <svg class="icon-moon" width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round">
          <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"/>
        </svg>
      </button>
      <button class="nav-toggle" aria-label="Toggle menu" type="button">
        <span></span>
      </button>
    </div>
  </nav>
</header>

    <div class="search-overlay" id="searchOverlay">
      <div class="search-container">
        <div class="search-input-wrap">
          <svg width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"><circle cx="11" cy="11" r="8"/><line x1="21" y1="21" x2="16.65" y2="16.65"/></svg>
          <input type="text" class="search-input" id="searchInput" placeholder="Search articles, tools, patterns..." autocomplete="off">
        </div>
        <div class="search-results" id="searchResults"></div>
        <div class="search-hint">
          <span><kbd>Esc</kbd> close</span>
          <span><kbd>&uarr;</kbd><kbd>&darr;</kbd> navigate</span>
          <span><kbd>Enter</kbd> open</span>
        </div>
      </div>
    </div>

    <main>
<div class="page-container">
  
<nav class="breadcrumbs" aria-label="Breadcrumb">
  <ol>
    <li><a href="../index.html">Home</a></li>
    
      
    
      
        <li><a href="../sources/index.html">Sources</a></li>
      
    
    <li aria-current="page">Mistral 3 family of models released</li>
  </ol>
</nav>



  <div class="article-layout has-toc">
    <article class="article">
      <header class="article-header">
        <h1 class="article-title">Mistral 3 family of models released</h1>
        <div class="meta-bar">
  
  <time class="meta-date" datetime="2025-12-02">
    December 2, 2025
  </time>
  

  

  <span class="meta-reading-time">24 min read</span>

  
  <span class="tier-badge tier-badge--2">Tier 2</span>
  

  <div class="meta-links">
    
    <a href="https://mistral.ai/news/mistral-3" class="meta-link" target="_blank" rel="noopener">Source &#8599;</a>
    
    
    
    <a href="https://news.ycombinator.com/item?id=46121889" class="meta-link" target="_blank" rel="noopener">HN &#8599;</a>
    
    
  </div>

  
  <span class="meta-stat">826 points</span>
  
  
  
  <span class="meta-stat">38 comments</span>
  
  
</div>

        
<div class="tag-pills">
  
  
  
  <a href="../tags/open-source-models.html" class="tag-pill">open-source-models</a>
  
  
  
  
  <a href="../tags/model-releases.html" class="tag-pill">model-releases</a>
  
  
  
  
  <a href="../tags/multimodal.html" class="tag-pill">multimodal</a>
  
  
  
  
  <a href="../tags/benchmarks.html" class="tag-pill">benchmarks</a>
  
  
  
  
  <a href="../tags/mistral.html" class="tag-pill">mistral</a>
  
  
</div>


      </header>

      <div class="article-content">
        <h2 id="summary">Summary</h2>
<p>Mistral AI released the Mistral 3 family, a new generation of open-source multimodal models under the Apache 2.0 license. The lineup includes three dense models at 3B, 8B, and 14B parameters (the Ministral variants), plus Mistral Large 3, a sparse mixture-of-experts model with 41B active parameters drawn from a 675B total pool.</p>
<p>All models feature native multimodal and multilingual capabilities, handling both text and images across more than 40 languages. The smaller Ministral variants target cost-efficiency, with the 14B reasoning variant achieving strong accuracy on math benchmarks. Mistral Large 3 ranks highly among open-source non-reasoning models on the LMArena leaderboard and demonstrates parity with leading instruction-tuned open-weight models for general tasks. A notable efficiency claim is that the Ministral family produces far fewer tokens than competitors while achieving comparable performance, significantly reducing computational costs.</p>
<p>The models were trained on NVIDIA Hopper GPUs with support for TensorRT-LLM and SGLang. Deployment targets range from data center systems to edge devices such as RTX PCs and Jetson hardware. The 3B vision model can even run in the browser after a 3GB download. Models are available through Mistral AI Studio, Amazon Bedrock, Azure, Hugging Face, and multiple third-party platforms.</p>
<p>The release was overshadowed somewhat by DeepSeek 3.2 launching around the same time. Technically, the Mistral Large 3 model uses a DeepSeek V2-style architecture, which some commenters noted Mistral did not prominently acknowledge. Regardless, the release represents a significant contribution to the open-source model ecosystem from one of Europe&rsquo;s leading AI companies.</p>
<h2 id="key-insights">Key Insights</h2>
<ul>
<li><strong>Open-source MoE at scale</strong>: Mistral Large 3 uses a 675B-parameter mixture-of-experts architecture with 41B active parameters, released under Apache 2.0, making it one of the most capable openly-licensed MoE models available.</li>
<li><strong>Efficiency gains through fewer tokens</strong>: The Ministral family claims dramatically fewer output tokens than competitors for equivalent performance, translating to major cost savings for inference workloads.</li>
<li><strong>Multimodal and multilingual from the start</strong>: All models in the family natively support vision and 40+ languages, addressing a common gap in open-weight models for non-English language support.</li>
<li><strong>Browser-runnable small models</strong>: The 3B model can run entirely in a web browser via WebGPU, lowering the barrier to experimentation significantly.</li>
</ul>
<h2 id="notable-quotes">Notable Quotes</h2>
<blockquote>
<p>&ldquo;insanely fast, cheap, reliable, and follows formatting instructions to the letter&rdquo; — barrell (HN)</p>
</blockquote>
<blockquote>
<p>&ldquo;Europe&rsquo;s bright star has been quiet for a while&rdquo; — mythz (HN)</p>
</blockquote>
<h2 id="hn-discussion-highlights">HN Discussion Highlights</h2>
<p><em>177 comments total</em></p>
<p><strong>barrell</strong>: I use large language models in <a href="http://phrasing.app">http://phrasing.app</a> to format data I can retrieve in a consistent skimmable manner. I switched to mistral-3-medium-0525 a few months back after struggling to get gpt-&hellip;</p>
<blockquote>
<p><strong>mrtksn</strong>: Some time ago I canceled all my paid subscriptions to chatbots because they are interchangeable so I just rotate between Grok, ChatGPT, Gemini, Deepseek and Mistral. On the API side of things my ex&hellip;</p>
</blockquote>
<blockquote>
<blockquote>
<p><strong>barrell</strong>: Yep I spent 3 days optimizing my prompt trying to get gpt-5 to work. Tried a bunch of different models (some Azure some OpenRouter) and got a better success rate with several others without any tai&hellip;</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<p><strong>distalx</strong>: What tools or process do you use to optimize your prompts?</p>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p><strong>barbazoo</strong>: &gt; I guess they hope I forget to cancel. Business model of most subscription based services.</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<p><strong>viking123</strong>: For me it&rsquo;s just that I am too lazy to start switching from my GPT subscription, I use it with codex and it&rsquo;s very good for my use-case. And the price at least here in Asia is not expensive at all &hellip;</p>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<p><strong>b3ing</strong>: I estimate at 10% of meetup runs like that</p>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p><strong>acuozzo</strong>: &gt; because they are interchangeable What is your use-case? Mine is: I use &ldquo;Pro&rdquo;/&ldquo;Max&rdquo;/&ldquo;DeepThink&rdquo; models to iterate on novel cross-domain applications of existing mathematics. My interaction is: I c&hellip;</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<p><strong>mrtksn</strong>: my use case is Google replacement, things that I can do by myself so I can verify and things that are not important so I don’t have to verify. Sure, they produce different output so sometimes I wil&hellip;</p>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p><strong>giancarlostoro</strong>: Maybe give Perplexity a shot? It has Grok, ChatGPT, Gemini, Kimi K2, I dont think it has Mistral unfortunately.</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<p><strong>mrtksn</strong>: I like perplexity actually but haven’t been using it since some time. Maybe I should give it a go :)</p>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<p><strong>VHRanger</strong>: Kagi has Mistral as well</p>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<p><strong>druskacik</strong>: This is my experience as well. Mistral models may not be the best according to benchmarks and I don&rsquo;t use them for personal chats or coding, but for simple tasks with pre-defined scope (such as cat&hellip;</p>
</blockquote>
<blockquote>
<blockquote>
<p><strong>leobg</strong>: Did you compare it to gemini-2.0-flash-lite?</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<p><strong>leobg</strong>: Answering my own question: Artificial Analysis ranks them close in terms of price (both 0.3 USD/1M tokens) and intelligence (27 / 29 for gemini/mistral), but ranks gemini-2.0-flash-lite higher in t&hellip;</p>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<p><strong>druskacik</strong>: I did some vibe-evals only and it seemed slightly worse for my use case, so I didn&rsquo;t change it.</p>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<p><strong>mbowcut2</strong>: It makes me wonder about the gaps in evaluating LLMs by benchmarks. There almost certainly is overfitting happening which could degrade other use cases. &ldquo;In practice&rdquo; evaluation is what inspired th&hellip;</p>
</blockquote>
<blockquote>
<blockquote>
<p><strong>pants2</strong>: The best benchmark is one that you build for your use-case. I finally did that for a project and I was not expecting the results. Frontier models are generally &ldquo;good enough&rdquo; for most use-cases but &hellip;</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<p><strong>airstrike</strong>: If you and others have any insights to share on structuring that benchmark, I&rsquo;m all ears. There a new model seemingly every week so finding a way to evaluate them repeatedly would be nice. The answ&hellip;</p>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<p><strong>dotancohen</strong>: How do you find and decide which obscure models to test? Do you manually review the model card for each new model on Hugging Face? Is there a better resource?</p>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p><strong>Legend2440</strong>: I don’t think benchmark overfitting is as common as people think. Benchmark scores are highly correlated with the subjective “intelligence” of the model. So is pretraining loss. The only exception &hellip;</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p><strong>pembrook</strong>: If the models from the big US labs are being overfit to benchmarks, than we also need to account for HN commenters overfitting positive evaluations to Chinese or European models based on their poli&hellip;</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<p><strong>astrange</strong>: Americans have an opposing bias via the phenomenon of &ldquo;safe edgy&rdquo;, where for obvious reasons they&rsquo;re uncomfortable with being biased towards anyone who looks like a US minority, and redirect all th&hellip;</p>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<p><strong>mentalgear</strong>: Thanks for sharing your use case of the mistral models, which are indeed top-notch ! I had a look at phrasing.app, and while a nice website, I found the copy of &ldquo;Hand-crafted. Phrasing was designed&hellip;</p>
</blockquote>
<blockquote>
<blockquote>
<p><strong>barrell</strong>: I don&rsquo;t see the contention. I do not use llms in the design, development, copywriting, marketing, blogging, or any other aspect of the crafting of the application. I labor over every word, every bu&hellip;</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<p><strong>willlma</strong>: It&rsquo;s interesting. I&rsquo;ve been tinkering with an article summarizing/highlighting browser extension, and realized that I don&rsquo;t want the end-user to have read AI-generated content because it&rsquo;s not as h&hellip;</p>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<p><strong>basilgohar</strong>: I admire and respect this stance. I have been very AI-hesitant and while I&rsquo;m using it more and more, I have spaces that I want to definitely keep human-only, as this is my preference. I&rsquo;m glad to h&hellip;</p>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<p><strong>metadat</strong>: Are you saying gpt-5 produces gibberish 15% of the time? Or are you comparing Mistral gibberish production rate to gpt-5.1&rsquo;s complex task failure rate? Does Mistral even have a Tool Use model? That&hellip;</p>
</blockquote>
<blockquote>
<blockquote>
<p><strong>barrell</strong>: Yes. I spent about 3 days trying to optimize the prompt to get gpt-5 to not produce gibberish, to no avail. Completions took several minutes, had an above 50% timeout rate (with a 6 minute timeout &hellip;</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<p><strong>data-ottawa</strong>: With gpt5 did you try adjusting the reasoning level to &ldquo;minimal&rdquo;? I tried using it for a very small and quick summarization task that needed low latency and any level above that took several second&hellip;</p>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<p><strong>barbazoo</strong>: Hard to gauge what gibberish is without an example of the data and what you prompted the LLM with.</p>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<p><strong>acuozzo</strong>: I have a need to remove loose &ldquo;signature&rdquo; lines from the last 10% of a tremendous e-mail dataset. Based on your experience, how do you think mistral-3-medium-0525 would do?</p>
</blockquote>
<blockquote>
<blockquote>
<p><strong>barrell</strong>: What&rsquo;s your acceptable error rate? Honestly ministral would probably be sufficient if you can tolerate a small failure rate. I feel like medium would be overkill. But I&rsquo;m no expert. I can&rsquo;t say I&rsquo;v&hellip;</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<p><strong>acuozzo</strong>: I&rsquo;d prefer for the error rate to be as close to 0% as possible under the strict requirement of having to use a local model. I have access to nodes with 8xH200, but I&rsquo;d prefer to not tie those up wi&hellip;</p>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<p><strong>mackross</strong>: Cool app. I couldn’t see a way to report an error in one of the default expressions.</p>
</blockquote>
<p><strong>msp26</strong>: The new large model uses DeepseekV2 architecture. 0 mention on the page lol. It&rsquo;s a good thing that open source models use the best arch available. K2 does the same but at least mentions &ldquo;Kimi K2 w&hellip;</p>
<blockquote>
<p><strong>Jackson__</strong>: So they spent all of their R&amp;D to copy deepseek, leaving none for the singular novel added feature: vision. To quote the hf page: &gt;Behind vision-first models in multimodal tasks: Mistral Large 3 ca&hellip;</p>
</blockquote>
<blockquote>
<blockquote>
<p><strong>Ey7NFZ3P0nzAe</strong>: Well, behind &ldquo;models&rdquo; not &ldquo;langual models&rdquo;. Of course models purely made for image stuff will completely wipe it out. The vision language models are useful for their generalist capabilities</p>
</blockquote>
</blockquote>
<blockquote>
<p><strong>make3</strong>: Architecture difference wrt vanilla transformers and between modern transformers are a tiny part of what makes a model nowadays</p>
</blockquote>
<blockquote>
<p><strong>halJordan</strong>: I don&rsquo;t think it&rsquo;s fair to demand everything be open and then get mad when they open-ness is used. It&rsquo;s an obsessive and harmful double standard.</p>
</blockquote>
<p><strong>simonw</strong>: The 3B vision model runs in the browser (after a 3GB model download). There&rsquo;s a very cool demo of that here: <a href="https://huggingface.co/spaces/mistralai/Ministral_3B_WebGPU">https://huggingface.co/spaces/mistralai/Ministral_3B_WebGPU</a> Pelicans are OK but not eart&hellip;</p>
<blockquote>
<p><strong>troyvit</strong>: I&rsquo;m reading this post and wondering what kind of crazy accessibility tools one could make. I think it&rsquo;s a little off the rails but imagine a tool that describes a web video for a blind user as it h&hellip;</p>
</blockquote>
<blockquote>
<blockquote>
<p><strong>GaggiX</strong>: This is not local but Gemini models can process very long videos and provide description with timestamps if asked for. <a href="https://ai.google.dev/gemini-api/docs/video-understanding#tr">https://ai.google.dev/gemini-api/docs/video-understanding#tr</a>&hellip;</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<p><strong>embedding-shape</strong>: Nor would it be describing things as they happen, but instead needing pre-processing, so in the end, very different :)</p>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<p><strong>user_of_the_wek</strong>: &gt; The image depicts and older man&hellip; Ouch</p>
</blockquote>
<p><strong>mythz</strong>: Europe&rsquo;s bright star has been quiet for a while, great to see them back and good to see them come back to Open Source light with Apache 2.0 licenses - they&rsquo;re too far from the SOTA pack that exclus&hellip;</p>
<blockquote>
<p><strong>rvz</strong>: All thanks to the US VCs that acutally have money to fund Mistral&rsquo;s entire business. Had they gone to the EU, Mistral would have gotten a miniscule grant from the EU to train their AI models.</p>
</blockquote>
<blockquote>
<blockquote>
<p><strong>amarcheschi</strong>: Mistral biggest investor is asml, although it became so later than other vcs</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p><strong>crimsoneer</strong>: I mean, one is a government, the other are VCs (also, I would be shocked if there isn&rsquo;t some French gov funding somewhere in the massive mistral pile).</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<p><strong>kergonath</strong>: &gt; I would be shocked if there isn&rsquo;t some French gov funding somewhere in the massive mistral pile There is a bit of it, yes, although how much exactly is difficult to know. It’s not all tax breaks &hellip;</p>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p><strong>whiplash451</strong>: 1. so what 2. asml</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<p><strong>rvz</strong>: 1. It matters. 2. Did ASML invest in Mistral in their first round of venture funding or was it US VCs all along that took that early risk and backed them from the very start? Risk aversion is in th&hellip;</p>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<p><strong>apexalpha</strong>: 1. Big problem 2. ASML was propped up by ASM and Philips, stepping in as &ldquo;VCs&rdquo;</p>
</blockquote>
</blockquote>
</blockquote>
<p><strong>timpera</strong>: Extremely cool! I just wish they would also include comparisons to SOTA models from OpenAI, Google, and Anthropic in the press release, so it&rsquo;s easier to know how it fares in the grand scheme of th&hellip;</p>
<blockquote>
<p><strong>Youden</strong>: They mentioned LMArena, you can get the results for that here: <a href="https://lmarena.ai/leaderboard/text">https://lmarena.ai/leaderboard/text</a> Mistral Large 3 is ranked 28, behind all the other major SOTA models. The delta between Mistral an&hellip;</p>
</blockquote>
<blockquote>
<blockquote>
<p><strong>jampekka</strong>: 1491 vs 1418 ELO means the stronger model wins about 60% of the time.</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<p><strong>supermatt</strong>: Probably naive questions: Does that also mean that Gemini-3 (the top ranked model) loses to mistral 3 40% of the time? Does that make Gemini 1.5x better, or mistral 2/3rd as good as Gemini, or can &hellip;</p>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<p><strong>qznc</strong>: I guess that could be considered comparative advertising then and companies generally try to avoid that scrutiny.</p>
</blockquote>
<blockquote>
<p><strong>constantcrying</strong>: The lack of the comparison (which absolutely was done), tells you exactly what you need to know.</p>
</blockquote>
<blockquote>
<blockquote>
<p><strong>bildung</strong>: I think people from the US often aren&rsquo;t aware how many companies from the EU simply won&rsquo;t risk losing their data to the providers you have in mind, OpenAI, Anthropic and Google. They simply are no &hellip;</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<p><strong>adam_patarino</strong>: We&rsquo;re seeing the same thing for many companies, even in the US. Exposing your entire codebase to an unreliable third party is not exactly SOC / ISO compliant. This is one of the core things that mo&hellip;</p>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<p><strong>leobg</strong>: Does your company use Microsoft Teams?</p>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<p><strong>BoorishBears</strong>: Mistral is founded by multiple Meta engineers, no? Funded mostly by US VCs? Hosted primarily on Azure? Do you really have to go out of your way to start calling their competition &ldquo;data leeches&rdquo; for&hellip;</p>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p><strong>popinman322</strong>: They&rsquo;re comparing against open weights models that are roughly a month away from the frontier. Likely there&rsquo;s an implicit open-weights political stance here. There are also plenty of reasons not to&hellip;</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<p><strong>kalkin</strong>: Scale AI wrote a paper a year ago comparing various models performance on benchmarks to performance on similar but held-out questions. Generally the closed source models performed better, and Mistr&hellip;</p>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<p><strong>extr</strong>: ??? Closed US frontier models are vastly more effective than anything OSS right now, the reason they didn’t compare is because they’re a different weight class (and therefore product) and it’s a bi&hellip;</p>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p><strong>crimsoneer</strong>: If someone is using these models, they probably can&rsquo;t or won&rsquo;t use the existing SOTA models, so not sure how useful those comparisons actually are. &ldquo;Here is a benchmark that makes us look bad from &hellip;</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<p><strong>constantcrying</strong>: Completely agree, that there are legitimate reasons to prefer comparison to e.g. deepeek models. But that doesn&rsquo;t change my point, we both agree that the comparisons would be extremely unfavorable.</p>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p><strong>tarruda</strong>: Here&rsquo;s what I understood from the blog post: - Mistral Large 3 is comparable with the previous Deepseek release. - Ministral 3 LLMs are comparable with older open LLMs of similar sizes.</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<p><strong>constantcrying</strong>: And implicit in this is that it compares very poorly to SOTA models. Do you disagree with that? Do you think these Models are beating SOTA and they did not include the benchmarks, because they forgot?</p>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<p><strong>rvz</strong>: &gt; I just wish they would also include comparisons to SOTA models from OpenAI, Google, and Anthropic in the press release, Why would they? They know they can&rsquo;t compete against the heavily closed-sou&hellip;</p>
</blockquote>
<p><strong>yvoschaap</strong>: Upvoting for Europe&rsquo;s best efforts.</p>
<blockquote>
<p><strong>sebzim4500</strong>: That&rsquo;s unfair to Europe. A bunch of AI work is done in London (Deepmind is based here for a start)</p>
</blockquote>
<blockquote>
<blockquote>
<p><strong>p2detar</strong>: That&rsquo;s ok. How could they know that there are companies like Aleph Alpha, Helsing or the famous DeepL. European companies are not that vocal, but that doesn&rsquo;t mean they aren&rsquo;t making progress in th&hellip;</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p><strong>Glemkloksdjf</strong>: Thats not the point. Deepmind is not an UK company, its google aka US. Mistral is a real EU based company.</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<p><strong>gishh</strong>: Using US VC dollars. Where their desks are isn’t really important.</p>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p><strong>GaggiX</strong>: London is not part of Europe anymore since Brexit /s</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<p><strong>ot</strong>: Is it so hard for people to understand that Europe is a continent, EU is a federation of European countries, and the two are not the same?</p>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<p><strong>tmoravec</strong>: Drifted to the Caribbean.</p>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p><strong>colesantiago</strong>: Deepmind doesn&rsquo;t exist anymore. Google DeepMind does exist.</p>
</blockquote>
</blockquote>
<blockquote>
<p><strong>LunaSea</strong>: Upvoting Windows 11 as the US&rsquo;s best effort at Operating Systems development.</p>
</blockquote>
<blockquote>
<blockquote>
<p><strong>DarmokJalad1701</strong>: Wouldn&rsquo;t that be macOS? Or BSD? Or Unix? CentOS?</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<p><strong>LunaSea</strong>: What&rsquo;s the market share of those compared to Windows and Linux?</p>
</blockquote>
</blockquote>
</blockquote>
<p><strong>mrinterweb</strong>: I don&rsquo;t like being this guy, but I think Deepseek 3.2 stole all the thunder yesterday. Notice that these comparisons are to Deepseek 3.1. Deepseek 3.2 is a big step up over 3.1, if benchmarks are t&hellip;</p>
<blockquote>
<p><strong>hiddencost</strong>: Idk. They look like they&rsquo;re ahead on the saturated benchmarks and behind on the unsaturated ones. Looks more like that over fit to the benchmarks.</p>
</blockquote>
<p><strong>simgt</strong>: I still don&rsquo;t understand what the incentive is for releasing genuinely good model weights. What makes sense however is OpenAI releasing a somewhat generic model like gpt-oss that games the benchmar&hellip;</p>
<blockquote>
<p><strong>mirekrusin</strong>: Because there is no money in making them closed. Open weight means secondary sales channels like their fine tuning service for enterprises [0]. They can&rsquo;t compete with large proprietary providers b&hellip;</p>
</blockquote>
<blockquote>
<p><strong>talliman</strong>: Until there is a sustainable, profitable and moat-building business model for generative AI, the competition is not to have the best proprietary model, but rather to raise the most VC money to be w&hellip;</p>
</blockquote>
<blockquote>
<blockquote>
<p><strong>mirekrusin</strong>: Explained well in this documentary [0]. [0] <a href="https://www.youtube.com/watch?v=BzAdXyPYKQo">https://www.youtube.com/watch?v=BzAdXyPYKQo</a></p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<p><strong>simgt</strong>: I was fully expecting that but it doesn&rsquo;t get old ;)</p>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p><strong>memming</strong>: It’s funny how future money drive the world. Fortunately it’s fueling progress this time around.</p>
</blockquote>
</blockquote>
<blockquote>
<p><strong>NitpickLawyer</strong>: &gt; gpt-oss that games the benchmarks just for PR. gpt-oss is killing the ongoing AIME3 competition on kaggle. They&rsquo;re using a hidden, new set of problems, IMO level, handcrafted to be &ldquo;AI hardened&rdquo;&hellip;.</p>
</blockquote>
<blockquote>
<blockquote>
<p><strong>lostmsu</strong>: Are they ahead of all other recent open models? Is there a leaderboard?</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<p><strong>NitpickLawyer</strong>: There is a leaderboard [1] but we&rsquo;ll have to wait till april for the competition to end to know what models they&rsquo;re using. The current number 3 on there (34/50) has mentioned in discussions that th&hellip;</p>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<p><strong>prodigycorp</strong>: gpt-oss are really solid models. by far the best at tool calling, and performant.</p>
</blockquote>
<blockquote>
<p><strong>nullbio</strong>: Google games benchmarks more than anyone, hence Gemini&rsquo;s strong bench lead. In reality though, it&rsquo;s still garbage for general usage.</p>
</blockquote>
<p><strong>tucnak</strong>: If the claims on multilingual and pretraining performance are accurate, this is huge! This may be the best-in-class multilingual stuff since the more recent Gemma&rsquo;s, where they used to be unmatched&hellip;</p>
<blockquote>
<p><strong>NitpickLawyer</strong>: &gt; I wonder why scores on TriviaQA vis-a-vis 14b model lags behind Gemma 12b so much; that one is not a formatting-heavy benchmark. My guess is the vast scale of google data. They&rsquo;ve been hoovering &hellip;</p>
</blockquote>
<p><strong>nullbio</strong>: Anyone else find that despite Gemini performing best on benches, it&rsquo;s actually still far worse than ChatGPT and Claude? It seems to hallucinate nonsense far more frequently than any of the others. &hellip;</p>
<blockquote>
<p><strong>apexalpha</strong>: No, I&rsquo;ve been using Gemini for help while learning / building my onprem k8s cluster and it has been almost spotless. Granted, this is a subject that is very well present in the training data but st&hellip;</p>
</blockquote>
<blockquote>
<blockquote>
<p><strong>Synthetic7346</strong>: I found gemini 3 to be pretty lackluster for setting up an onprem k8s cluster - sonnet 4.5 was more accurate from the get go, required less handholding</p>
</blockquote>
</blockquote>
<blockquote>
<p><strong>mvkel</strong>: Open weight LLMs aren&rsquo;t supposed to &ldquo;beat&rdquo; closed models, and they never will. That isn’t their purpose. Their value is as a structural check on the power of proprietary systems; they guarantee a c&hellip;</p>
</blockquote>
<blockquote>
<blockquote>
<p><strong>cmrdporcupine</strong>: This may be the case, but DeepSeek 3.2 is &ldquo;good enough&rdquo; that it competes well with Sonnet 4 &ndash; maybe 4.5 &ndash; for about 80% of my use cases, at a fraction of the cost. I feel we&rsquo;re only a year or two&hellip;</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<p><strong>troyvit</strong>: I think you&rsquo;re right, and I feel the same about Mistral. It&rsquo;s &ldquo;good enough&rdquo;, super cheap, privacy friendly, and doesn&rsquo;t burn coal by the shovel-full. No need to pay through the nose for the SOTA mo&hellip;</p>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p><strong>barrell</strong>: I can attest to Mistral beating OpenAI in my use cases pretty definitively :)</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<p><strong>theshrike79</strong>: In my use cases mistral has been next to useless. Granted my uses have been programming related. Mistral prints the answer almost immediately and is also completely and utterly hallucinating everyt&hellip;</p>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p><strong>re-thc</strong>: &gt; Open weight LLMs aren&rsquo;t supposed to &ldquo;beat&rdquo; closed models, and they never will. That isn’t their purpose. Do things ever work that way? What if Google did Open source Gemini. Would you say the sam&hellip;</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<p><strong>lowkey_</strong>: Not the above poster, but: OpenAI went closed (despite open literally being in the name) once they had the advantage. Meta also is going closed now that they&rsquo;ve caught up. Open-source makes sense t&hellip;</p>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p><strong>pants2</strong>: &gt; Their value is as a structural check on the power of proprietary systems Unfortunately that doesn&rsquo;t pay the electricity bill</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<p><strong>array_key_first</strong>: It kind of does, because the proprietary systems are unacceptable for many usecases because they are proprietary. There&rsquo;s a lot of businesses who do not want to hand over their sensitive data to ha&hellip;</p>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<p><strong>dchest</strong>: Nope, Gemini 3 is hallucinating less than GPT-5.1 for my questions.</p>
</blockquote>
<blockquote>
<p><strong>mrtksn</strong>: Yep, Gemini is my least favorite and I’m convinced that the hype around it isn’t organic because I don’t see the claimed “superiority”, quite the opposite.</p>
</blockquote>
<blockquote>
<blockquote>
<p><strong>cmrdporcupine</strong>: I think a lot of the hype around Gemini comes down to people who aren&rsquo;t using it for coding but for other things maybe. Frankly, I don&rsquo;t actually care about or want &ldquo;general intelligence&rdquo; &ndash; I want&hellip;</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<p><strong>erichocean</strong>: I exclusively use Gemini Pro for coding, and it&rsquo;s been writing ~100% of the code I produce since July. It&rsquo;s great.</p>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<p><strong>tootie</strong>: No? My recent experience with Gemini was terrific. The last big test I gave of Claude it spun an immaculate web of lies before I forced it to confess.</p>
</blockquote>
<blockquote>
<p><strong>llm_nerd</strong>: What does your comment have to do with the submission? What a weird non-sequitur. I even went looking at the linked article to see if it somehow compares with Gemini. It doesn&rsquo;t, and only relates t&hellip;</p>
</blockquote>
<blockquote>
<p><strong>cmrdporcupine</strong>: I also had bad luck when I finally tried Gemini 3 in the gemini CLI coding tool. I am unclear if it&rsquo;s the model or their bad tooling/prompting. It had, as you said, hallucination problems, and it a&hellip;</p>
</blockquote>
<blockquote>
<p><strong>bluecalm</strong>: My experience is the opposite although I don&rsquo;t use it to write code but to explore/learn about algorithms and various programming ideas. It&rsquo;s amazing. I am close to cancelling my ChatGPT subscripti&hellip;</p>
</blockquote>
<blockquote>
<p><strong>minimaxir</strong>: For noncoding tasks, Gemini atleast allows for easier grounding with Google Search.</p>
</blockquote>
<blockquote>
<p><strong>alfalfasprout</strong>: If anything it&rsquo;s a testament to human intelligence that benchmarks haven&rsquo;t really been a good measure of a model&rsquo;s competence for some time now. They provide a relative sorting to some degree, with&hellip;</p>
</blockquote>
<blockquote>
<p><strong>gunalx</strong>: Have used gemini3 to GEW shot a few problems GPT5 struggled on.</p>
</blockquote>
<blockquote>
<p><strong>moffkalast</strong>: Yes, and likewise with Kimi K2. Despite being on the top of open source benches it makes up more batshit nonsense than even Llama 3. Trust no one, test your use case yourself is pretty much the onl&hellip;</p>
</blockquote>
<blockquote>
<p><strong>VeejayRampay</strong>: no, I find Gemini to be the best</p>
</blockquote>
<p><strong>arnaudsm</strong>: Geometric mean of MMMLU + GPQA-Diamond + SimpleQA + LiveCodeBench : - Gemini 3.0 Pro : 84.8 - DeepSeek 3.2 : 83.6 - GPT-5.1 : 69.2 - Claude Opus 4.5 : 67.4 - Kimi-K2 (1.2T) : 42.0 - Mistral Large 3&hellip;</p>
<blockquote>
<p><strong>jasonjmcghee</strong>: How is there such a gap between Gemini 3 vs GPT 5.1/Opus 4.5? What is Gemini 3 crushing the others on?</p>
</blockquote>
<blockquote>
<blockquote>
<p><strong>arnaudsm</strong>: Could be optimized for benchmarks, but Gemini 3 has been stellar for my tasks so far. Maybe an architectural leap?</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<p><strong>netdur</strong>: I believe it is the system instructions that make the difference for Gemini, as I use Gemini on AI Studio with my system prompts to get it to do what I need it to do, which is not possible with gem&hellip;</p>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p><strong>gishh</strong>: Gamed tests?</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<p><strong>rdtsc</strong>: I always joke that Google pays for a dedicated developer to spend their full time just to make pelicans on bicycles look good. They certainly have the cash to do it.</p>
</blockquote>
</blockquote>
</blockquote>
<p><strong>tootyskooty</strong>: Since no one has mentioned it yet: note that the benchmarks for large are for the base model, not for the instruct model available in the API. Most likely reason is that the instruct model underper&hellip;</p>
<p><strong>esafak</strong>: Well done to the France&rsquo;s Mistral team for closing the gap. If the benchmarks are to be believed, this is a viable model, especially at the edge.</p>
<blockquote>
<p><strong>nullbio</strong>: Benchmarks are never to be believed, and that has been the case since day 1.</p>
</blockquote>
<p><strong>hnuser123456</strong>: Looks like their own HF link is broken or the collection hasn&rsquo;t been made public yet. The 14B instruct model is here: <a href="https://huggingface.co/mistralai/Ministral-3-14B-Instruct-25">https://huggingface.co/mistralai/Ministral-3-14B-Instruct-25</a>&hellip; The unsloth qua&hellip;</p>
<blockquote>
<p><strong>janpio</strong>: Seems fixed now: <a href="https://huggingface.co/collections/mistralai/mistral-large-3">https://huggingface.co/collections/mistralai/mistral-large-3</a> <a href="https://huggingface.co/collections/mistralai/ministral-3">https://huggingface.co/collections/mistralai/ministral-3</a></p>
</blockquote>
<p><strong>andhuman</strong>: This is big. The first really big open weights model that understands images.</p>
<blockquote>
<p><strong>yoavm</strong>: How is this different from Llama 3.2 &ldquo;vision capabilities&rdquo;? <a href="https://www.llama.com/docs/how-to-guides/vision-capabilities">https://www.llama.com/docs/how-to-guides/vision-capabilities</a>&hellip;</p>
</blockquote>
<blockquote>
<blockquote>
<p><strong>Havoc</strong>: Guessing GP commenter considers Apache more &ldquo;open&rdquo; than Meta&rsquo;s license. Which to be fair isn&rsquo;t terrible but also not quite as clean as straight apache</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<p><strong>mesebrec</strong>: Llama&rsquo;s license explicitly disallows its usage in the EU. If that doesn&rsquo;t even meet the threshold for &ldquo;terrible&rdquo;, then what does?</p>
</blockquote>
</blockquote>
</blockquote>
<p><strong>trvz</strong>: Sad to see they&rsquo;ve apparently fully given up on releasing their models via torrent magnet URLs shared on Twitter; those will stay around long after Hugging Face is dead.</p>
<blockquote>
<p><strong>ThrowawayTestr</strong>: How does HF manage to serve such big files?</p>
</blockquote>
<blockquote>
<blockquote>
<p><strong>nikcub</strong>: s3 + cloudfront <a href="https://huggingface.co/blog/rearchitecting-uploads-and-downl">https://huggingface.co/blog/rearchitecting-uploads-and-downl</a>&hellip;</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<p><strong>ThrowawayTestr</strong>: I meant more how do they pay for all that bandwidth. I can download a 20gb model in like 2 minutes</p>
</blockquote>
</blockquote>
</blockquote>
<p><strong>accrual</strong>: Congrats on the release, Mistral team! I haven&rsquo;t used Mistral much until today but am impressed. I normally use Gemma 3 27B locally, but after regenerating some responses with Mistral 3 14B, the ou&hellip;</p>
<p><strong>Tiberium</strong>: A bit interesting that they used Deepseek 3&rsquo;s architecture for their Large model :)</p>
<p><strong>RandyOrion</strong>: Thank you Mistral for releasing new small parameter-efficient (aka dense) models.</p>
<p><strong>lalassu</strong>: It&rsquo;s sad that they only compare to open weight models. I feel most users don&rsquo;t care much about OSS/not OSS. The value proposition is the quality of the generation for some use case. I guess it says&hellip;</p>
<blockquote>
<p><strong>para_parolu</strong>: It’s not for users but for businesses. There is demand for inhouse use with data privacy. Regular users can’t even run large model due to lack of compute.</p>
</blockquote>
<blockquote>
<p><strong>troyvit</strong>: Glad I&rsquo;m not most users. I&rsquo;m down for 80% of the quality for an open weight model. Hell I&rsquo;ve been using Linux for 25 years so I suppose I&rsquo;m used to not-the-greatest-but-free.</p>
</blockquote>
<blockquote>
<p><strong>hopelite</strong>: It seems to be a reasonable comparison since that is the primary/differentiating characteristic of the model. It’s really common to also and seemingly only ever see the comparison of closed weight/&hellip;</p>
</blockquote>
<p><strong>mortsnort</strong>: I use a small model as a chatbot of sorts in a game I&rsquo;m making. I was hoping the 3b could replace qwen 4b, but it&rsquo;s far worse at following instructions and providing entertaining content. I suppose&hellip;</p>
<p><strong>dmezzetti</strong>: Looking forward to trying them out. Great to see they are Apache 2.0&hellip;always good to have easy-to-understand licensing.</p>
<p><strong>jasonjmcghee</strong>: I wish they showed how they compared to models larger/better and what the gap is, rather than only models they&rsquo;re better than. Like how does 14B compare to Qwen30B-A3B? (Which I think is a lot of p&hellip;</p>
<p><strong>GaggiX</strong>: The small dense model seems particularly good for their small sizes, I can&rsquo;t wait to test them out.</p>
<p><strong>codybontecou</strong>: Do all of these models, regardless of parameters, support tool use and structured output?</p>
<blockquote>
<p><strong>Y_Y</strong>: In principle any model can do these. Tool use is just detecting something like &ldquo;I should run a db query for pattern X&rdquo; and structured output is even easier, just reject output tokens that don&rsquo;t mat&hellip;</p>
</blockquote>
<blockquote>
<p><strong>Ey7NFZ3P0nzAe</strong>: Yes they all support tool use at least.</p>
</blockquote>
<p><strong>domoritz</strong>: Urg, the bar charts to not start at 0. It&rsquo;s making it impossible to compare across model sizes. That&rsquo;s a pretty basic chart design principle. I hope they can fix it. At least give me consistent y s&hellip;</p>
<p><strong>Frannky</strong>: I haven&rsquo;t tried a Mistral model in ages. Llama and Mistral feel like something I was using in another era. Are they good?</p>
<p><strong>RYJOX</strong>: I find that there are too many paid sub models at the minute with non legitimate progress to warrant the money spent. Recently cancelled GPT.</p>
<p><strong>tmaly</strong>: I see several 3.x versions on Openrouter.ai, any idea which of those are the new models?</p>
<blockquote>
<p><strong>PhilippGille</strong>: <a href="https://openrouter.ai/mistralai/mistral-large-2512">https://openrouter.ai/mistralai/mistral-large-2512</a></p>
</blockquote>
<p><strong>Aissen</strong>: Anyone succeed in running it with vLLM?</p>
<blockquote>
<p><strong>Patrick_Devine</strong>: The instruct models are available on Ollama (e.g. <code>ollama run ministral-3:8b</code>), however the reasoning models still are a wip. I was trying to get them to work last night and it works for single tur&hellip;</p>
</blockquote>
<blockquote>
<p><strong>dloss</strong>: Yes, the 3B variant, with vLLM 0.11.2. Parameters are given on the HF page. Had to override the temperature to 0.15 though (as suggested on HF) to avoid random looking syllables.</p>
</blockquote>
<blockquote>
<p><strong>Aissen</strong>: It now seems to work with the latest vLLM git.</p>
</blockquote>
<p><strong>pixel_popping</strong>: fyi Mistral admins, there is no dates showing on your article.</p>
<p><strong>s_dev</strong>: I was subscribing to these guys purely to support the EU tech scene. So I was on Pro for about 2 years while using ChatGPT and Claude. Went to actually use it, got a message saying that I missed a &hellip;</p>
<blockquote>
<p><strong>cycomanic</strong>: I&rsquo;m not sure I understand you correctly, but it seems you had a subscription missed one payment some time ago, but now expect that your subscription works because the missed month was in the past a&hellip;</p>
</blockquote>
<blockquote>
<blockquote>
<p><strong>s_dev</strong>: &gt;This sounds like the you expect your subscription to work as an on-demand service? That&rsquo;s exactly what it is. &gt;I&rsquo;m not sure I understand you correctly, I understand perfectly well, I don&rsquo;t agree w&hellip;</p>
</blockquote>
</blockquote>
<blockquote>
<p><strong>shlomo_z</strong>: This seems like a legitimate complaint&hellip; I wonder why it&rsquo;s downvoted</p>
</blockquote>
<blockquote>
<blockquote>
<p><strong>s_dev</strong>: My critique is more levelled at Mistral and not specifically what they&rsquo;ve just released so it could be that some see what I have to say as off topic. Also a lot of Europeans are upset at US tech do&hellip;</p>
</blockquote>
</blockquote>
<p><strong>ThrowawayTestr</strong>: Awesome! Can&rsquo;t wait till someone abliterates them.</p>
<p><strong>RomanPushkin</strong>: Mistral presented DeepSeek 3.2</p>
<p><strong>another_twist</strong>: I am not sure why Meta paid 13B+ to hire some kid vs just hiring back or acquiring these folks. They&rsquo;ll easily catch up.</p>
<blockquote>
<p><strong>Rastonbury</strong>: Age aside, not sure what Zuck was thinking, seeing as Scale AI was in data labelling and not training models, perhaps he thought he was a good operator? Then again the talent scarcity is in scienti&hellip;</p>
</blockquote>
<blockquote>
<p><strong>vintagedave</strong>: What is this referring to? I googled and the company was founded in 2016. No one involved can to a “kid”?</p>
</blockquote>
<blockquote>
<blockquote>
<p><strong>another_twist</strong>: True no one involved in Scale AI right now is a kid. But, their expertise is in data labelling not cutting edge AI. Compare that to the Mistral team. They launched a new LLM within 6months of found&hellip;</p>
</blockquote>
</blockquote>

      </div>

      

      <nav class="article-nav">
        
        <a href="../sources/2025-12-30-openai-cash-burn.html" class="article-nav-link article-nav-link--prev">
          <span class="article-nav-direction">Previous</span>
          <span class="article-nav-title">OpenAI&#39;s cash burn will be one of the big bubble questions of 2026</span>
        </a>
        
        
        <a href="../sources/2026-01-06-agentic-frameworks-2026.html" class="article-nav-link article-nav-link--next">
          <span class="article-nav-direction">Next</span>
          <span class="article-nav-title">Agentic Frameworks in 2026: Less Hype, More Autonomy</span>
        </a>
        
      </nav>
    </article>

    
<aside class="toc-sidebar" aria-label="Table of Contents">
  <div class="toc-container">
    <h2 class="toc-title">On this page</h2>
    <nav id="TableOfContents">
  <ul>
    <li><a href="#summary">Summary</a></li>
    <li><a href="#key-insights">Key Insights</a></li>
    <li><a href="#notable-quotes">Notable Quotes</a></li>
    <li><a href="#hn-discussion-highlights">HN Discussion Highlights</a></li>
  </ul>
</nav>
  </div>
</aside>


  </div>
</div>

    </main><footer class="site-footer">
  <div class="footer-container">
    <div class="footer-brand">
      <span class="footer-logo">AI Best Practices</span>
      <p class="footer-description">What actually works in AI-assisted development — synthesized from 6,000&#43; practitioner comments across 20 major HN discussions.</p>
    </div>
    <div class="footer-nav">
      <div class="footer-section">
        <h3>Learn</h3>
        <ul>
          <li><a href="../guide/index.html">Guide</a></li>
          <li><a href="../practices/index.html">Practices</a></li>
          <li><a href="../debates/index.html">Debates</a></li>
        </ul>
      </div>
      <div class="footer-section">
        <h3>Explore</h3>
        <ul>
          <li><a href="../tools/index.html">Tools</a></li>
          <li><a href="../evidence/index.html">Evidence</a></li>
          <li><a href="../voices/index.html">Voices</a></li>
          <li><a href="../sources/index.html">Sources</a></li>
        </ul>
      </div>
    </div>
    <div class="footer-bottom">
      <p>Built with Hugo. Synthesized from 32 HN discussions and 6,000+ practitioner comments. 78 pages across 198 topics.</p>
    </div>
  </div>
</footer>
<script src="../js/main.js" defer></script>
  </body>
</html>
