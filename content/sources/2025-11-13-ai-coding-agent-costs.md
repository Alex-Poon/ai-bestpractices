---
title: "I spent $638 on AI coding agents in 6 weeks"
source_url: "https://news.ycombinator.com/item?id=45914307"
hn_url: "https://news.ycombinator.com/item?id=45914307"
date: 2025-11-13
hn_points: 1
hn_comment_count: 5
tags: [cost-optimization, cursor, claude-code, model-selection, pricing]
tier: 2
weight: 97
---

## Summary

A founder and CTO building an AI-first CRM product shared a detailed breakdown of their AI coding costs, revealing surprisingly high expenses from using Cursor with Claude models. Over a six-week period spanning October and November 2025, the author accumulated $638 in on-demand charges, with October alone costing $348.56 and hitting Cursor's $400 limit.

The cost analysis revealed that Claude 4.5 Sonnet Thinking requests ranged from $0.02 to $0.06 depending on context size, which seemed modest per-request but compounded rapidly at 200+ daily requests. The author experimented with 7 different models (GPT-5, Gemini 2.5 Pro, Cheetah, and others) but found Claude consumed 85% of the budget because it consistently produced the best results.

The post asked fundamental questions about the sustainability of AI-assisted development costs, projecting annual expenses of $5,500+ just for code assistance. It raised the question of whether the productivity gains justify these costs and at what price point developers would reconsider their usage patterns.

A follow-up comment from the author included detailed Cursor usage statistics generated by Sonnet: total cost of $928.45 over 70 days, average of 70.7 requests per 5-hour block, median time between requests of just 13 seconds indicating burst activity patterns, and an impressive 88.8% cache hit rate that significantly reduced what costs would otherwise have been.

## Key Insights

- **Costs compound at scale**: Per-request costs of $0.02-0.06 seem trivial but reach $400+/month with heavy usage patterns of 200+ daily requests
- **Model quality drives spend**: Despite trying 7 alternatives, 85% of budget went to Claude because quality differences in output justified the premium
- **Cache hit rates matter**: An 88.8% cache hit rate significantly controlled costs, suggesting caching strategies are critical for heavy users
- **Pricing model instability**: The author observed that AI coding pricing has not stabilized, creating uncertainty for budgeting

## Notable Quotes

> "more than some cloud bills" â€” nthypes (HN submission)

## HN Discussion Highlights

The discussion generated 1 point and 5 comments, reflecting the niche audience for cost analysis posts.

**Detailed usage analytics**
- **nthypes** (OP): Shared Cursor-generated statistics showing $928.45 over 70 days, average $13.86/day, with peak activity at 1-2 PM and Saturdays being the busiest day. The 88.8% cache hit rate was highlighted as a major cost saver.

**Value assessment and alternatives**
- **Rochus**: Asked about concrete ROI, reporting mixed results with Claude where re-work consumed most time savings, but finding GPT-5 produced code that compiled and ran correctly more often, all covered by a Perplexity subscription
- **mnky9800n**: Recommended switching to Claude Code Max at $125/month flat rate, noting they run multiple agents doing research as a scientist without cost surprises
- **Woods369**: Pointed out that pricing is still stabilizing, noting Warp terminal's new lower-cost plans and an industry acknowledgment that current pricing doesn't scale sustainably

**The investment framing**
- **6510**: Suggested that AI coding costs are an ongoing investment in skill development, noting that proficiency with these tools will determine future competitive advantage
