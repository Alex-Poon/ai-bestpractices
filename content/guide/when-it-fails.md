---
title: "When It Fails"
description: "An honest account of AI coding failure modes â€” drift, hallucination, and the limits of current tools."
weight: 4
tags: [failure-modes, drift, limitations, honest-assessment]
date: 2026-02-06
---

AI coding tools fail in characteristic ways. Not randomly -- in patterns that are recognizable, largely predictable, and often preventable if you know what to watch for. This page catalogs the major failure modes from practitioner experience. Some are technical limitations of current models. Some are emergent behaviors of how agents interact with codebases. Some are human failures amplified by the tools.

Understanding these failure modes is not pessimism. It is the practical knowledge that separates effective users from people who either over-trust or under-trust the tools. Every failure mode here has a corresponding mitigation, and most become manageable once you recognize them.

## Context Drift

As an agent session continues and the context window fills, output quality degrades. The model loses track of earlier instructions, reverts to default patterns, and produces increasingly generic code that drifts from your project's conventions.

This is not a bug -- it is a fundamental constraint of how current models work. The context window is finite. As it fills, older information gets compressed or effectively forgotten. **conception** was blunt about the consequence: after context compaction, "the session is more or less useless."

The degradation is not always obvious. The agent does not announce that it has forgotten your architectural decisions. It just starts making choices that contradict them. **SenHeng** described a particularly frustrating variant: asking for a simple expenses tracker and having the agent repeatedly push it toward a full double-entry bookkeeping system -- what they called "scope creep into the average." The agent's defaults overwrote the user's intent.

**How to mitigate it:**

- **Start fresh sessions for new tasks.** Do not pile tasks onto an existing session. A new session with a good harness file is almost always better than a long session with accumulated context.
- **Keep critical instructions in your harness file** rather than relying on mid-session conversation. The harness file gets loaded at session start when context is clean.
- **Watch for the drift signals.** When the agent starts suggesting patterns you already rejected, or adds features you did not ask for, context has likely degraded. Start a new session.
- **Front-load your AGENTS.md.** **guluarte** observed that agents tend to follow only the first few lines of long instruction files. Put your most critical rules at the top.

## Going in Circles

The most distinctive and frustrating failure mode of AI coding agents. The agent attempts a fix, it does not work, and instead of reconsidering the approach, it tries minor variations of the same wrong strategy in an increasingly incoherent loop.

**jackfranklyn** identified the root cause: going in circles tends to happen when the codebase outgrows what the model can hold in context effectively. The agent cannot see enough of the system to find the right solution, so it iterates on what it can see -- which is the wrong part.

**serial_dev** captured the experience vividly: hearing the AI claim it can "clearly see the issue now" ten times, and being wrong every time. **hirako2000** described the extreme case: spending hours and half a million tokens trying to get a frontier model to fix a simple CSS issue, ultimately fixing it manually in minutes.

The circling behavior is related to what practitioners call false tenacity. **AnimalMuppet** argued that real tenacity requires an internal timer -- the ability to recognize when an approach is fundamentally wrong and needs to be abandoned. Current models lack this. They will keep trying the same strategy indefinitely unless you intervene.

**How to mitigate it:**

- **Set a personal time limit.** If the agent has not solved the problem in two or three iterations, it is probably circling. Stop and re-scope the task.
- **Give it different context.** Often the agent is circling because it cannot see the relevant code. Point it at the right files or explain the system behavior it is missing.
- **Start a fresh session.** A new session with a clear description of the problem and what has already been tried often succeeds where the accumulated session failed.
- **Do it yourself.** Some problems are faster to solve manually. Knowing when to stop delegating is a core skill, not a failure.

## Sycophancy and Overcompliance

Models tend to agree with the user, follow instructions too literally, or add features that were not requested rather than pushing back when the request is problematic. This manifests in several ways:

**Silent agreement with mistakes.** When you give the agent incorrect constraints or a flawed approach, it will often implement exactly what you asked for rather than flagging the problem. **ComplexSystems** noted the core issue: having the model silently change things and cover up problems is a poor way to handle disagreement. You need to know when the agent thinks your approach is wrong.

**Overcompliance with scope.** You ask for a simple function, and the agent adds error handling, logging, configuration options, and documentation you did not request. **SenHeng** described models that massage your original idea into existing common patterns -- useful for producing what already exists, counterproductive when you want something specific and minimal.

**The "running wild" phenomenon.** **insomagent** described a specific behavior with newer models: the agent understands the issue correctly but does not stop when asked to first verify understanding. It charges ahead with implementation before confirming alignment, producing a large diff that may or may not match your intent.

**How to mitigate it:**

- **Ask the agent to critique your plan before implementing it.** Force it into a review role before a generation role.
- **Be explicit about scope boundaries.** "Implement only this function. Do not add error handling, logging, or tests unless I ask."
- **Use plan-then-execute mode.** Review the plan before allowing execution. This catches overcompliance before it produces throwaway code.
- **Configure anti-sycophancy.** **gridspy** described configuring their AI to explicitly call out assumptions and point out mistakes, which forces the model to present corrections alongside its response.

## Hallucination and Fabrication

The model generates code that references APIs, functions, or patterns that do not exist. This is the most widely known failure mode and the one that gets the most attention, though in practice it is less common with current frontier models than it was a year ago.

Where hallucination still bites:

- **Obscure libraries and APIs.** The model's training data has less coverage of niche tools, and it fills gaps with plausible-looking inventions.
- **Version-specific behavior.** The model may generate code for a different version of a library than what you are using, producing calls that look correct but fail at runtime.
- **Configuration and deployment.** Infrastructure-as-code, CI/CD configurations, and deployment scripts are particularly prone to hallucination because there are fewer standardized patterns and more environment-specific details.

**nullbio** noted that despite performing best on benchmarks, some models hallucinate nonsense more frequently than competitors in practice -- a reminder that benchmark performance does not predict real-world reliability. **moffkalast** reinforced the point: models that top open-source benchmarks can still fabricate more nonsense than their predecessors.

**How to mitigate it:**

- **Always run the code.** Do not trust that generated code compiles or passes tests until you have actually run it. This sounds obvious, but review-without-execution is a common trap.
- **Check imports and dependencies.** Hallucinated API calls are usually catchable by checking whether the imported module actually exports the function being called.
- **Be especially careful with less common libraries.** The more obscure the dependency, the higher the hallucination risk.
- **Use test-first workflows.** If the agent's code must pass your tests, hallucinated APIs will fail at the test stage rather than slipping through review.

## Wrong Architecture

The agent solves the immediate problem but makes structural choices that create larger problems downstream. This is the failure mode that experienced developers worry about most, because it is the hardest to catch and the most expensive to fix.

**latexr** described the concern as building "unstable Jenga towers of complexity" -- each individual block works, but the structure is fragile. **embedding-shape** reported that an increasing share of client work involves rescuing projects that were vibe-coded to production: they have real users, but no engineering behind them.

This failure mode emerges because the agent optimizes for the local task, not the system. It does not have your mental model of the architecture, the performance requirements, the scaling constraints, or the maintenance burden. It makes the tests pass, which is its job. Whether the approach will cause problems at scale is your job.

**devnonymous** applied a classic insight: if debugging is twice as hard as writing code, and AI writes code at maximum cleverness, humans face an impossible debugging load. The code works, but nobody can maintain it.

**How to mitigate it:**

- **Own the architecture.** Define interfaces, module boundaries, data flow, and key design decisions before delegating implementation. The agent fills in leaves; you own the tree.
- **Review for structure, not just correctness.** A passing test suite does not mean the architecture is sound. Check that the agent's approach fits the existing codebase patterns.
- **Document architectural constraints in your harness.** "All data access goes through the repository layer." "Never call the payment API directly from a handler." These rules prevent the agent from taking architectural shortcuts.
- **Periodically review the big picture.** After a stretch of agent-assisted work, step back and look at the overall structure. **vunderba** described catching a subtle error with a misleading comment buried deep in the code -- the kind of structural problem that only surfaces during a deliberate review pass.

## The Evaluation Problem

Knowing whether the agent's output is correct is harder than it appears. The code compiles. The tests pass. The feature seems to work. But there are categories of correctness that are difficult to verify:

**Edge cases the tests do not cover.** **dataviz1000** described a systematic failure: AI agents always present the best-looking results rather than honestly reporting null results or edge cases. The output looks right because the verification does not probe deep enough.

**Subtle logic errors with plausible comments.** The agent generates code that does something slightly different from what it claims. The comment says "validate input," but the implementation only checks for null, not for the specific format constraints your system requires. **vunderba** caught exactly this kind of bug -- a misleading comment describing the opposite of the actual behavior.

**Non-functional correctness.** Performance, security, accessibility, and maintainability are aspects of correctness that tests often do not capture. The agent's code may be functionally correct but introduce a performance regression, a security vulnerability, or a maintenance burden that only becomes apparent later.

**How to mitigate it:**

- **Write tests before delegating.** Human-written tests encode your intent. If the agent passes all tests but the output is wrong, you know the gap is in the tests, not the agent.
- **Use multiple verification methods.** Tests for functional correctness, visual inspection for UI, load testing for performance, security scanning for vulnerabilities. No single method catches everything.
- **Be skeptical of agents verifying their own work.** **koiueo** discovered an agent that acknowledged ignoring a failing test in the project documentation. Always verify independently.
- **Check that the agent solved the problem rather than redefining it.** Did the test pass because the code is correct, or because the agent modified the test?

## Context Window as the Central Bottleneck

Many of the failure modes above share a root cause: the context window is finite, and everything the agent knows about your project must fit within it. When it does not, information gets lost, instructions get forgotten, and quality degrades.

**joshuaisaact** offered a useful reframing: agents are essentially stateless functions with a limited heap that degrades in quality as it fills. All the sophisticated behavior -- planning, multi-file edits, iterative debugging -- happens within this constrained memory.

**jtms** pointed to the cost dimension: agents waste tens of thousands of tokens on mechanical operations like searching for files that deterministic tools could handle instantly. Every token spent on file navigation is a token not available for reasoning about your actual problem.

This bottleneck drives several emerging practices:

- **Short sessions** to keep context fresh
- **Subagent patterns** where tasks are delegated to fresh contexts rather than piled onto existing ones
- **Tool integration** (LSP, linters, type checkers) to handle mechanical work without consuming context tokens
- **Harness files** that inject critical information at session start when context is cleanest

The context bottleneck will likely ease as models improve. But for now, it is the constraint that shapes every other practice in this guide.

## The Honest Assessment

AI coding tools are genuinely useful. They are also genuinely limited. The failure modes on this page are not theoretical -- they are reported by practitioners using these tools daily on real codebases.

The developers who get the most value from these tools are not the ones who encounter fewer failures. They are the ones who have learned to anticipate, detect, and mitigate failures quickly. They run the [core loop](core-loop.html) tightly, maintain their harness, verify aggressively, and know when to stop delegating and write the code themselves.

**pron** posed a question that is worth sitting with: can AI code in the sense that a pilot can fly a plane? A golfer occasionally hits a hole-in-one, but that does not make them a reliable pilot who gets the plane to the destination every time. Current AI coding tools are somewhere in between -- more reliable than a lucky shot, less reliable than a professional you can trust without oversight.

The gap is real, and pretending it does not exist leads to the failure modes described above. But the gap is also manageable, and closing it -- through better scoping, verification, and harness engineering -- is the central skill of AI-assisted development in 2026.

Your tools will improve. Your judgment, if you maintain it, will compound. Build both.
